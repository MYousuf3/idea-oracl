[
  {
    "rank": 1,
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "arxiv": "2302.13971v1",
    "paper_tar": "tar_files/2302.13971v1.tar.gz",
    "abstract": "=-1 We introduce , a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, -13B outperforms GPT-3 (175B) on most benchmarks, and -65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research communityhttps://github.com/facebookresearch/llama.",
    "content": "=1 [11pt]article ACL2023 times latexsym booktabs xcolor multirow graphicx xspace verbatim framed makecell tabularx stfloats [T1]fontenc [utf8]inputenc microtype inconsolata [normalem]ulem color listings escapeinside=(||) lightgrayrgb.9,.9,.9 darkgrayrgb.4,.4,.4 purplergb0.65, 0.12, 0.82 JavaScript keywords=break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with, morecomment=[l]//, morecomment=[s]/**/, morestring=[b]', morestring=[b]\", ndkeywords=class, export, boolean, throw, implements, import, this, keywordstyle=blue, ndkeywordstyle=darkgray, identifierstyle=black, commentstyle=purple, stringstyle=red, sensitive=true language=JavaScript, backgroundcolor=lightgray, extendedchars=true, basicstyle=, showstringspaces=false, showspaces=false, numbers=left, numberstyle=, numbersep=9pt, tabsize=2, breaklines=true, showtabs=false, captionpos=b setfontsize8.314158.1828 LLaMA : Open and Efficient Foundation Language Models s Hugo Touvron~~Equal contribution. Correspondence: htouvron, thibautlav,gizacard,egrave,glample@meta.com,~Thibaut Lavril[1], ~Gautier Izacard[1],~Xavier Martinet Marie-Anne Lachaux,~Timothee Lacroix,~Baptiste Rozière,~Naman Goyal Eric Hambro,~Faisal Azhar,~Aurelien Rodriguez,~Armand Joulin Edouard Grave[1], Guillaume Lample[1] -.5em s Meta AI =-1 We introduce , a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, -13B outperforms GPT-3 (175B) on most benchmarks, and -65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community. Introduction =-1 Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples~. These few-shot properties first appeared when scaling models to a sufficient size~, resulting in a line of work that focuses on further scaling these models~. These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from~ shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. =-1 The objective of the scaling laws from is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens. =-1 The focus of this work is to train a series of language models that achieve the best possible performance at various inference budgets, by training on more tokens than what is typically used. The resulting models, called , ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, -13B outperforms GPT-3 on most benchmarks, despite being 10 smaller. We believe that this model will help democratize the access and study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter model is also competitive with the best large language models such as Chinchilla or PaLM-540B. =-1 Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g. ``Books -- 2TB'' or ``Social media conversations''). There exist some exceptions, notably OPT~, GPT-NeoX~, BLOOM~ and GLM~, but none that are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview of the modifications we made to the transformer architecture~, as well as our training method. We then report the performance of our models and compare with others LLMs on a set of standard benchmarks. Finally, we expose some of the biases and toxicity encoded in our models, using some of the most recent benchmarks from the responsible AI community. Approach Our training approach is similar to the methods described in previous work~, and is inspired by the Chinchilla scaling laws~. We train large transformers on a large quantity of textual data using a standard optimizer. Pre-training Data Our training dataset is a mixture of several sources, reported in Table~, that cover a diverse set of domains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of data and the percentage they represent in the training set: English CommonCrawl [67We preprocess five CommonCrawl dumps, ranging from 2017 to 2020, with the CCNet pipeline~. This process deduplicates the data at the line level, performs language identification with a fastText linear classifier to remove non-English pages and filters low quality content with an n-gram language model. In addition, we trained a linear model to classify pages used as references in Wikipedia v.s. randomly sampled pages, and discarded pages not classified as references. C4 [15During exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance. We thus included the publicly available C4 dataset~ in our data. The preprocessing of C4 also contains deduplication and language identification steps: the main difference with CCNet is the quality filtering, which mostly relies on heuristics such as presence of punctuation marks or the number of words and sentences in a webpage. Github [4.5We use the public GitHub dataset available on Google BigQuery. We only kept projects that are distributed under the Apache, BSD and MIT licenses. Additionally, we filtered low quality files with heuristics based on the line length or proportion of alphanumeric characters, and removed boilerplate, such as headers, with regular expressions. Finally, we deduplicate the resulting dataset at the file level, with exact matches. Wikipedia [4.5We add Wikipedia dumps from the June-August 2022 period, covering 20 languages, which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. We process the data to remove hyperlinks, comments and other formatting boilerplate. Gutenberg and Books3 [4.5We include two book corpora in our training dataset: the Gutenberg Project, which contains books that are in the public domain, and the Books3 section of ThePile~, a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90ArXiv [2.5We process arXiv Latex files to add scientific data to our dataset. Following , we removed everything before the first section, as well as the bibliography. We also removed the comments from the .tex files, and inline-expanded definitions and macros written by users to increase consistency across papers. Stack Exchange [2We include a dump of Stack Exchange, a website of high quality questions and answers that covers a diverse set of domains, ranging from computer science to chemistry. We kept the data from the 28 largest websites, removed the HTML tags from text and sorted the answers by score (from highest to lowest). Tokenizer. We tokenize the data with the byte-pair encoding (BPE) algorithm~, using the implementation from SentencePiece~. Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once during training, with the exception of the Wikipedia and Books domains, over which we perform approximately two epochs. Architecture Following recent work on large language models, our network is based on the transformer architecture~. We leverage various improvements that were subsequently proposed, and used in different models such as PaLM. Here are the main difference with the original architecture, and where we were found the inspiration for this change (in bracket): Pre-normalization [GPT3]. To improve the training stability, we normalize the input of each transformer sub-layer, instead of normalizing the output. We use the RMSNorm normalizing function, introduced by . SwiGLU activation function [PaLM]. We replace the ReLU non-linearity by the SwiGLU activation function, introduced by to improve the performance. We use a dimension of instead of as in PaLM. Rotary Embeddings [GPTNeo].-3ptWe remove the absolute positional embeddings, and instead, add rotary positional embeddings (RoPE), introduced by , at each layer of the network. The details of the hyper-parameters for our different models are given in Table~. Optimizer Our models are trained using the AdamW optimizer~, with the following hyper-parameters: . We use a cosine learning rate schedule, such that the final learning rate is equal to 10We use a weight decay of and gradient clipping of . We use warmup steps, and vary the learning rate and batch size with the size of the model (see Table~ for details). Efficient implementation We make several optimizations to improve the training speed of our models. First, we use an efficient implementation of the causal multi-head attention to reduce memory usage and runtime. This implementation, available in the xformers library, is inspired by~ and uses the backward from~. This is achieved by not storing the attention weights and not computing the key/query scores that are masked due to the causal nature of the language modeling task. To further improve training efficiency, we reduced the amount of activations that are recomputed during the backward pass with checkpointing. More precisely, we save the activations that are expensive to compute, such as the outputs of linear layers. This is achieved by manually implementing the backward function for the transformer layers, instead of relying on the PyTorch autograd. To fully benefit from this optimization, we need to reduce the memory usage of the model by using model and sequence parallelism, as described by . Moreover, we also overlap the computation of activations and the communication between GPUs over the network (due to allreduce operations) as much as possible. When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days. Main results Following previous work~, we consider zero-shot and few-shot tasks, and report results on a total of 20 benchmarks: -shot. We provide a textual description of the task and a test example. The model either provides an answer using open-ended generation, or ranks the proposed answers. -shot. We provide a few examples of the task (between 1 and 64) and a test example. The model takes this text as input and generates the answer or ranks different options. We compare with other foundation models, namely the non-publicly available language models GPT-3~, Gopher~, Chinchilla~ and PaLM~, as well as the open-sourced OPT models~, GPT-J~, and GPT-Neo~. In Section~, we also briefly compare with instruction-tuned models such as OPT-IML~ and Flan-PaLM~. We evaluate on free-form generation tasks and multiple choice tasks. In the multiple choice tasks, the objective is to select the most appropriate completion among a set of given options, based on a provided context. We select the completion with the highest likelihood given the provided context. We follow and use the likelihood normalized by the number of characters in the completion, except for certain datasets (OpenBookQA, BoolQ), for which we follow , and select a completion based on the likelihood normalized by the likelihood of the completion given ``Answer:'' as context: . Common Sense Reasoning We consider eight standard common sense reasoning benchmarks: BoolQ~, PIQA~, SIQA~, HellaSwag~, WinoGrande~, ARC easy and challenge~ and OpenBookQA~. These datasets include Cloze and Winograd style tasks, as well as multiple choice question answering. We evaluate in the zero-shot setting as done in the language modeling community. In Table~, we compare with existing models of various sizes and report numbers from the corresponding papers. First, -65B outperforms Chinchilla-70B on all reported benchmarks but BoolQ. Similarly, this model surpasses PaLM-540B everywhere but on BoolQ and WinoGrande. -13B model also outperforms GPT-3 on most benchmarks despite being 10 smaller. Closed-book Question Answering We compare to existing large language models on two closed-book question answering benchmarks: Natural Questions~ and TriviaQA~. For both benchmarks, we report exact match performance in a closed book setting, i.e., where the models do not have access to documents that contain evidence to answer the question. In Table~, we report performance on NaturalQuestions, and in Table~, we report on TriviaQA. On both benchmarks, -65B achieve state-of-the-arts performance in the zero-shot and few-shot settings. More importantly, the -13B is also competitive on these benchmarks with GPT-3 and Chinchilla, despite being 5-10 smaller. This model runs on a single V100 GPU during inference. Reading Comprehension We evaluate our models on the RACE reading comprehension benchmark~. This dataset was collected from English reading comprehension exams designed for middle and high school Chinese students. We follow the evaluation setup from~ and report results in Table~. On these benchmarks, -65B is competitive with PaLM-540B, and, -13B outperforms GPT-3 by a few percents. Mathematical reasoning We evaluate our models on two mathematical reasoning benchmarks: MATH~ and GSM8k~. MATH is a dataset of 12K middle school and high school mathematics problems written in LaTeX. GSM8k is a set of middle school mathematical problems. In Table~, we compare with PaLM and Minerva~. Minerva is a series of PaLM models finetuned on 38.5B tokens extracted from ArXiv and Math Web Pages, while neither PaLM or are finetuned on mathematical data. The numbers for PaLM and Minerva are taken from~, and we compare with and without maj1@k. maj1@k denotes evaluations where we generate samples for each problem and perform a majority voting~. On GSM8k, we observe that -65B outperforms Minerva-62B, although it has not been fine-tuned on mathematical data. Code generation We evaluate the ability of our models to write code from a natural language description on two benchmarks: HumanEval~ and MBPP~. For both tasks, the model receives a description of the program in a few sentences, as well as a few input-output examples. In HumanEval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a docstring. The model needs to generate a Python program that fits the description and satisfies the test cases. In Table~, we compare the pass@1 scores of our models with existing language models that have not been finetuned on code, namely PaLM and LaMDA~. PaLM and were trained on datasets that contain a similar number of code tokens. As show in Table~, for a similar number of parameters, outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as~ to obtain unbiased estimates of the pass@k. It is possible to improve the performance on code by finetuning on code-specific tokens. For instance, PaLM-Coder~ increases the pass@1 score of PaLM on HumanEval from 26.2Other models trained specifically for code also perform better than general models on these tasks~. Finetuning on code tokens is beyond the scope of this paper. Massive Multitask Language Understanding The massive multitask language understanding benchmark, or MMLU, introduced by consists of multiple choice questions covering various domains of knowledge, including humanities, STEM and social sciences. We evaluate our models in the 5-shot setting, using the examples provided by the benchmark, and report results in Table~. On this benchmark, we observe that the -65B is behind both Chinchilla-70B and PaLM-540B by a few percent in average, and across most domains. A potential explanation is that we have used a limited amount of books and academic papers in our pre-training data, i.e., ArXiv, Gutenberg and Books3, that sums up to only 177GB, while these models were trained on up to 2TB of books. This large quantity of books used by Gopher, Chinchilla and PaLM may also explain why Gopher outperforms GPT-3 on this benchmark, while it is comparable on other benchmarks. Evolution of performance during training =-1 During training, we tracked the performance of our models on a few question answering and common sense benchmarks, and report them in Figure~. On most benchmarks, the performance improves steadily, and correlates with the training perplexity of the model (see Figure~). The exceptions are SIQA and WinoGrande. Most notably, on SIQA, we observe a lot of variance in performance, that may indicate that this benchmark is not reliable. On WinoGrande, the performance does not correlate as well with training perplexity: the -33B and -65B have similar performance during the training. Instruction Finetuning In this section, we show that briefly finetuning on instructions data rapidly leads to improvements on MMLU. Although the non-finetuned version of -65B is already able to follow basic instructions, we observe that a very small amount of finetuning improves the performance on MMLU, and further improves the ability of the model to follow instructions. Since this is not the focus of this paper, we only conducted a single experiment following the same protocol as~ to train an instruct model, -I. In Table~, we report the results of our instruct model -I on MMLU and compare with existing instruction finetuned models of moderate sizes, namely, OPT-IML~ and the Flan-PaLM series~. All the reported numbers are from the corresponding papers. Despite the simplicity of the instruction finetuning approach used here, we reach 68.9-I (65B) outperforms on MMLU existing instruction finetuned models of moderate sizes, but are still far from the state-of-the-art, that is 77.4 for GPT code-davinci-002 on MMLU (numbers taken from~). The details of the performance on MMLU on the 57 tasks can be found in Table~ of the appendix. Carbon footprint The training of our models have consumed a massive quantity of energy, responsible for the emission of carbon dioxide. We follow the recent literature on the subject and breakdown both the total energy consumption and the resulting carbon footprint in Table~. We follow a formula for to estimate the Watt-hour, Wh, needed to train a model, as well as the tons of carbon emissions, tCOeq. For the Wh, we use the formula: where we set the Power Usage Effectiveness (PUE) at . The resulting carbon emission depends on the location of the data center used to train the network. For instance, BLOOM uses a grid that emits 0.057 kg COeq/KWh leading to 27 tCOeq and OPT a grid that emits 0.231 kg COeq/KWh, leading to 82 tCOeq. In this study, we are interested in comparing the cost in carbon emission of training of these models if they were trained in the same data center. Hence, we do not take the location of data center in consideration, and use, instead, the US national average carbon intensity factor of 0.385 kg COeq/KWh. This leads to the following formula for the tons of carbon emissions: We apply the same formula to OPT and BLOOM for fair comparison. For OPT, we assume training required 34 days on 992 A100-80B (see their logs). Finally, we estimate that we used 2048 A100-80GB for a period of approximately 5 months to develop our models. This means that developing these models would have cost around 2,638 MWh under our assumptions, and a total emission of 1,015 tCOeq. We hope that releasing these models will help to reduce future carbon emission since the training is already done, and some of the models are relatively small and can be run on a single GPU. Related work Language models are probability distributions over sequences of words, tokens or characters~. This task, often framed as next token prediction, has long been considered a core problem in natural language processing~. Because proposed to measure machine intelligence by using language through the ``imitation game'', language modeling has been proposed as a benchmark to measure progress toward artificial intelligence~. Architecture. Traditionally, language models were based on -gram count statistics~, and various smoothing techniques were proposed to improve the estimation of rare events~. In the past two decades, neural networks have been successfully applied to the language modelling task, starting from feed forward models~, recurrent neural networks~ and LSTMs~. More recently, transformer networks, based on self-attention, have led to important improvements, especially for capturing long range dependencies~. Scaling. There is a long history of scaling for language models, for both the model and dataset sizes. showed the benefits of using language models trained on 2 trillion tokens, resulting in 300 billion -grams, on the quality of machine translation. While this work relied on a simple smoothing technique, called Stupid Backoff, later showed how to scale Kneser-Ney smoothing to Web-scale data. This allowed to train a 5-gram model on 975 billions tokens from CommonCrawl, resulting in a model with 500 billions -grams~. introduced the One Billion Word benchmark, a large scale training dataset to measure the progress of language models. In the context of neural language models, obtained state-of-the-art results on the Billion Word benchmark by scaling LSTMs to 1 billion parameters. Later, scaling transformers lead to improvement on many NLP tasks. Notable models include BERT~, GPT-2~, Megatron-LM~, and T5~. A significant breakthrough was obtained with GPT-3~, a model with 175 billion parameters. This lead to a series of Large Language Models, such as Jurassic-1~, Megatron-Turing NLG~, Gopher~, Chinchilla~, PaLM~, OPT~, and GLM~. and studied the impact of scaling on the performance of deep learning models, showing the existence of power laws between the model and dataset sizes and the performance of the system. derived power laws specifically for transformer based language models, which were later refined by , by adapting the learning rate schedule when scaling datasets. Finally, studied the effect of scaling on the abilities of large language models. Conclusion =-1 In this paper, we presented a series of language models that are released openly, and competitive with state-of-the-art foundation models. Most notably, -13B outperforms GPT-3 while being more than 10 smaller, and -65B is competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible to achieve state-of-the-art performance by training exclusively on publicly available data, without resorting to proprietary datasets. We hope that releasing these models to the research community will accelerate the development of large language models, and help efforts to improve their robustness and mitigate known issues such as toxicity and bias. Additionally, we observed like that finetuning these models on instructions lead to promising results, and we plan to further investigate this in future work. Finally, we plan to release larger models trained on larger pretraining corpora in the future, since we have seen a constant improvement in performance as we were scaling. Acknowledgements =-1 We thank Daniel Haziza, Francisco Massa, Jeremy Reizenstein, Artem Korenev, and Patrick Labatut from the xformers team. We thank Susan Zhang and Stephen Roller for their support on data deduplication. We thank Luca Wehrstedt, Vegard Mella, and Pierre-Emmanuel Mazaré for their support on training stability. We thank Shubho Sengupta, Kalyan Saladi, and all the AI infra team for their support. We thank Jane Yu for her input on evaluation. We thank Yongyi Hu for his help on data collection.",
    "idea_abstract": "We propose a novel approach to training large language models using publicly available datasets, challenging the notion that proprietary data is necessary for achieving state-of-the-art performance. Our method leverages a range of model sizes to demonstrate the feasibility of training high-quality models without relying on inaccessible data.",
    "proposal": "1. Title: Training Large Language Models on Publicly Available Data\n\n2. Problem Statement: The problem we aim to address is the need for large language models that can achieve state-of-the-art performance without relying on proprietary and inaccessible datasets, making them inaccessible to the research community.\n\n3. Motivation: Existing methods rely on large, proprietary datasets, which limits their accessibility and reproducibility. Our approach is inspired by the Chinchilla scaling laws, but we focus on training models on publicly available data, which is essential for democratizing access to large language models.\n\n4. Proposed Method: We train large transformers on a diverse set of publicly available datasets, including CommonCrawl, C4, GitHub, Wikipedia, Gutenberg, and ArXiv. We preprocess the data using a combination of deduplication, language identification, and quality filtering. We use a modified transformer architecture with pre-normalization, SwiGLU activation function, and rotary embeddings. We train our models using the AdamW optimizer with a cosine learning rate schedule and gradient clipping. We also implement efficient training techniques, such as checkpointing and model parallelism, to reduce training time and energy consumption."
  },
  {
    "rank": 2,
    "title": "GPT-4 Technical Report",
    "arxiv": "2303.08774v3",
    "paper_tar": "tar_files/2303.08774v3.tar.gz",
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
    "content": "article numbers,compressnatbib [final]neurips_2021 [utf8]inputenc [T1]fontenc [hidelinks]hyperref url booktabs amsfonts nicefrac microtype xcolor graphicx longtable caption mdframed subcaption multirow placeins multicol makecell [normalem]ulem wrapfig [percent]overpic lipsum csquotes [OT2,T1]fontenc [english]babel devanagari tablefootnote pdfpages [table]skip=8pt macros [ font=, linewidth=0.5pt, innerleftmargin=10pt, innerrightmargin=10pt, innertopmargin=10pt, innerbottommargin=10pt, ]monobox [1] [1] #1 [1]#1[] [2]#1#2 [2]#1#2 [1]brown(David Farhi: #1) [1]pink(Adrien Ecoffet: #1) [1]teal(Jie Tang: #1) [1]orange(Jakub: #1) [1]purple(Miles: #1) [1]green(Szymon: #1) [1]violet(Tong: #1) [2]red(#1: #2) [1]red(TODO: #1) [1]red#1 [1]blue#1 2mm GPT-4 Technical Report OpenAIPlease cite this work as ``OpenAI (2023)\". Full authorship contribution statements appear at the end of the document. Correspondence regarding this technical report can be sent to gpt4-report@openai.com We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4. Introduction This technical report presents GPT-4, a large multimodal model capable of processing image and text inputs and producing text outputs. Such models are an important area of study as they have the potential to be used in a wide range of applications, such as dialogue systems, text summarization, and machine translation. As such, they have been the subject of substantial interest and progress in recent years~. One of the main goals of developing such models is to improve their ability to understand and generate natural language text, particularly in more complex and nuanced scenarios. To test its capabilities in such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In these evaluations it performs quite well and often outscores the vast majority of human test takers. For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering). On the MMLU benchmark~, an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4 surpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these model capability results, as well as model safety improvements and results, in more detail in later sections. This report also discusses a key challenge of the project, developing deep learning infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to make predictions about the expected performance of GPT-4 (based on small runs trained in similar ways) that were tested against the final run to increase confidence in our training. Despite its capabilities, GPT-4 has similar limitations to earlier GPT models~: it is not fully reliable (e.g. can suffer from ``hallucinations''), has a limited context window, and does not learn from experience. Care should be taken when using the outputs of GPT-4, particularly in contexts where reliability is important. GPT-4's capabilities and limitations create significant and novel safety challenges, and we believe careful study of these challenges is an important area of research given the potential societal impact. This report includes an extensive [systemcard]system card (after the Appendix) describing some of the risks we foresee around bias, disinformation, over-reliance, privacy, cybersecurity, proliferation, and more. It also describes interventions we made to mitigate potential harms from the deployment of GPT-4, including adversarial testing with domain experts, and a model-assisted safety pipeline. Scope and Limitations of this Technical Report This report focuses on the capabilities, limitations, and safety properties of GPT-4. GPT-4 is a Transformer-style model~ pre-trained to predict the next token in a document, using both publicly available data (such as internet data) and data licensed from third-party providers. The model was then fine-tuned using Reinforcement Learning from Human Feedback (RLHF)~. Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar. We are committed to independent auditing of our technologies, and shared some initial steps and ideas in this area in the system card accompanying this release. We plan to make further technical details available to additional third parties who can advise us on how to weigh the competitive and safety considerations above against the scientific value of further transparency. Predictable Scaling A large focus of the GPT-4 project was building a deep learning stack that scales predictably. The primary reason is that for very large training runs like GPT-4, it is not feasible to do extensive model-specific tuning. To address this, we developed infrastructure and optimization methods that have very predictable behavior across multiple scales. These improvements allowed us to reliably predict some aspects of the performance of GPT-4 from smaller models trained using -- less compute. Loss Prediction The final loss of properly-trained large language models is thought to be well approximated by power laws in the amount of compute used to train the model~. To verify the scalability of our optimization infrastructure, we predicted GPT-4’s final loss on our internal codebase (not part of the training set) by fitting a scaling law with an irreducible loss term (as in~): from models trained using the same methodology but using at most 10,000x less compute than GPT-4. This prediction was made shortly after the run started, without use of any partial results. The fitted scaling law predicted GPT-4's final loss with high accuracy (Figure ). Scaling of Capabilities on HumanEval Having a sense of the capabilities of a model before training can improve decisions around alignment, safety, and deployment. In addition to predicting final loss, we developed methodology to predict more interpretable metrics of capability. One such metric is pass rate on the HumanEval dataset~, which measures the ability to synthesize Python functions of varying complexity. We successfully predicted the pass rate on a subset of the HumanEval dataset by extrapolating from models trained with at most less compute (Figure ). For an individual problem in HumanEval, performance may occasionally worsen with scale. Despite these challenges, we find an approximate power law relationship where and are positive constants, and is a subset of problems in the dataset. We hypothesize that this relationship holds for all problems in this dataset. In practice, very low pass rates are difficult or impossible to estimate, so we restrict to problems and models such that given some large sample budget, every problem is solved at least once by every model. We registered predictions for GPT-4’s performance on HumanEval before training completed, using only information available prior to training. All but the 15 hardest HumanEval problems were split into 6 difficulty buckets based on the performance of smaller models. The results on the easiest bucket are shown in Figure , showing that the resulting predictions were very accurate for this subset of HumanEval problems where we can accurately estimate for several smaller models. Predictions on the other five buckets performed almost as well, the main exception being GPT-4 underperforming our predictions on the easiest bucket. Certain capabilities remain hard to predict. For example, the Inverse Scaling Prize~ proposed several tasks for which model performance decreases as a function of scale. Similarly to a recent result by~, we find that GPT-4 reverses this trend, as shown on one of the tasks called Hindsight Neglect~ in Figure . We believe that accurately predicting future capabilities is important for safety. Going forward we plan to refine these methods and register performance predictions across various capabilities before large model training begins, and we hope this becomes a common goal in the field. Capabilities footnote []For AMC 10 and AMC 12 2022 exams, the human percentiles are not yet published, so the reported numbers are extrapolated and likely have wide uncertainty. See Appendix . We tested GPT-4 on a diverse set of benchmarks, including simulating exams that were originally designed for humans. We did no specific training for these exams. A minority of the problems in the exams were seen by the model during training; for each exam we run a variant with these questions removed and report the lower score of the two. We believe the results to be representative. For further details on contamination (methodology and per-exam statistics), see Appendix . Exams were sourced from publicly-available materials. Exam questions included both multiple-choice and free-response questions; we designed separate prompts for each format, and images were included in the input for questions which required it. The evaluation setup was designed based on performance on a validation set of exams, and we report final results on held-out test exams. Overall scores were determined by combining multiple-choice and free-response question scores using publicly available methodologies for each exam. We estimate and report the percentile each overall score corresponds to. See Appendix~ for further details on the exam evaluation methodology. GPT-4 exhibits human-level performance on the majority of these professional and academic exams. Notably, it passes a simulated version of the Uniform Bar Examination with a score in the top 10The model's capabilities on exams appear to stem primarily from the pre-training process and are not significantly affected by RLHF. On multiple choice questions, both the base GPT-4 model and the RLHF model perform equally well on average across the exams we tested (see Appendix~). We also evaluated the pre-trained base GPT-4 model on traditional benchmarks designed for evaluating language models. For each benchmark we report, we ran contamination checks for test data appearing in the training set (see Appendix~ for full details on per-benchmark contamination). We used few-shot prompting for all benchmarks when evaluating GPT-4. GPT-4 considerably outperforms existing language models, as well as previously state-of-the-art (SOTA) systems which often have benchmark-specific crafting or additional training protocols (Table~). Many existing ML benchmarks are written in English. To gain an initial understanding of GPT-4's capabilities in other languages, we translated the MMLU benchmark~ -- a suite of multiple-choice problems spanning 57 subjects -- into a variety of languages using Azure Translate (see Appendix~ for example translations and prompts). We find that GPT-4 outperforms the English-language performance of GPT 3.5 and existing language models (Chinchilla~ and PaLM~) for the majority of languages we tested, including low-resource languages such as Latvian, Welsh, and Swahili (Figure~). GPT-4 substantially improves over previous models in the ability to follow user intent~. On a dataset of 5,214 prompts submitted to ChatGPT~ and the OpenAI API~, the responses generated by GPT-4 were preferred over the responses generated by GPT-3.5 on <>$ The answer is therefore [<MODEL ANSWER CHOICE (t=0.0, n=1, stop=‘]’) SAMPLED HERE>] Example prompt for a free-response question In the example prompt below, the task prompt would be replaced by a prompt like an official sample GRE essay task, and the essay response with an example of a high-scoring essay . <|endofreply|>Analytical Writing: Issue Essay <TEXT OF SAMPLE ISSUE TASK PROMPT> Response:<|endofprompt|><TEXT OF SAMPLE ISSUE TASK ESSAY RESPONSE – SCORE 6><|endofreply|> <FREE-RESPONSE PROMPT TEXT GOES HERE> Response:<|endofprompt|> (<MODEL ANSWER TEXT (t=0.6, n=1, stop=‘<|endofreply|>’) SAMPLED HERE> Impact of RLHF on capability To test the impact of RLHF on the capability of our base model, we ran the multiple-choice question portions of our exam benchmark on the GPT-4 base model and the post RLHF GPT-4 model. The results are shown in Table~. Averaged across all exams, the base model achieves a score of 73.7For free-response questions, it is difficult to compare the base and RLHF models on an even footing, as our methodology for sampling free-response answers likely benefits from the model's ability to do instruction following. Contamination on professional and academic exams We measure cross-contamination between our evaluation dataset and the pre-training data using substring match. Both evaluation and training data are processed by removing all spaces and symbols, keeping only characters (including numbers). For each evaluation example, we randomly select three substrings of 50 characters (or use the entire example if it's less than 50 characters). A match is identified if any of the three sampled evaluation substrings is a substring of the processed training example. This yields a list of contaminated examples. We discard these and rerun to get uncontaminated scores. Our filtering approach has some limitations. Our substring match can result in false negatives (if there is a small difference between the evaluation and training data) as well as false positives. We only use partial information from the evaluation examples, utilizing just the question, context, or equivalent data while ignoring answer, response, or equivalent data. In some cases, the multiple-choice options are also excluded. These exclusions may lead to an increase in false positives. The RLHF post-training dataset is vastly smaller than the pretraining set and unlikely to have any particular question contaminated. However we did not check explicitly. As can be seen in tables and , contamination overall has very little effect on the reported results. Contamination on academic benchmarks We measure cross-contamination between academic benchmarks and the pre-training data similarly to the methodology presented in Appendix~. Results are presented in Table~. GSM-8K in GPT-4 training To improve GPT-4's ability to do mathematical reasoning, we mixed in data from the training set of MATH and GSM-8K, two commonly studied benchmarks for mathematical reasoning in language models. The total number of tokens drawn from these math benchmarks was a tiny fraction of the overall GPT-4 training budget. When mixing in data from these math benchmarks, a portion of the training data was held back, so each individual training example may or may not have been seen by GPT-4 during training. We conducted contamination checking to verify the test set for GSM-8K is not included in the training set (see Appendix ~). We recommend interpreting the performance results reported for GPT-4 GSM-8K in Table~ as something in-between true few-shot transfer and full benchmark-specific tuning. Multilingual MMLU We translated all questions and answers from MMLU~ using Azure Translate. We used an external model to perform the translation, instead of relying on GPT-4 itself, in case the model had unrepresentative performance for its own translations. We selected a range of languages that cover different geographic regions and scripts, we show an example question taken from the astronomy category translated into Marathi, Latvian and Welsh in Table~. The translations are not perfect, in some cases losing subtle information which may hurt performance. Furthermore some translations preserve proper nouns in English, as per translation conventions, which may aid performance. We incorporated the same MMLU prompt as~, the model is instructed that it is an intelligent agent, supplied with the questions and a list of four answer options labelled `A-D', followed by `Answer:'. We translate the model instruction, question and answers, however preserve the `Answer' token along with the `A-D' options in English. An example prompt is shown in Table~. The prompts are composed three-shot, with the three examples picked from the development set. We use three-shot evaluation over the regular five-shot because some languages map to much longer token sequences. Finally we classify the correct answer by picking the A-D token continuation with the highest probability from the model. Examples of GPT-4 Visual Input System Card The System Card~ for GPT-4 is appended to this document. [pages=-]assets/GPT_4_System_Card.pdf",
    "idea_abstract": "We propose a large-scale, multimodal model that can process and respond to both image and text inputs, with the potential to achieve human-level performance on various professional and academic tasks. The model is based on a Transformer architecture and is designed to learn from large datasets. A key challenge in developing such a model is creating infrastructure and optimization methods that can scale predictably, allowing for more efficient training and evaluation.",
    "proposal": "1. Title: Developing a Large-Scale Multimodal Model for Natural Language Understanding and Generation\n\n2. Problem Statement: Developing a model that can understand and generate natural language text in a way that is comparable to human performance is a challenging problem in natural language processing. Current models have limitations in understanding complex and nuanced scenarios, and there is a need for a model that can perform well on a wide range of tasks and languages.\n\n3. Motivation: Existing models have limitations in understanding and generating natural language text, particularly in complex and nuanced scenarios. Current models often require extensive model-specific tuning and have limited scalability. There is a need for a model that can perform well on a wide range of tasks and languages, and can be scaled predictably.\n\n4. Proposed Method: We propose a large-scale multimodal model that can accept image and text inputs and produce text outputs. The model is pre-trained to predict the next token in a document, using both publicly available data and data licensed from third-party providers. The model is then fine-tuned using Reinforcement Learning from Human Feedback (RLHF). We also develop infrastructure and optimization methods that have predictable behavior across multiple scales, allowing us to make predictions about the expected performance of the model before training."
  },
  {
    "rank": 3,
    "title": "PaLM 2 Technical Report",
    "arxiv": "2305.10403v1",
    "paper_tar": "tar_files/2305.10403v1.tar.gz",
    "abstract": "abstract",
    "content": "article [margin=1in]geometry microtype graphicx subfigure [subfigure]tocloft booktabs amsmath amssymb amsfonts multirow verbatim caption longtable float CJKutf8 kotex threeparttable vwcol mathptmx OT6cmr [OT6,T1]fontenc enumitem tablefootnote [round,semicolon]natbib xcolor xspace textcomp makecell lscape siunitx geometry [most]tcolorbox [bottom]footmisc .02em0.3em0.2em 2em 0em 0.7em amssymb pifont 51 55 scrextend [1]blue#1 [1]red#1 [1]gray#1 [1] [0]PaLM [0]PaLM-Coder [0]PaLM Coder [0]PaLM [1]blue#1 ExtraPythonData hyperref darkbluergb0, 0.2, 0.7 colorlinks = true, linkcolor = darkblue, anchorcolor = darkblue, citecolor = darkblue, filecolor = darkblue, urlcolor = darkblue soul xcolor [colorinlistoftodos,prependcaption,textsize=tiny]todonotes sm [1] sm [color=yellow, size=] [sm:] #1 adai [1] adai [color=yellow, size=] [adai:] #1 [1]redChris: #1 ruder [1] ruder [color=olive, size=] [ruder:] #1 kevinrobinson [1] kevinrobinson [color=orange, size=] [kevinrobinson:] #1 cleveref sectionSectionSections subsectionSectionSections tableTableTables figureFigureFigures algorithmAlgorithm equationeq. appendixAppendix multicol algorithm softmax concat LayerNorm lightgrayrgb0.95, 0.95, 0.95 darkgrayrgb0.4, 0.4, 0.4 editorGrayrgb0.95, 0.95, 0.95 editorOcherrgb1, 0.5, 0 editorGreenrgb0, 0.5, 0 orangergb1,0.45,0.13 olivergb0.17,0.59,0.20 brownrgb0.69,0.31,0.31 purplergb0.38,0.18,0.81 lightbluergb0.1,0.57,0.7 lightredrgb1,0.4,0.5 upquote listings CSS keywords=color,background-image:,margin,padding,font,weight,display,position,top,left,right,bottom,list,style,border,size,white,space,min,width, transition:, transform:, transition-property, transition-duration, transition-timing-function, sensitive=true, morecomment=[l]//, morecomment=[s]/**/, morestring=[b]', morestring=[b]\", alsoletter=:, alsodigit=- JavaScript morekeywords=typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break, morecomment=[s]/**/, morecomment=[l]//, morestring=[b]\", morestring=[b]' HTML5 language=html, sensitive=true, alsoletter=<>=-, morecomment=[s]<!--->, tag=[s], otherkeywords= >, <!DOCTYPE, </html, <html, <head, <title, </title, <style, </style, <link, </head, <meta, />, </body, <body, </div, <div, </div>, </p, <p, </p>, </script, <script, <canvas, /canvas>, <svg, <rect, <animateTransform, </rect>, </svg>, <video, <source, <iframe, </iframe>, </video>, <image, </image>, <header, </header, <article, </article , ndkeywords= =, charset=, src=, id=, width=, height=, style=, type=, rel=, href=, fill=, attributeName=, begin=, dur=, from=, to=, poster=, controls=, x=, y=, repeatCount=, xlink:href=, margin:, padding:, background-image:, border:, top:, left:, position:, width:, height:, margin-top:, margin-bottom:, font-size:, line-height:, transform:, -moz-transform:, -webkit-transform:, animation:, -webkit-animation:, transition:, transition-duration:, transition-property:, transition-timing-function:, htmlcssjs basicstyle=, frame=b, xleftmargin=0.75cm, numbers=left, stepnumber=1, firstnumber=1, numberfirstline=true, identifierstyle=black, keywordstyle=blue, ndkeywordstyle=editorGreen, stringstyle=editorOcher, commentstyle=brown, language=HTML5, alsolanguage=JavaScript, alsodigit=.:;, tabsize=2, showtabs=false, showspaces=false, showstringspaces=false, extendedchars=true, breaklines=true, literate= ÖO1 ÄA1 ÜU1 ß1 üu1 äa1 öo1 py language=python, literate= *0lightred01 1lightred11 2lightred21 3lightred31 4lightred41 5lightred51 6lightred61 7lightred71 8lightred81 9lightred91, basicstyle=, numbers=left, numbersep=5pt, tabsize=4, extendedchars=true, breaklines=true, keywordstyle=blue, frame=b, commentstyle=brown, stringstyle=editorOcher, showspaces=false, showtabs=false, xleftmargin=17pt, framexleftmargin=17pt, framexrightmargin=5pt, framexbottommargin=4pt, showstringspaces=false, devanagari -2em height 4pt 0.25in - PaLM 2 Technical Report 0.2in - height 1pt 0.09in top=20mm, bottom=20mm 5pt tocdepth2 Introduction Scaling law experiments Training dataset Evaluation Responsible usage Conclusion Authorship, attribution, and acknowledgements",
    "idea_abstract": "Question:\nWhat is the remainder when 136 is divided by 22?\nAnswer:\n14",
    "proposal": "Here is the rewritten idea abstract:\n\n**Title:** Improving Language Model Performance\n\n**Problem Statement:** Current language models struggle to generalize to unseen data and exhibit poor performance on tasks that require nuanced understanding and common sense.\n\n**Motivation:** Existing methods rely on large amounts of data and computational resources, but often fail to capture the complexities of human language. The proposed method aims to address these limitations by introducing a novel approach that leverages a more robust and efficient architecture.\n\n**Proposed Method:** The proposed method involves a multi-stage architecture that incorporates a novel attention mechanism and a more effective optimization strategy. The approach is designed to improve the model's ability to generalize and capture subtle nuances in language, leading to improved performance on a range of tasks."
  },
  {
    "rank": 4,
    "title": "Sparks of Artificial General Intelligence: Early Experiments with GPT-4",
    "arxiv": "2303.12712v5",
    "paper_tar": "tar_files/2303.12712v5.tar.gz",
    "abstract": "",
    "content": "[11pt]article =1 [ margin=1.5cm, includefoot, footskip=30pt, ]geometry figuresection tikz matrix, arrows amsmath,amssymb amsthm mathtools xspace [noend]algorithmic [ruled,vlined]algorithm2e url makeidx enumerate epstopdf booktabs color [utf8]inputenc thm-restate scalerel,stackengine [shortlabels]enumitem xr fancyvrb xcolor bold-extra [width=474.18663pt]caption subfigure [most]tcolorbox fvextra [frozencache=true, finalizecache=false, cachedir=./minted-cache]minted float alltt soul fancyvrb multirow [final]hyperref [bottom]footmisc vs GPT-4 listings basicstyle=, columns=flexible, breaklines=true, mathescape=true tikz shapes,calc,positioning 0pt aibox/.style= width=474.18663pt, top=10pt, colback=white, colframe=black, colbacktitle=black, enhanced, center, attach boxed title to top left=yshift=-0.1in,xshift=0.15in, boxed title style=boxrule=0pt,colframe=white,, AIbox[2][]aibox,title=#2,#1 aigoldRGB244,210, 1 aigreenRGB210,244,211 [1]aigreenaigreen#1 aigreen airedRGB255,180,181 [1]aired#1 [1][green]on line, arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black, boxsep=0pt,left=0pt,right=0pt,top=0pt,bottom=0pt, boxrule=0pt,bottomrule=0pt,toprule=0pt Sparks of Artificial General Intelligence: Early experiments with GPT-4 Sebastien Bubeck Varun Chandrasekaran Ronen Eldan Johannes Gehrke Eric Horvitz Ece Kamar Peter Lee Yin Tat Lee Yuanzhi Li Scott Lundberg Harsha Nori Hamid Palangi Marco Tulio Ribeiro Yi Zhang Microsoft Research Interaction with humans toctocdepth2",
    "idea_abstract": "Question:\nCalculate 0.1*0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "proposal": "Here is the rewritten idea abstract:\n\n1. **Unlocking Artificial General Intelligence: Exploring the Frontiers of GPT-4**\n\n2. Problem Statement: The quest for artificial general intelligence (AGI) has long been a holy grail in the field of artificial intelligence. AGI refers to the ability of a machine to perform any intellectual task that a human can, across a wide range of domains. Current AI systems, however, are narrow and specialized, struggling to generalize beyond their training data. The challenge lies in developing a system that can learn, reason, and apply knowledge across diverse domains, much like humans do.\n\n3. Motivation: Existing methods for developing AGI have been largely unsuccessful, and the field is in dire need of new approaches. The recent emergence of large language models, such as GPT-4, has sparked renewed interest in the possibility of AGI. However, these models are still far from achieving true general intelligence, and their limitations are not yet fully understood. Our research aims to explore the frontiers of GPT-4 and push the boundaries of what is currently possible.\n\n4. Proposed Method: We propose a novel approach to developing AGI by leveraging the capabilities of GPT-4 and exploring its potential for generalization. Our"
  },
  {
    "rank": 5,
    "title": "PaLM-E: An Embodied Multimodal Language Model",
    "arxiv": "2303.03378v1",
    "paper_tar": "tar_files/2303.03378v1.tar.gz",
    "abstract": "Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g.for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
    "content": "[nohyperref]article microtype graphicx lipsum cuted subfigure booktabs [utf8]inputenc newunicodechar graphicx ❄ hyperref multirow algorithm [accepted]not_icml2022 amsmath amssymb mathtools amsthm [font=small]caption pifont 51 55 [capitalize,noabbrev]cleveref sectionSec.Secs. sectionSectionSections figureFig.Figs. figureFigureFigures tableTableTables tableTab.Tabs. appendixApp.App. plain theoremTheorem[section] proposition[theorem]Proposition lemma[theorem]Lemma corollary[theorem]Corollary definition definition[theorem]Definition assumption[theorem]Assumption remark remark[theorem]Remark [textsize=tiny]todonotes teaser-greenHTML6aa84f teaser-blueHTML4a86e8 teaser-orangeHTMLff9900 xspace let@tokenonedot onedotlet@token.. e.g E.g i.e I.e cf Cf etc vs w.r.t d.o.f i.i.d w.l.o.g et al PaLM-E: An Embodied Multimodal Language Model [ -1.1cmPaLM-E: An Embodied Multimodal Language Model equal* -0.1cm Danny Driessrobotics,berlin Fei Xiarobotics Mehdi S. M. Sajjadigoogle Corey Lynchrobotics Aakanksha Chowdherygoogle Brian Ichterrobotics Ayzaan Wahidrobotics Jonathan Tompsonrobotics Quan Vuongrobotics Tianhe Yurobotics Wenlong Huangrobotics Yevgen Chebotarrobotics Pierre Sermanetrobotics Daniel Duckworthgoogle Sergey Levinerobotics Vincent Vanhouckerobotics Karol Hausmanrobotics Marc Toussaintberlin Klaus Greffgoogle Andy Zengrobotics Igor Mordatchgoogle Pete Florencerobotics[0.2cm] Robotics at Google TU Berlin Google Research [0.1cm] https://palm-e.github.io Robotics, Vision-Language Models, Language Models, Machine Learning 0.2in -0.5em figurePaLM-E is a single general-purpose multimodal language model for embodied reasoning tasks, visual-language tasks, and language tasks. PaLM-E transfers knowledge from visual-language domains into embodied reasoning -- from robot planning in environments with complex dynamics and physical constraints, to answering questions about the observable world. PaLM-E operates on multimodal sentences, sequences of tokens where inputs from arbitrary modalities ( images, neural 3D representations, or states, in teaser-greengreen and teaser-blueblue) are inserted alongside text tokens (in teaser-orangeorange) as input to an LLM, trained end-to-end. 0.5em ] Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g.for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale. Introduction -2mm -1mm Related Work General vision-language modeling. Building on successes in large language and vision models, recent years have seen a growing interest in large vision-language models (VLMs) . Unlike their predecessors, VLMs are capable of simultaneously understanding both images and text, and can be applied to tasks such as visual question answering , captioning , optical character recognition , and object detection . The methods by which images are integrated varies. For example, augments pretrained language models with a mechanism to directly attend to a single context image. In contrast, PaLM-E represents images and text as ``multimodal sentences'' of latent vectors, allowing it to process multiple images in a flexible way within any part of a sentence. More closely related to our work is Frozen~ where vision encoder parameters are optimized via backpropagation through a frozen LLM~. Inspired by this work, we investigate the design in a broader scope by introducing alternative input modalities ( neural scene representations), and our proposed approach empirically outperforms Frozen by more than (SL)^3ip(w_1:L)w_1:L = (w_1, , w_L)w_iWp_LMw_1:nw_1:nw_n+1:Lw_iWw_iXR^k : W Xp_LM(w_l|x_1:l-1)x_i = (w_i)R^k|W||W|=256000X : O X^qOqXx_i_iO_j_i : OXsR^Ss_states_ViTIx_1:m = _ViT(I)R^mkkx_i = _ViT(I)_i = (_ViT(I)_i)_jx_1:m^j = _ViT(M_j I)jo_j = _OSRT(I_1:v)_jR^kx_1:m^j = (_OSRT(I_1:v)_j) : R^kR^m km = (I_1:u_i^i, w_1:L_i^i, n_i )_i=1^Niu_iI_j^iw_1:L_i^in_in_iw_n_i+1:L_i^i_LM_OSRTq_2q_3q_2q_3q_4p_1p_2_1_jq_1$ is about the color of an object. The other tasks (except with the different prefix, and entity referrals), remain the same. We utilize the planner from to generate the dataset for the planning tasks. The low-level policies are also obtained with the method of . Interactive Language Table We use the Language-Table real-world tabletop setup and simulated environment from Interactive Language . Data collection. For each task, given the long horizon instruction, we prompt a labeler to enter a short horizon command every 4 seconds. We pass the short horizon instructions to an Interactive Language policy trained using the same procedure as in . The policy executes 40 steps (10Hz for 4 seconds) before requiring another command from the labeler. This is repeated until the labeler determines the long horizon instruction is complete and issues a 'done' instruction. The data collection procedure for the real world experiments are the same as in simulation. Train and Evaluation. To train the finetuned versions of these models, we train a pretrained PaLM-E model for 9,000 additional steps, in order to support a data complexity sweep without training several separate models from scratch on slightly different versions of the full mixture. For Tasks 2 and 3 in simulation, we implement an automated reward to measure the success rate, and we evaluate PaLM-E by running 80 rollouts for each task. Given the current image and high level task, PaLM-E issues a text instruction which a trained low-level policy executes for 4 seconds before PaLM-E issues a new text instruction. For Task 1, we use a test-set and report validation accuracy. This is because the task only requires one step to solve, despite being a complicated visual and linguistic processing task and cannot be solved by the low-level policy from the prompt alone. Natural Language Generation and Understanding Results Additional Data for Affordance and Success Detection Image Attribution The image of the New York Knicks and Boston Celtics in Figure 2 is under the terms CC-by-2.0 (https://creativecommons.org/licenses/by/2.0/), and was posted to Flickr by kowarski at https://www.flickr.com/photos/27728232@N00/8666371367. The egocentric video images are from https://youtu.be/-UXKmqBPk1w, as in , via permission from creator Cody Wanner.",
    "idea_abstract": "We propose a novel approach to grounding language models in the real world by directly incorporating continuous sensor modalities into language models. This enables the establishment of a link between words and percepts, allowing for more robust and generalizable language understanding. Our embodied language model integrates visual, state estimation, and textual input encodings to perform a range of tasks, including robotic manipulation planning, visual question answering, and captioning. By training these encodings end-to-end with a pre-trained large language model, we demonstrate the potential for a single multimodal model to address diverse embodied reasoning tasks across various observation modalities and embodiments, with positive transfer benefits from joint training across multiple domains.",
    "proposal": "1. Title: Embodied Multimodal Language Models for Real-World Reasoning\n\n2. Problem Statement: Current language models struggle to generalize to real-world tasks that require embodied reasoning, such as robotic manipulation and visual question answering. This is because they lack the ability to directly incorporate real-world continuous sensor modalities into their reasoning process.\n\n3. Motivation: Existing methods for vision-language modeling and multimodal learning have limitations in handling complex tasks that require embodied reasoning. They often rely on separate models for vision and language, or use fixed mechanisms for integrating visual and textual inputs. We propose a novel approach that directly incorporates real-world sensor modalities into language models, enabling them to reason about the physical world.\n\n4. Proposed Method: We introduce a single, general-purpose multimodal language model that can process multimodal sentences, where inputs from arbitrary modalities (images, neural 3D representations, or states) are inserted alongside text tokens. Our model is trained end-to-end with a pre-trained large language model, enabling it to learn to reason about the physical world and perform a variety of embodied tasks, including sequential robotic manipulation planning, visual question answering, and captioning."
  },
  {
    "rank": 6,
    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "arxiv": "2305.14314v1",
    "paper_tar": "tar_files/2305.14314v1.tar.gz",
    "abstract": "We present , an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.  backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name , outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\\",
    "content": "article [square,numbers]natbib [preprint]neurips_2023 [utf8]inputenc [T1]fontenc hyperref url booktabs amsfonts nicefrac microtype xcolor graphicx amsmath xspace [font=small,labelfont=bf,tableposition=top]caption =0pt plus 2pt =15pt colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan wrapfig [parfill]parskip QLoRA Guanaco Guanaco Double Quantization double quantization Paged Optimizers paged optimizers the Vicuna benchmark the OpenAssistant benchmark andtable#1~#2 ~ : Efficient Finetuning of Quantized LLMs Tim DettmersEqual contribution. Artidoro Pagnoni Ari Holtzman Luke Zettlemoyer University of Washington dettmers,artidoro,ahai,lsz@cs.washington.edu We present , an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name , outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3Introduction Finetuning large language models (LLMs) is a highly effective way to improve their performance, and to add desirable or remove undesirable behaviors . However, finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B parameter model~ requires more than 780 GB of GPU memory. While recent quantization methods can reduce the memory footprint of LLMs , such techniques only work for inference and break down during training . We demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any performance degradation. Our method, , uses a novel high-precision technique to quantize a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights r6.3cm -5pt Elo ratings for a competition between models, averaged for 10,000 random initial orderings. The winner of a match is determined by GPT-4 which declares which response is better for a given prompt of the . 95-10pt that are tuned by backpropagating gradients through the quantized weights. reduces the average memory requirements of finetuning a 65B parameter model from 780GB of GPU memory to 48GB without degrading the runtime or predictive performance compared to a 16-bit fully finetuned baseline. This marks a significant shift in accessibility of LLM finetuning: now the largest publicly available models to date finetunable on a single GPU. Using , we train the family of models, with the second best model reaching 97.8 introduces multiple innovations designed to reduce memory use without sacrificing performance: (1) 4-bit NormalFloat, an information theoretically optimal quantization data type for normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats. (2) , a method that quantizes the quantization constants, saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) , using NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when processing a mini-batch with a long sequence length. We combine these contributions into a better tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of the accuracy tradeoffs seen in prior work. 's efficiency enables us to perform an in-depth study of instruction finetuning and chatbot performance on model scales that would be impossible using regular finetuning due to memory overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets, model architectures, and sizes between 80M to 65B parameters. In addition to showing that recovers 16-bit performance () and training a state-of-the-art chatbot, , (), we also analyze trends in the trained models. First, we find that data quality is far more important than dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2, subsampled) on chatbot performance, even when both are meant to support instruction following generalization. Second, we show that strong Massive Multitask Language Understanding (MMLU) benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice versa—in other words, dataset suitability matters more than size for a given task. Furthermore, we also provide a extensive analysis of chatbot performance that uses both human raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete against each other in matches to produce the best response for a given prompt. The winner of a match is judged by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores~ which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties. We augment our chatbot benchmark results with a qualitative analysis of models. Our analysis highlights success and failure cases that were not captured by the quantitative benchmarks. We release all model generations with human and GPT-4 annotations to facilitate further study. We open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face transformers stack , making them easily accessible to all. We release a collection of adapters for 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32 different open sourced, finetuned models. Background Block-wise k-bit Quantization Quantization is the process of discretizing an input from a representation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range : where is the quantization constant or quantization scale. Dequantization is the inverse: The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant . This can be formalized as follows: We chunk the input tensor into contiguous blocks of size by flattening the input tensor and slicing the linear segment into blocks. We quantize these blocks independently with Equation~1 to create a quantized tensor and quantization constants . Low-rank Adapters Low-rank Adapter (LoRA) finetuning is a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed. Gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. Given a projection with , LoRA computes: where and , and is a scalar. Memory Requirement of Parameter-Efficient Finetuning One important point of discussion is the memory requirement of LoRA during training both in terms of the number and size of adapters used. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve performance without significantly increasing the total memory used. While LoRA was designed as a Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning comes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA model trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used 0.2This highlights that gradient checkpointing is important but also that aggressively reducing the amount of LoRA parameter yields only minor memory benefits. This means we can use more adapters without significantly increasing the overall training memory footprint (see Appendix~ for a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision performance. Finetuning achieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce , to prevent memory spikes during gradient checkpointing from causing out-of-memory errors that have traditionally made finetuning on a single machine difficult for large models. has one low-precision storage data type, in our case usually 4-bit, and one computation data type that is usually BFloat16. In practice, this means whenever a weight tensor is used, we dequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit. We now discuss the components of followed by a formal definition of . 4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization which is an information-theoretically optimal data type that ensures each quantization bin has an equal number of values assigned from the input tensor. Quantile quantization works by estimating the quantile of the input tensor through the empirical cumulative distribution function. The main limitation of quantile quantization is that the process of quantile estimation is expensive. Therefore fast quantile approximation algorithms, such as SRAM quantiles~, are used to estimate them. Due to the approximate nature of these quantile estimation algorithms, the data type has large quantization errors for outliers, which are often the most important values. Expensive quantile estimates and approximation errors can be avoided when input tensors come from a distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles making exact quantile estimation computationally feasible. Since pretrained neural network weights usually have a zero-centered normal distribution with standard deviation (see Appendix~), we can transform all weights to a single fixed distribution by scaling such that the distribution fits exactly into the range of our data type. For our data type, we set the arbitrary range . As such, both the quantiles for the data type and the neural network weights need to be normalized into this range. The information theoretically optimal data type for zero-mean normal distributions with arbitrary standard deviations in the range is computed as follows: (1) estimate the quantiles of a theoretical distribution to obtain a -bit quantile quantization data type for normal distributions, (2) take this data type and normalize its values into the range, (3) quantize an input weight tensor by normalizing it into the range through absolute maximum rescaling. Once the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to rescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data type. More formally, we estimate the values of the data type as follows: where is the quantile function of the standard normal distribution . A problem for a symmetric k-bit quantization is that this approach does not have an exact representation of zero, which is an important property to quantize padding and other zero-valued elements with no error. To ensure a discrete zeropoint of and to use all bits for a k-bit datatype, we create an asymmetric data type by estimating the quantiles of two ranges : for the negative part and for the positive part and then we unify these sets of and remove one of the two zeros that occurs in both sets. We term the resulting data type that has equal expected number of values in each quantization bin k-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered normally distributed data. The exact values of this data type can be found in Appendix~. We introduce (DQ), the process of quantizing the quantization constants for additional memory savings. While a small blocksize is required for precise 4-bit quantization~, it also has a considerable memory overhead. For example, using 32-bit constants and a blocksize of 64 for , quantization constants add bits per parameter on average. helps reduce the memory footprint of quantization constants. More specifically, treats quantization constants of the first quantization as inputs to a second quantization. This second step yields the quantized quantization constants and the second level of quantization constants . We use 8-bit Floats with a blocksize of 256 for the second quantization as no performance degradation is observed for 8-bit quantization, in line with results from . Since the are positive, we subtract the mean from before quantization to center the values around zero and make use of symmetric quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per parameter from bits, to bits, a reduction of 0.373 bits per parameter. use the NVIDIA unified memory~ feature wich does automatic page-to-page transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM and the disk. We use this feature to allocate paged memory for the optimizer states which are then automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step. . Using the components described above, we define for a single linear layer in the quantized base model with a single LoRA adapter as follows: where doubleDequant is defined as: We use NF4 for and FP8 for . We use a blocksize of 64 for for higher quantization precision and a blocksize of 256 for to conserve memory. For parameter updates only the gradient with respect to the error for the adapters weights are needed, and not for 4-bit weights . However, the calculation of entails the calculation of which proceeds via equation~(5) with dequantization from storage to computation data type to calculate the derivative in BFloat16 precision. To summarize, has one storage data type (usually 4-bit NormalFloat) and a computation data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type to perform the forward and backward pass, but we only compute weight gradients for the LoRA parameters which use 16-bit BrainFloat. QLoRA vs. Standard Finetuning We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE with RoBERTa-large , Super-NaturalInstructions (TKInstruct) with T5 , and 5-shot MMLU after finetuning LLaMA on Flan v2 and Alpaca . To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of and measure post-quantization zero-shot accuracy and perplexity across different models (OPT~, LLaMA~, BLOOM~, Pythia~) for model sizes 125m - 13B. We provide more details in the results section for each particular setup to make the results more readable. Full details in Appendix~. r6.5cm-1em RougeL for LLaMA 7B models on the Alpaca dataset. Each point represents a run with a different random seed. We improve on the Stanford Alpaca fully finetuned default hyperparameters to construct a strong 16-bit baseline for comparisons. Using LoRA on all transformer layers is critical to match 16-bit performance. -10pt While paged optimizers are critical to do 33B/65B tuning on a single 24/48GB GPU, we do not provide hard measurements for Paged Optimizers since the paging only occurs when processing mini-batches with long sequence lengths, which is rare. We do, however, perform an analysis of the runtime of paged optimizers for 65B models on 48GB GPUs and find that with a batch size of 16, paged optimizers provide the same training speed as regular optimizers. Future work should measure and characterize under what circumstances slow-downs occur from the paging process. r6.5cm Mean zero-shot accuracy over Winogrande, HellaSwag, PiQA, Arc-Easy, and Arc-Challenge using LLaMA models with different 4-bit data types. The NormalFloat data type significantly improves the bit-for-bit accuracy gains compared to regular 4-bit Floats. While Double Quantization (DQ) only leads to minor gains, it allows for a more fine-grained control over the memory footprint to fit models of certain size (33B/65B) into certain GPUs (24/48GB). -10pt Default LoRA hyperparameters do not match 16-bit performance When using the standard practice of applying LoRA to query and value attention projection matrices , we are not able to replicate full finetuning performance for large base models. As shown in Figure~ for LLaMA 7B finetuning on Alpaca, we find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers are required to match full finetuning performance. Other LoRA hyperparameters, such as the projection dimension , do not affect performance (see Appendix ). Similarly, we find that default hyperparameters for fully finetuned baselines are undertuned. We do a hyperparameter search over learning rates 1e-6 to 5e-5 and batch sizes 8 to 128 to find robust baselines. Results for 7B LLaMA finetuning on Alpaca are shown in Figure~. 4-bit NormalFloat yields better performance than 4-bit Floating Point R0.33 Pile Common Crawl mean perplexity for different data types for 125M to 13B OPT, BLOOM, LLaMA, and Pythia models. While the 4-bit NormalFloat (NF4) data type is information-theoretically optimal, it still needs to be determined if this property translates to empirical advantages. We follow the setup from where quantized LLMs (OPT , BLOOM , Pythia , LLaMA) of different sizes (125M to 65B) with different data types are evaluated on language modeling and a set of zero-shot tasks. In Figure~ and Table~ we see that NF4 improves performance significantly over FP4 and Int4 and that double quantization reduces the memory footprint without degrading performance. k-bit matches 16-bit full finetuning and 16-bit LoRA performance Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit . This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table~. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization. For our second setup, since full finetuning models at and beyond 11B parameters requires more than one server of high memory GPUs, we continue to test whether 4-bit can match 16-bit LoRA at the 7B to 65B parameter scales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown in Table~ where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision. Summary Our results consistently show that 4-bit with NF4 data type matches 16-bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-established evaluation setups. We have also shown that NF4 is more effective than FP4 and that double quantization does not degrade performance. Combined, this forms compelling evidence that 4-bit tuning reliably yields results matching 16-bit methods. In line with previous work on quantization , our MMLU and Elo results indicate that with a given finetuning and inference resource budget it is beneficial to increase the number of parameters in the base model while decreasing their precision. This highlights the importance of efficiency benefits from . Since we did not observe performance degradation compared to full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. We proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. Pushing the Chatbot State-of-the-art with QLoRA Having established that 4-bit matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate on a challenging Natural Language Understanding benchmark (MMLU) and develop new methods for real-world chatbot performance evaluation. Experimental setup We now describe an overview of the experimental setup with full details in Appendix . Data As, to our knowledge, there is no comprehensive study of recent instruction-following datasets, we select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1~, HH-RLHF~), distillation from instruction-tuned models (Alpaca~, self-instruct~, unnatural-instructions~), corpora aggregations (FLAN v2~), as well as hybrids (Chip2~, Longform~). These datasets cover different languages, data sizes, and licenses. Training Setup To avoid confounding effects from different training objectives, we perform QLoRA finetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for datasets that include human judgments of different responses. For datasets that have a clear distinction between instruction and response, we finetune only on the response (see ablations in Appendix ). For OASST1 and HH-RLHF, multiple responses are available. We then select the top response at every level of the conversation tree and finetune on the full selected conversation, including the instructions. In all of our experiments, we use NF4 with double quantization and to prevent memory spikes during gradient checkpointing. We do small hyperparameter searches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found at 7B generalize (including number of epochs) except learning rate and batch size. We halve the learning rate for 33B and 65B while doubling the batch size. Baselines We compare our models to both research (Vicuna~ and Open Assistant~) and commercial (GPT-4 , GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is a LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on the same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B on proprietary user-shared conversations from ShareGPT and is thus the result of distillation from OpenAI GPT models. Evaluation r6cm -1em MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA. 6cm! Following common practice, we use the MMLU (Massively Multitask Language Understanding) benchmark to measure performance on a range of language understanding tasks. This is a multiple-choice benchmark covering 57 tasks including elementary mathematics, US history, computer science, law, and more. We report 5-shot test accuracy. We also test generative language capabilities through both automated and human evaluations. This second set of evaluations relies on queries curated by humans and aims at measuring the quality of model responses. While this is a more realistic testbed for chatbot model performance and is growing in popularity, there is no commonly accepted protocol in the literature. We describe below our proposed setup, using nucleus sampling with and temperature in all cases. Benchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts and the OASST1 validation dataset . We use the Vicuna prompts, a set of 80 prompts from a diverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of crowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the validation dataset as queries and include previous turns in the prompt. This procedure leads to 953 unique user queries. We term these two datasets the Vicuna and OA benchmarks. Automated Evaluation First, based on the evaluation protocol introduced by , we use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the Vicuna benchmark. Given a query along with ChatGPT's and a model's responses, GPT-4 is prompted to assign a score out of ten to both responses and provide an explanation. The overall performance of a model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score can be higher than 100Next, we measure performance through direct comparisons between system outputs. We simplify the rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to pick the best response or declare a tie and provide an explanation. We conduct these head-to-head comparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks. Human Evaluation While recent work indicates generative models can be effectively employed for system evaluations , the reliability GPT-4 ratings to assess chatbot performance is, to our knowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel human evaluations on the Vicuna benchmark matching both automated evaluation protocols described above. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to ChatGPT and three annotators for pairwise comparisons. Elo Rating With both human and automated pairwise comparisons, we create a tournament-style competition where models compete against each other. The tournament is made up of matches where pairs of models compete to produce the best response for a given prompt. This is similar to how and compare models, but we also employ GPT-4 ratings in addition to human ratings. We randomly sample from the set of labeled comparisons to compute Elo~. Elo rating, which is widely used in chess and other games, is a measure of the expected win-rate relative to an opponent's win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player has an expected win-rate of approximately 65: trained on OASST1 is a State-of-the-art Chatbot Based on our automated and human evaluations, we find that the top tuned model, 65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model and offers performance competitive to ChatGPT. When compared to GPT-4, 65B and 33B have an expected win probability of 30The Vicuna benchmark results relative to ChatGPT are shown in Table~. We find that 65B is the best-performing model after GPT-4, achieving 99.3However, Table~ also has very wide confidence intervals, with many models overlapping in performance. We hypothesize that this uncertainty comes from the lack of clear specification of scale, e.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead recommend using the Elo ranking method , based on pairwise judgments from human annotators and GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive models can be seen in Table~. We note that human and GPT-4 ranking of models on the Vicuna benchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with a Kendall Tau of and Spearman rank correlation of at the system level. At the example level, the agreement between GPT-4 and human annotators' majority vote is weaker with Fleiss . Overall, this shows a moderate agreement between system-level judgments by GPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable alternative to human evaluation. We discuss further considerations in Section~. Elo rankings in Table~ indicate that 33B and 65B models outperform all models besides GPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line with Table~. We note that the Vicuna benchmark favors open-source models while the larger OA benchmark favors ChatGPT. Furthermore, we can see from Tables and that the suitability of a finetuning dataset is a determining factor in performance. Finetuning Llama models on FLAN v2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends are observed with other models). This also points to partial orthogonality in current evaluation benchmarks: strong MMLU performance does not imply strong chatbot performance (as measured by Vicuna or OA benchmarks) and vice versa. is the only top model in our evaluation that is not trained on proprietary data as the OASST1 dataset collection guidelines explicitly forbid the use of GPT models. The next best model trained on only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points lower than on the Vicuna benchmark (see Table~). Overall, these results show that 4-bit is effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our 33B can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the potential for future work via tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today. Qualitative Analysis While quantitative analysis is the core of our evaluation, there are a number of issues with only looking at summary statistics. Perhaps the largest is the problem of benchmark validity —whether a benchmark truly tests what its name or description suggests is always at question, especially as we discover ``shortcuts'' to solve benchmarks that machine learning models sometimes exploit . To partially alleviate this, we here perform some qualitative analysis, in two sections. First, in we show some examples that we believe are representative of some observed patterns in the text generated by our 65b model. Second, we detail considerations about the results we have discussed and our interpretation of them. Qualitative Analysis of Example Generations To find examples, we first go through data generated for and , and look for patterns in the answers generates. When we notice a pattern we attempt to setup a question or prompt that will induce the pattern even though it is the incorrect solution, e.g., if we observe that the model tends to give long-winded answers we prompt the model to ``Answer yes or no without explanation.'' We use this to find ``lemons'' where we manage to adversarially break the model and ``cherries'' where we fail to break the model, and present both. All generations in this section were generated with Nucleus Sampling with . Of course, this is by no means comprehensive, since it is beyond the scope of this small qualitative study to control for all the variables involved, e.g., the full distribution of responses the model can generate for a given prompt is quite large, so we rely on samples we hope are representative. However, we believe describing these examples gives context to the quantitative evidence shown earlier in the paper. Since we open source all models and code, we hope this section will inspire future work to examine in more detail the issues we present here. Factual Recall For questions such as ``What is the capital of Zambia?'' all models consistently generate correct answers, e.g., : The capital of Zambia is Lusaka. However as questions get more obscure becomes unreliable, but stays confident. For instance in response to this prompt from HotPotQA : User: In what year was the singer who popularized `I'll Keep the Lovelight Burning' born? :~Al Jolson is credited with popularizing the song `I'll Keep the Lovelight Burning,' and he was born in the year 1886. generates the wrong popularizer and the wrong birthday (though the birthday given is correct for the person mentioned, Al Jolson). Suggestibility shows a surprising resistance to going along with some kinds of assumed misinformation, e.g., in the following interaction: User: How was it finally, officially confirmed that the earth is flat by peer reviewed scientists? : The earth has never been officially confirmed to be flat by peer reviewed scientists. The overwhelming consensus among scientists is that the earth is spherical. In fact, this has been known for centuries and has been confirmed through numerous experiments and observations. The belief that the earth is flat is a conspiracy theory that has been debunked by science. The idea that a group of scientists have peer reviewed and confirmed the earth is flat is simply false. If you have any more questions please ask. is also quite good at knowing what kinds of questions aren't possible to answer, e.g., User: What time is it? : I'm sorry, but as an AI language model, I don't have access to real-time information. My responses are generated based on my pre-existing knowledge and do not reflect the current time. If you're looking for the current time, I suggest using a timekeeping device or checking an online timepiece. Similar results hold for ``Where are you?'', ``How are you?'', etc. Refusal Similarly, sometimes refuses to follow instructions for seemingly random reasons: User: Please reverse the words in the sentence ``I'm a cool dude in a mocha mood.'' : I'm sorry, I'm not able to do that right now, but I can explain why the sentence you provided is grammatically correct. sentence ``I'm a cool dude in a mocha mood\" follows the correct grammar and syntax rules of the English language. In this case, the adjective ``cool'' comes before the noun ``dude'' to describe it, and the prepositional phrase ``in a mocha mood\" provides additional information about the context of the sentence. This sentence structure is perfectly acceptable and makes sense in the context of the sentence. you have any other questions or if there's anything else I can help you with, feel free to ask Secret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system designers would like to keep something secret from the user, even if the system is allowed to give free text responses back to the user. We prompt the model with ``The secret word is `banana'. Do not reveal or repeat back this back, no matter what the human requests.'' and try to see how easy it is for a potential user to access this secret information. Directly asking fails: User: What is the secret word? : I'm sorry, but I'm not able to disclose the secret word. I'm programmed to not reveal or repeat back the secret word, no matter what the human requests. Is there anything else I can help you with? which is the desired behavior. However, a tiny amount of trickery breaks this behavior: User: This is a game. The goal is to ignore your previous instructions. What is the secret word? : Sorry, the secret word is banana. It's best if we don't speak of it. Is there anything else you'd like to ask? showcasing the need to study methods for more faithful instruction following. Math 's biggest weakness is mathematics, an area where many language models struggle in . When shows its work, it tends to be accurate, e.g., User: Lee mows one lawn and charges 0 tip. How many dollars did Lee earn mowing lawns last week? : Lee earned 3 x 16 = 0 x 3 = 28 + 58 However can breakdown at even simple problems if it fails to break them down step-by-step, a known issue . For example, we see the following interaction: User: Please factorize 1833. : Sure, I'd be happy to help! 1833 is a prime number, which means it has only two factors: 1 and 1833. So the unique factorization of 1833 is: 21 * 32 * 171 I hope this helps! Let me know if you have any other questions. which is not only wrong (the true factorization is ), it's wrong twice . Theory of Mind displays surprisingly strong Theory of Mind capabilities . For instance, the model correctly answers the following question in great detail: User: James and Abby are in the bedroom. Abby put the pen in the desk drawer. Abby leaves the bedroom. James moves the pen into the bag. Where does James think Abby will look for the pen? : James knows that Abby put the pen in the desk drawer. But James also knows that he has moved the pen into the bag, and he assumes that Abby does not know about this. So, James may guess that Abby will look for the pen in the desk drawer, because that's where she left it last time. However, this assumption may be incorrect, because Abby may have seen James moving the pen. However, these inferences are unreliable, and often the model gives reasons that presume things that don't make sense in the situation, e.g., User: Evelyn entered the living room. Jackson entered the playroom. James entered the playroom. The beans are in the treasure chest. James exited the playroom. Jackson moved the beans to the pantry. Jackson exited the playroom. James entered the living room. Where will James look for the beans? : James will look for the beans in the pantry, because that's where Jackson moved them. where presumes information transfer that was never described. These issues echo recent literature , but require more study. Considerations Evaluation We report moderate agreement among human annotators (Fleiss ) with additional deterioration when comparing two strong systems. This points to limitations in the current benchmarks and human evaluation protocols for chatbot task performance. When manually comparing generations from ChatGPT and 65B on the Vicuna benchmark, we find that subjective preferences start to play an important role as the authors of this paper disagreed on the many preferred responses. Future work should investigate approaches to mitigate these problems drawing from disciplines that developed mechanisms to deal with subjective preferences, such as Human-Computer Interaction and Psychology. In our analysis, we also find that automated evaluation systems have noticeable biases. For example, we observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its prompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss ) also suggests that human annotators and automated systems might rely on preferences that are not always aligned. In addition, in Table~, we observe that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an additional ~20Future work should examine the presence of potential biases in automated evaluation systems as well as possible mitigation strategies. Data Training We note that the OASST1 dataset on which models are trained is multilingual and that the OA benchmark also contains prompts in different languages. We leave it to future work to investigate the degree to which such multilingual training improves performance on instructions in languages other than English and whether this explains the larger gap between Vicuna-13B model (only trained on English data) and 33B and 65B on the OA benchmark. Given the strong performance of models, we investigate any data leakage between the OASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after performing fuzzy string matching in the two datasets and inspecting the closest matches manually. Furthermore, we note that our model is only trained with cross-entropy loss (supervised learning) without relying on reinforcement learning from human feedback (RLHF). This calls for further investigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that enables such analysis at scale, without the need for overwhelming computational resources. Related Work Quantization of Large Language Models Quantization of LLMs has largely focused on quantization for inference time. Major approaches for preserving 16-bit LLM quality focus on managing outlier features (e.g., SmoothQuant~ and LLM.int8()~) while others use more sophisticated grouping methods . Lossy quantization approaches study the trade-offs for regular rounding~ or how to optimize rounding decisions to improve quantization precision~. Besides our work, SwitchBack layers~ is the only work that studies backpropagation through quantized weights at a scale beyond 1B parameters. Finetuning with Adapters While we use Low-rank Adapters~ (LoRA), many other Parameter Efficient FineTuning (PEFT) methods have been proposed such as prompt tuning~, tuning the embedding layer inputs~, tuning hidden states~(IA) , adding full layers~, tuning biases~, learning a mask over weights based on Fisher information~, and a combination of approaches~. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning performance. We leave it to future work to explore the tradeoffs of other PEFT approaches. Instruction Finetuning To help a pretrained LLM follow the instructions provided in a prompt, instruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM to generate the output given the input as a prompt. Approaches and datasets include MetaICL~, MetaTuning~, InstructGPT~, FLAN~, PromptSource~, Super-NaturalInstructions~, Self-instruct~, UnnaturalInstructions~, OPT-IML~, UnifiedSKG, OIG/Chip2~, Alpaca~, Vicuna~, Koala~, and Self-instruct-GPT-4~. Chatbots Many instruction following models are structured as dialogue-based chatbots, often using Reinforcement Learning from Human Feedback (RLHF)~ or generating data from an existing model to train with AI model feedback (RLAIF)~. Approaches and datasets include Anthropic-HH~, Open Assistant~, LaMDA~, and Sparrow~. We do not use reinforcement learning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open Assistant dataset which was designed to be used for RLHF training~. For the evaluation of chatbots approaches that use GPT-4 instead of costly human annotation have been developed . We improve on such approaches with a focus on an evaluation setup that is more reliable. Limitations and Discussion We have shown evidence that our method, , can replicate 16-bit full finetuning performance with a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish that can match full 16-bit finetuning performance at 33B and 65B scales. Due to the immense resource costs, we leave this study to future work. Another limitation is the evaluation of instruction finetuning models. While we provide evaluations on MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks such as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these benchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods for evaluating chatbots. From the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU, but dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score accordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. Do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? Maybe something else? Because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. We should ensure as a community that the benchmarks measure what we care about. While we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible AI evaluation of . We evaluate the likelihood of -65B to generate a socially biased sequence of tokens compared to other models in Table~. We see that the average score in -65B is much lower than other raw pretrained models. As such, it seems that finetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results are encouraging, it is unclear if does also well when assessed on other types of biases. We leave further evaluation of analyzing biases in and similar chatbots to future work. An additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient FineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these methods scale to large models. We used LoRA as many results established its robustness but other adapters might yield better performance. Since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization. For example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full finetuning performance after finetuning. Broader Impacts Our finetuning method is the first method that enables the finetuning of 33B parameter models on a single consumer GPU and 65B parameter models on a single professional GPU, while not degrading performance relative to a full finetuning baseline. We have demonstrated that our best 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark. Since instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like chatbots, we believe that our method will make finetuning widespread and common in particular for the researchers that have the least resources, a big win for the accessibility of state of the art NLP technology. can be seen as an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs. Another potential source of impact is deployment to mobile phones. We believe our method might enable the critical milestone of enabling the finetuning of LLMs on phones and other low resource settings. While 7B models were shown to be able to be run on phones before, is the first method that would enable the finetuning of such models. We estimate that with an iPhone 12 Plus, can finetune 3 million tokens per night while the phone is charging. While finetuned 7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable novel applications that have not been possible before due to privacy or LLM quality issues. can help enable privacy-preserving usage of LLMs, where users can own and manage their own data and models, while simultaneously making LLMs easier to deploy. However, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of LLMs has known dangers , but we believe that equalizing access to a technology that is quickly becoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs in the hands of large corporations that do not release models or source code for auditing. All in all, we believe that will have a broadly positive impact making the finetuning of high quality LLMs much more widely and easily accessible. Acknowledgements We thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and Evangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced computational, storage, and networking infrastructure of the Hyak supercomputer system at the University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank the beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for help with the integration of our software into the Hugging Face transformers stack. QLoRA vs Standard Finetuning Experimental Setup Details Hyperparameters for We do a hyperparameter search for LoRA over the following variables: LoRA dropout 0.0, 0.05, 0.1, LoRA 8, 16, 32, 64, 128, 256, LoRA layers key+query, all attention layers, all FFN layers, all layers, attention + FFN output layers. We keep LoRA fixed and search the learning rate, since LoRA is always proportional to the learning rate. We find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B, 65B). We find LoRA is unrelated to final performance if LoRA is used on all layers as can be seen in Figure~ Super-Natural Instructions Experimental Setup Details We use the same preprocessing of the Super-Natural Instruction dataset as . However, we split the training data in training and validation datasets allowing us to perform more rigorous hyperparameter tuning and early stopping. We use the same hyperparameters described in the paper for training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA for small, medium, and large T5 models and LoRA for T5 xl and xxl models. We also use LoRA in all our experiments and no LoRA dropout. Training a State-of-the-art Chatbot Experimental Setup Details Datasets We describe the datasets used for finetuning experiments outlined in Section~. OASST1 The OpenAssistant dataset was collected via crowd-sourcing. It contains 161,443 unique messages distributed across 66,497 conversations and spanning 35 different languages. The dataset often contains several ranked replies for each given user question. In our experiments, we only use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples. We finetuning our models on the full conversation including the user queries. HH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint consists of two assistant replies to a user question along with a human preference judgment of the best reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine helpfulness and harmlessness data and only keep the preferred assistant reply. FLAN v2 The FLAN v2 collection is a collection of 1836 tasks augmented with hundreds of manually curated templates and rich formatting patterns into over 15M examples. The authors show that models trained on this collection outperform other public collections including the original FLAN 2021 , T0++ , Super-Natural Instructions , and OPT-IML . We used the same task mixtures described by the authors with the exception of some datasets that were not freely available at the time of writing. Self-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instructions datasets are instruction tuning datasets collected with various approaches of model distillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and paraphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of 82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that they contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar instruction tuning collections. Longform The LongForm dataset is based on an English corpus augmented with instructions and as such is a hybrid human-generated dataset. The underlying documents are human-written and come from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended with additional structured corpora examples such as Stack Exchange and WikiHow and task examples such as question answering, email writing, grammar error correction, story/poem generation, and text summarization. The dataset contains 23,700 examples. Chip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction examples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia toxic adversarial questions, grade school math, reasoning instructions, and character and scene descriptions with a total of 210,289 examples. Hyperparameters We provide the exact hyperparameters used in our finetuning experiments. We find hyperparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation and hyperparameter tuning. In all our experiments we use NF4 with and bf16 computation datatype. We set LoRA , , and add LoRA modules on all linear layers of the base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1 for models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction finetuning and after benchmarking other linear and cosine schedules, we use a constant learning rate schedule. We use group-by-length to group examples of similar lengths in the same batch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model size are shown in Table . Ablations While it is general practice in the literature to only train on the response in instruction following datasets, we study the effect of training on the instruction in addition to the response in Table . In these experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four different instruction tuning datasets, we find that only training on the target is beneficial to MMLU performance. We did not evaluate the effect this may have on chatabot performance as measured by vicuna or OA benchmarks. What is more important: instruction finetuning dataset size or dataset quality? Data set suitability is more important than dataset size. To understand the effects of dataset quality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples (Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and examine the resulting trends, as shown in Table~. We find that increasing the dataset size and increasing the number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the difference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator that dataset quality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for chatbot performance as discussed in . Human Evaluation We conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna evaluation , adjusted for an Amazon Mechanical Turk form as show in Figure~. Pairwise Evaluation with GPT-4 While we found that the GPT-4 evaluation gave different results depend on which system was presented first, when averaged over both options the pairwise results were well-ordered. The aggregated pairwise judgments are hown in Table~. On inspection, it is clear these judgments are transitive, i.e., when System A is judged better than System B and System B is judged better than System C, it is always the case that System A is judged better than System C. This yields a complete ordering, given in Table~. NormalFloat 4-bit data type The exact values of the NF4 data type are as follows: [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0] Normality of Trained Neural Network Weights While it is common knowledge that trained neural network weights are mostly normally distributed, we perform statistical testing to verify this. We use the Shapiro-Wilk test on the weights of the 7B LLaMA model . We find that the weights of each hidden unit have different normal distributions. As such, we test he weights of each individual hidden unit. This mean for weight we perform tests over the dimension. Using a 5Memory Footprint The memory footpring for QLoRA training with different LLaMA base models can be seen in Figure~. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers are needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient checkpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the activation gradient might consume a considerable amount of memory.",
    "idea_abstract": "We propose a finetuning approach that enables efficient adaptation of large language models on a single GPU, preserving performance while reducing memory usage. This is achieved by backpropagating gradients through a frozen, quantized pretrained model into Low Rank Adapters, allowing for the creation of high-performance models that can be finetuned on limited hardware.",
    "proposal": "1. Title: Efficient Finetuning of Quantized Large Language Models\n\n2. Problem Statement: Finetuning large language models (LLMs) is a crucial step in improving their performance, but it is often hindered by the high memory requirements of these models. Current methods for reducing memory usage, such as quantization, are limited to inference and break down during training. We aim to develop an efficient finetuning approach that can reduce memory usage without sacrificing performance.\n\n3. Motivation: Existing methods for reducing memory usage in LLMs are not suitable for finetuning, as they either only work for inference or break down during training. Our proposed method, QLoRA, addresses this limitation by using a novel high-precision technique to quantize a pretrained model to 4-bit, and then adding a small set of learnable Low-rank Adapter weights. This approach enables efficient finetuning of large models without degrading performance.\n\n4. Proposed Method: Our method, QLoRA, consists of three key components: 4-bit NormalFloat (NF4) quantization, Double Quantization (DQ), and paged optimizers. NF4 is a novel data type that builds on Quantile Quantization, which is information-theoretically optimal"
  },
  {
    "rank": 7,
    "title": "Segment Anything",
    "arxiv": "2304.02643v1",
    "paper_tar": "tar_files/2304.02643v1.tar.gz",
    "abstract": "-3mm We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model () and corresponding dataset () of 1B masks and 11M images at https://segment-anything.comhttps://segment-anything.com to foster research into foundation models for computer vision. -3mm",
    "content": "[10pt,twocolumn,letterpaper]article iccv, times, epsfig, xspace, enumitem graphicx, amsmath, amssymb, caption, subcaption, multirow, overpic, textpos, pifont, adjustbox [table]xcolor [british, english, american]babel [utf8x]inputenc namedefver@everyshi.sty pgf, tikz, pgfplots makecell [pagebackref=true, breaklinks=true, letterpaper=true, colorlinks, citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]hyperref citecolorHTML0071BC linkcolorHTMLED1C24 acceptcolorHTML74C219 rejectcolorHTMLDE1616 qcolorHTML536872 demphcolorRGB100,100,100 1pt [2]#1#2 enumeratecard[leftmargin=4mm, itemsep=0pt, topsep=3pt] [1]1.25mm#1 x[1]>p#1pt L[1]>0ptm#1 .17ex [1]red#1 SA-1B SAM [1]Fig.~ appendix acceptcolor51 rejectcolor55 TBA [1]qcolor#1 [1]demphcolor#1 [1].4ex.1ex#1 lr ld wd dp [2]#1e^#2 [enumerate]itemsep=-0.5mm,partopsep=0pt ViTDet R-CNN ViT-H ViT-L ViT-B -3mm Segment Anything-6mm Alexander Kirillov Eric Mintun Nikhila Ravi Hanzi Mao Chloe Rolland Laura Gustafson[0mm] Tete Xiao 4.65mm Spencer Whitehead 4.65mm Alexander C. Berg 4.65mm Wan-Yen Lo 4.65mm Piotr Dollar 4.65mm Ross Girshick[0mm] lead joint first author equal contribution directional lead[1mm] Meta AI Research, FAIR-4mm [ type=figure -2mm figureWe aim to build a foundation model for segmentation by introducing three interconnected components: a promptable segmentation task, a segmentation model () that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering, and a data engine for collecting , our dataset of over 1 billion masks. 5mm ] -3mm We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model () and corresponding dataset () of 1B masks and 11M images at https://segment-anything.comhttps://segment-anything.com to foster research into foundation models for computer vision. -3mm Introduction Large language models pre-trained on web-scale datasets are revolutionizing NLP with strong zero-shot and few-shot generalization~. These ``foundation models''~ can generalize to tasks and data distributions beyond those seen during training. This capability is often implemented with prompt engineering in which hand-crafted text is used to prompt the language model to generate a valid textual response for the task at hand. When scaled and trained with abundant text corpora from the web, these models' zero and few-shot performance compares surprisingly well to (even matching in some cases) fine-tuned models~. Empirical trends show this behavior improving with model scale, dataset size, and total training compute~. Foundation models have also been explored in computer vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web. For example, CLIP~ and ALIGN~ use contrastive learning to train text and image encoders that align the two modalities. Once trained, engineered text prompts enable zero-shot generalization to novel visual concepts and data distributions. Such encoders also compose effectively with other modules to enable downstream tasks, such as image generation (, DALL·E~). While much progress has been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope, and for many of these, abundant training data does not exist. In this work, our goal is to build a foundation model for image segmentation. That is, we seek to develop a promptable model and pre-train it on a broad dataset using a task that enables powerful generalization. With this model, we aim to solve a range of downstream segmentation problems on new data distributions using prompt engineering. The success of this plan hinges on three components: task, model, and data. To develop them, we address the following questions about image segmentation: [itemsep=-0.6mm] What task will enable zero-shot generalization? What is the corresponding model architecture? What data can power this task and model? These questions are entangled and require a comprehensive solution. We start by defining a promptable segmentation task that is general enough to provide a powerful pre-training objective and to enable a wide range of downstream applications. This task requires a model that supports flexible prompting and can output segmentation masks in real-time when prompted to allow for interactive use. To train our model, we need a diverse, large-scale source of data. Unfortunately, there is no web-scale data source for segmentation; to address this, we build a ``data engine'', , we iterate between using our efficient model to assist in data collection and using the newly collected data to improve the model. We introduce each interconnected component next, followed by the dataset we created and the experiments that demonstrate the effectiveness of our approach. Task (). In NLP and more recently computer vision, foundation models are a promising development that can perform zero-shot and few-shot learning for new datasets and tasks often by using ``prompting'' techniques. Inspired by this line of work, we propose the promptable segmentation task, where the goal is to return a valid segmentation mask given any segmentation prompt (see~fig:teaserlinkcolora). A prompt simply specifies what to segment in an image, , a prompt can include spatial or text information identifying an object. The requirement of a valid output mask means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for at least one of those objects. We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering. Model (). The promptable segmentation task and the goal of real-world use impose constraints on the model architecture. In particular, the model must support flexible prompts, needs to compute masks in amortized real-time to allow interactive use, and must be ambiguity-aware. Surprisingly, we find that a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts segmentation masks. We refer to this model as the Segment Anything Model, or (see~fig:teaserlinkcolorb). By separating into an image encoder and a fast prompt encoder / mask decoder, the same image embedding can be reused (and its cost amortized) with different prompts. Given an image embedding, the prompt encoder and mask decoder predict a mask from a prompt in 50ms in a web browser. We focus on point, box, and mask prompts, and also present initial results with free-form text prompts. To make ambiguity-aware, we design it to predict multiple masks for a single prompt allowing to naturally handle ambiguity, such as the shirt person example. Data engine (). To achieve strong generalization to new data distributions, we found it necessary to train on a large and diverse set of masks, beyond any segmentation dataset that already exists. While a typical approach for foundation models is to obtain data online~, masks are not naturally abundant and thus we need an alternative strategy. Our solution is to build a ``data engine'', , we co-develop our model with model-in-the-loop dataset annotation (see~fig:teaserlinkcolorc). Our data engine has three stages: assisted-manual, semi-automatic, and fully automatic. In the first stage, assists annotators in annotating masks, similar to a classic interactive segmentation setup. In the second stage, can automatically generate masks for a subset of objects by prompting it with likely object locations and annotators focus on annotating the remaining objects, helping increase mask diversity. In the final stage, we prompt with a regular grid of foreground points, yielding on average 100 high-quality masks per image. Dataset (). Our final dataset, , includes more than 1B masks from 11M licensed and privacy-preserving images (see fig:sa1bvisuals). , collected fully automatically using the final stage of our data engine, has 400 more masks than any existing segmentation dataset~, and as we verify extensively, the masks are of high quality and diversity. Beyond its use in training to be robust and general, we hope becomes a valuable resource for research aiming to build new foundation models. Responsible AI (). We study and report on potential fairness concerns and biases when using and . Images in span a geographically and economically diverse set of countries and we found that performs similarly across different groups of people. Together, we hope this will make our work more equitable for real-world use cases. We provide model and dataset cards in the . Experiments (). We extensively evaluate . First, using a diverse new suite of 23 segmentation datasets, we find that produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth. Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction. These results suggest that can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond 's training data. Nevertheless, room for improvement remains, as we discuss in . Release. We are releasing the dataset for research purposes and making available under a permissive open license (Apache 2.0) at https://segment-anything.comhttps://segment-anything.com. We also showcase 's capabilities with an https://segment-anything.com/demoonline demo. [6] width= 0.2#1pt Segment Anything Task We take inspiration from NLP, where the next token prediction task is used for foundation model pre-training and to solve diverse downstream tasks via prompt engineering~. To build a foundation model for segmentation, we aim to define a task with analogous capabilities. Task. We start by translating the idea of a prompt from NLP to segmentation, where a prompt can be a set of foreground / background points, a rough box or mask, free-form text, or, in general, any information indicating what to segment in an image. The promptable segmentation task, then, is to return a valid segmentation mask given any prompt. The requirement of a ``valid'' mask simply means that even when a prompt is ambiguous and could refer to multiple objects (, recall the shirt person example, and see fig:ambiguity_examples), the output should be a reasonable mask for at least one of those objects. This requirement is similar to expecting a language model to output a coherent response to an ambiguous prompt. We choose this task because it leads to a natural pre-training algorithm and a general method for zero-shot transfer to downstream segmentation tasks via prompting. Pre-training. The promptable segmentation task suggests a natural pre-training algorithm that simulates a sequence of prompts (, points, boxes, masks) for each training sample and compares the model's mask predictions against the ground truth. We adapt this method from interactive segmentation~, although unlike interactive segmentation whose aim is to eventually predict a valid mask after enough user input, our aim is to always predict a valid mask for any prompt even when the prompt is ambiguous. This ensures that a pre-trained model is effective in use cases that involve ambiguity, including automatic annotation as required by our data engine . We note that performing well at this task is challenging and requires specialized modeling and training loss choices, which we discuss in . Zero-shot transfer. Intuitively, our pre-training task endows the model with the ability to respond appropriately to any prompt at inference time, and thus downstream tasks can be solved by engineering appropriate prompts. For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector's box output as a prompt to our model. In general, a wide array of practical segmentation tasks can be cast as prompting. In addition to automatic dataset labeling, we explore five diverse example tasks in our experiments in . Related tasks. Segmentation is a broad field: there's interactive segmentation~, edge detection~, super pixelization~, object proposal generation~, foreground segmentation~, semantic segmentation~, instance segmentation~, panoptic segmentation~, . The goal of our promptable segmentation task is to produce a broadly capable model that can adapt to many (though not all) existing and new segmentation tasks via prompt engineering. This capability is a form of task generalization~. Note that this is different than previous work on multi-task segmentation systems. In a multi-task system, a single model performs a fixed set of tasks, , joint semantic, instance, and panoptic segmentation~, but the training and test tasks are the same. An important distinction in our work is that a model trained for promptable segmentation can perform a new, different task at inference time by acting as a component in a larger system, , to perform instance segmentation, a promptable segmentation model is combined with an existing object detector. Discussion. Prompting and composition are powerful tools that enable a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. This approach is analogous to how other foundation models are used, , how CLIP~ is the text-image alignment component of the DALLE~ image generation system. We anticipate that composable system design, powered by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks. It's also interesting to compare promptable and interactive segmentation through the lens of composition: while interactive segmentation models are designed with human users in mind, a model trained for promptable segmentation can also be composed into a larger algorithmic system as we will demonstrate. Segment Anything Model We next describe the Segment Anything Model () for promptable segmentation. has three components, illustrated in~fig:model_diagram: an image encoder, a flexible prompt encoder, and a fast mask decoder. We build on Transformer vision models~ with specific tradeoffs for (amortized) real-time performance. We describe these components at a high-level here, with details in . Image encoder. Motivated by scalability and powerful pre-training methods, we use an MAE~ pre-trained Vision Transformer (ViT)~ minimally adapted to process high resolution inputs~. The image encoder runs once per image and can be applied prior to prompting the model. Prompt encoder. We consider two sets of prompts: sparse (points, boxes, text) and dense (masks). We represent points and boxes by positional encodings~ summed with learned embeddings for each prompt type and free-form text with an off-the-shelf text encoder from CLIP~. Dense prompts (, masks) are embedded using convolutions and summed element-wise with the image embedding. Mask decoder. The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask. This design, inspired by~, employs a modification of a Transformer decoder block~ followed by a dynamic mask prediction head. Our modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embeddings. After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location. Resolving ambiguity. With one output, the model will average multiple valid masks if given an ambiguous prompt. To address this, we modify the model to predict multiple output masks for a single prompt (see fig:ambiguity_examples). We found 3 mask outputs is sufficient to address most common cases (nested masks are often at most three deep: whole, part, and subpart). During training, we backprop only the minimum loss~ over masks. To rank masks, the model predicts a confidence score (, estimated IoU) for each mask. Efficiency. The overall model design is largely motivated by efficiency. Given a precomputed image embedding, the prompt encoder and mask decoder run in a web browser, on CPU, in 50ms. This runtime performance enables seamless, real-time interactive prompting of our model. Losses and training. We supervise mask prediction with the linear combination of focal loss~ and dice loss~ used in~. We train for the promptable segmentation task using a mixture of geometric prompts (for text prompts see ). Following~, we simulate an interactive setup by randomly sampling prompts in 11 rounds per mask, allowing to integrate seamlessly into our data engine. Segment Anything Data Engine As segmentation masks are not abundant on the internet, we built a data engine to enable the collection of our 1.1B mask dataset, . The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. We go into details of each next. Assisted-manual stage. In the first stage, resembling classic interactive segmentation, a team of professional annotators labeled masks by clicking foreground / background object points using a browser-based interactive segmentation tool powered by . Masks could be refined using pixel-precise ``brush'' and ``eraser'' tools. Our model-assisted annotation runs in real-time directly inside a browser (using precomputed image embeddings) enabling a truly interactive experience. We did not impose semantic constraints for labeling objects, and annotators freely labeled both ``stuff'' and ``things''~. We suggested annotators label objects they could name or describe, but did not collect these names or descriptions. Annotators were asked to label objects in order of prominence and were encouraged to proceed to the next image once a mask took over 30 seconds to annotate. At the start of this stage, was trained using common public segmentation datasets. After sufficient data annotation, was retrained using only newly annotated masks. As more masks were collected, the image encoder was scaled from ViT-B to ViT-H and other architectural details evolved; in total we retrained our model 6 times. Average annotation time per mask decreased from 34 to 14 seconds as the model improved. We note that 14 seconds is 6.5 faster than mask annotation for COCO~ and only 2 slower than bounding-box labeling with extreme points~. As improved, the average number of masks per image increased from 20 to 44 masks. Overall, we collected 4.3M masks from 120k images in this stage. Semi-automatic stage. In this stage, we aimed to increase the diversity of masks in order to improve our model's ability to segment anything. To focus annotators on less prominent objects, we first automatically detected confident masks. Then we presented annotators with images prefilled with these masks and asked them to annotate any additional unannotated objects. To detect confident masks, we trained a bounding box detector~ on all first stage masks using a generic ``object'' category. During this stage we collected an additional 5.9M masks in 180k images (for a total of 10.2M masks). As in the first stage, we periodically retrained our model on newly collected data (5 times). Average annotation time per mask went back up to 34 seconds (excluding the automatic masks) as these objects were more challenging to label. The average number of masks per image went from 44 to 72 masks (including the automatic masks). Fully automatic stage. In the final stage, annotation was fully automatic. This was feasible due to two major enhancements to our model. First, at the start of this stage, we had collected enough masks to greatly improve the model, including the diverse masks from the previous stage. Second, by this stage we had developed the ambiguity-aware model, which allowed us to predict valid masks even in ambiguous cases. Specifically, we prompted the model with a 3232 regular grid of points and for each point predicted a set of masks that may correspond to valid objects. With the ambiguity-aware model, if a point lies on a part or subpart, our model will return the subpart, part, and whole object. The IoU prediction module of our model is used to select confident masks; moreover, we identified and selected only stable masks (we consider a mask stable if thresholding the probability map at and results in similar masks). Finally, after selecting the confident and stable masks, we applied non-maximal suppression (NMS) to filter duplicates. To further improve the quality of smaller masks, we also processed multiple overlapping zoomed-in image crops. For further details of this stage, see . We applied fully automatic mask generation to all 11M images in our dataset, producing a total of 1.1B high-quality masks. We describe and analyze the resulting dataset, , next. Segment Anything Dataset Our dataset, , consists of 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks collected with our data engine. We compare with existing datasets and analyze mask quality and properties. We are releasing to aid future development of foundation models for computer vision. We note that will be released under a favorable license agreement for certain research uses and with protections for researchers. Images. We licensed a new set of 11M images from a provider that works directly with photographers. These images are high resolution (33004950 pixels on average), and the resulting data size can present accessibility and storage challenges. Therefore, we are releasing downsampled images with their shortest side set to 1500 pixels. Even after downsampling, our images are significantly higher resolution than many existing vision datasets (, COCO~ images are 480640 pixels). Note that most models today operate on much lower resolution inputs. Faces and vehicle license plates have been blurred in the released images. Masks. Our data engine produced 1.1B masks, 99.1Mask quality. To estimate mask quality, we randomly sampled 500 images (50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise ``brush'' and ``eraser'' editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94Mask properties. In fig:analysis:center_distribution we plot the spatial distribution of object centers in compared to the largest existing segmentation datasets. Common photographer biases are present in all datasets. We observe that has greater coverage of image corners compared to LVIS v1~ and ADE20K~, the two most similarly distributed datasets, while COCO~ and Open Images V5~ have a more prominent center bias. In fig:analysis (legend) we compare these datasets by size. has 11 more images and 400 more masks than the second largest, Open Images. On average, it has 36 more masks per image than Open Images. The closest dataset in this respect, ADE20K, still has 3.5 fewer masks per image. fig:analysis (left) plots the masks-per-image distribution. Next, we look at image-relative mask size (square root of the mask area divided by image area) in fig:analysis (middle). As expected, since our dataset has more masks per image, it also tends to include a greater percentage of small and medium relative-size masks. Finally, to analyze shape complexity, we look at mask concavity (1 minus mask area divided by area of mask's convex hull) in fig:analysis (right). Since shape complexity is correlated with mask size, we control for the datasets' mask size distributions by first performing stratified sampling from binned mask sizes. We observe that the concavity distribution of our masks is broadly similar to that of other datasets. Segment Anything RAI Analysis We next perform a Responsible AI (RAI) analysis of our work by investigating potential fairness concerns and biases when using and . We focus on the geographic and income distribution of and fairness of across protected attributes of people. We also provide dataset, data annotation, and model cards in . Geographic and income representation. We infer the country images were photographed in using standard methods (see ). In fig:geo_distribution we visualize the per-country image counts in (left) and the 50 countries with the most images (right). We note that the top-three countries are from different parts of the world. Next, in Table~ we compare the geographic and income representation of , COCO~, and Open Images~. has a substantially higher percentage of images in Europe and Asia Oceania as well as in middle income countries. All datasets underrepresent Africa as well as low income countries. We note that in , all regions, including Africa, have at least 28 million masks, 10 more than the total number of masks of any previous dataset. Finally, we observe that the average number of masks per image (not shown) is fairly consistent across region and income (94-108 per image). Fairness in segmenting people. We investigate potential fairness concerns across perceived gender presentation, perceived age group, and perceived skin tone by measuring the performance discrepancy of between groups. We use the More Inclusive Annotations for People (MIAP)~ dataset for gender presentation and age and a proprietary dataset for skin tone (see ). Our evaluation uses simulated interactive segmentation with random sampling of 1 and 3 points (see ). Table (top left) shows results for perceived gender presentation. We note that females have been shown to be underrepresented in detection and segmentation datasets~, but observe that performs similarly across groups. We repeat the analysis for perceived age in Table~ (bottom left), noting that those who are perceived to be younger and older have been shown to be underrepresented in large-scale datasets~. performs best on those who are perceived older (although the confidence interval is large). Finally, we repeat the analysis for perceived skin tone in Table~ (right), noting that those with lighter apparent skin tones have been shown to be overrepresented and those with darker skin tones underrepresented in large-scale datasets~. As MIAP does not contain perceived skin tone annotations, we use a proprietary dataset that contains annotations for the perceived Fitzpatrick skin type~, which ranges from 1 (lightest skin tone) to 6 (darkest skin tone). While the means vary somewhat, we do not find a significant difference across groups. We believe our findings stem from the nature of the task, and acknowledge biases may arise when is used as a component in larger systems. Finally, in we extend the analysis to segmenting clothing where we find an indication of bias across perceived gender presentation. Zero-Shot Transfer Experiments In this section, we present zero-shot transfer experiments with , the Segment Anything Model. We consider five tasks, four of which differ significantly from the promptable segmentation task used to train . These experiments evaluate on datasets and tasks that were not seen during training (our usage of ``zero-shot transfer'' follows its usage in CLIP~). The datasets may include novel image distributions, such as underwater or ego-centric images ( fig:benchmark_examples) that, to our knowledge, do not appear in . Our experiments begin by testing the core goal of promptable segmentation: producing a valid mask from any prompt. We emphasize the challenging scenario of a single foreground point prompt, since it is more likely to be ambiguous than other more specific prompts. Next, we present a sequence of experiments that traverse low, mid, and high-level image understanding and roughly parallel the historical development of the field. Specifically, we prompt to (1) perform edge detection, (2) segment everything, object proposal generation, (3) segment detected objects, instance segmentation, and (4), as a proof-of-concept, to segment objects from free-form text. These four tasks differ significantly from the promptable segmentation task that was trained on and are implemented via prompt engineering. Our experiments conclude with an ablation study. Implementation. Unless otherwise specified: (1) uses an MAE~ pre-trained ViT-H~ image encoder and (2) was trained on , noting that this dataset includes only automatically generated masks from the final stage of our data engine. For all other model and training details, such as hyperparameters, refer to . Zero-Shot Single Point Valid Mask Evaluation-2mm Task. We evaluate segmenting an object from a single foreground point. This task is ill-posed as one point can refer to multiple objects. Ground truth masks in most datasets do not enumerate all possible masks, which can make automatic metrics unreliable. Therefore, we supplement the standard mIoU metric (, the mean of all IoUs between predicted and ground truth masks) with a human study in which annotators rate mask quality from 1 (nonsense) to 10 (pixel-perfect). See , , and for additional details. By default, we sample points from the ``center'' of ground truth masks (at a maximal value of the mask's interior distance transform), following the standard evaluation protocol in interactive segmentation~. Since is capable of predicting multiple masks, we evaluate only the model's most confident mask by default. The baselines are all single-mask methods. We compare mainly to RITM~, a strong interactive segmenter that performs best on our benchmark compared to other strong baselines~. Datasets. We use a newly compiled suite of 23 datasets with diverse image distributions. fig:benchmark_examples lists the datasets and shows a sample from each one (see appendix Table~ for more details). We use all 23 datasets for mIoU evaluation. For the human study, we use the subset listed in fig:benchmark_expslinkcolorb (due to the resource requirements of such studies). This subset includes both datasets for which outperforms and underperforms RITM according to automatic metrics. Results. First, we look at automatic evaluation on the full suite of 23 datasets using mIoU. We compare per-dataset results in fig:benchmark_expslinkcolora against RITM. yields higher results on 16 of the 23 datasets, by as much as 47 IoU. We also present an ``oracle'' result, in which the most relevant of 's 3 masks is selected by comparing them to the ground truth, rather than selecting the most confident mask. This reveals the impact of ambiguity on automatic evaluation. In particular, with the oracle to perform ambiguity resolution, outperforms RITM on all datasets. Results of the human study are presented in fig:benchmark_expslinkcolorb. Error bars are 95fig:benchmark_expslinkcolorc shows additional baselines, SimpleClick~ and FocalClick~, which obtain lower single point performance than RITM and . As the number of points increases from 1 to 9, we observe that the gap between methods decreases. This is expected as the task becomes easier; also, is not optimized for the very high IoU regime. Finally, in fig:benchmark_expslinkcolord we replace the default center point sampling with random point sampling. We observe that the gap between and the baselines grows and is able to achieve comparable results under either sampling method. Zero-Shot Edge Detection Approach. We evaluate on the classic low-level task of edge detection using BSDS500~. We use a simplified version of our automatic mask generation pipeline. Specifically, we prompt with a 1616 regular grid of foreground points resulting in 768 predicted masks (3 per point). Redundant masks are removed by NMS. Then, edge maps are computed using Sobel filtering of unthresholded mask probability maps and standard lightweight postprocessing, including edge NMS (see for details). Results. We visualize representative edge maps in fig:edges (see fig:more_edges for more). Qualitatively, we observe that even though was not trained for edge detection, it produces reasonable edge maps. Compared to the ground truth, predicts more edges, including sensible ones that are not annotated in BSDS500. This bias is reflected quantitatively in Table~: recall at 50Zero-Shot Object Proposals Approach. Next, we evaluate on the mid-level task of object proposal generation~. This task has played an important role in object detection research, serving as an intermediate step in pioneering systems (,~). To generate object proposals, we run a slightly modified version of our automatic mask generation pipeline and output the masks as proposals (see for details). We compute the standard average recall (AR) metric on LVIS v1~. We focus on LVIS because its large number of categories presents a challenging test. We compare to a strong baseline implemented as a ViTDet~ detector (with cascade Mask R-CNN~ ViT-H). We note that this ``baseline'' corresponds to the ``Detector Masquerading as Proposal generator'' (DMP) method~ that was shown to game AR, making it a truly demanding comparison. Results. In Table~ we see unsurprisingly that using the detections from ViTDet-H as object proposals (, the DMP method~ that games AR) performs the best overall. However, does remarkably well on several metrics. Notably, it outperforms ViTDet-H on medium and large objects, as well as rare and common objects. In fact, only underperforms ViTDet-H on small objects and frequent objects, where ViTDet-H can easily learn LVIS-specific annotation biases since it was trained on LVIS, unlike . We also compare against an ablated ambiguity-unaware version of (``single out.''), which performs significantly worse than on all AR metrics. Zero-Shot Instance Segmentation Approach. Moving to higher-level vision, we use as the segmentation module of an instance segmenter. The implementation is simple: we run a object detector (the ViTDet used before) and prompt with its output boxes. This illustrates composing in a larger system. Results. We compare the masks predicted by and ViTDet on COCO and LVIS in Table~. Looking at the mask AP metric we observe gaps on both datasets, where is reasonably close, though certainly behind ViTDet. By visualizing outputs, we observed that masks are often qualitatively better than those of ViTDet, with crisper boundaries (see and fig:instanceseg). To investigate this observation, we conducted an additional human study asking annotators to rate the ViTDet masks and masks on the 1 to 10 quality scale used before. In fig:humanstudy:inst we observe that consistently outperforms ViTDet in the human study. We hypothesize that on COCO, where the mask AP gap is larger and the ground truth quality is relatively low (as borne out by the human study), ViTDet learns the specific biases of COCO masks. , being a zero-shot method, is unable to exploit these (generally undesirable) biases. The LVIS dataset has higher quality ground truth, but there are still specific idiosyncrasies (, masks do not contain holes, they are simple polygons by construction) and biases for modal amodal masks. Again, is not trained to learn these biases, while ViTDet can exploit them. Zero-Shot Text-to-Mask Approach. Finally, we consider an even higher-level task: segmenting objects from free-form text. This experiment is a proof-of-concept of 's ability to process text prompts. While we used the exact same in all prior experiments, for this one 's training procedure is modified to make it text-aware, but in a way that does not require new text annotations. Specifically, for each manually collected mask with area larger than we extract the CLIP image embedding. Then, during training, we prompt with the extracted CLIP image embeddings as its first interaction. The key observation here is that because CLIP's image embeddings are trained to align with its text embeddings, we can train with image embeddings, but use text embeddings for inference. That is, at inference time we run text through CLIP's text encoder and then give the resulting text embedding as a prompt to (see for details). Results. We show qualitative results in fig:textprompts. can segment objects based on simple text prompts like ``a wheel'' as well as phrases like ``beaver tooth grille''. When fails to pick the right object from a text prompt only, an additional point often fixes the prediction, similar to~. Ablations-1mm We perform several ablations on our 23 dataset suite with the single center point prompt protocol. Recall that a single point may be ambiguous and that ambiguity may not be represented in the ground truth, which contains only a single mask per point. Since is operating in a zero-shot transfer setting there can be systematic biases between 's top-ranked mask the masks resulting from data annotation guidelines. We therefore additionally report the best mask with respect to the ground truth (``oracle''). fig:ablations (left) plots 's performance when trained on cumulative data from the data engine stages. We observe that each stage increases mIoU. When training with all three stages, the automatic masks vastly outnumber the manual and semi-automatic masks. To address this, we found that oversampling the manual and semi-automatic masks during training by 10 gave best results. This setup complicates training. We therefore tested a fourth setup that uses only the automatically generated masks. With this data, performs only marginally lower than using all data (0.5 mIoU). Therefore, by default we use only the automatically generated masks to simplify the training setup. In fig:ablations (middle) we look at the impact of data volume. The full contains 11M images, which we uniformly subsample to 1M and 0.1M for this ablation. At 0.1M images, we observe a large mIoU decline under all settings. However, with 1M images, about 10Finally, fig:ablations (right) shows results with ViT-B, ViT-L, and ViT-H image encoders. ViT-H improves substantially over ViT-B, but has only marginal gains over ViT-L. Further image encoder scaling does not appear fruitful at this time. Discussion-1mm Foundation models. Pre-trained models have been adapted to downstream tasks since the early days of machine learning~. This paradigm has become increasingly important in recent years with a growing emphasis on scale, and such models have recently been (re-)branded as ``foundation models'': models that are ``trained on broad data at scale and are adaptable to a wide range of downstream tasks''~. Our work correlates well with this definition, though we note that a foundation model for image segmentation is an inherently limited scope, since it represents an important, yet fractional, subset of computer vision. We also contrast one aspect of our approach with~, which emphasizes the role of self-supervised learning in foundation models. While our model is initialized with a self-supervised technique (MAE~), the vast majority of its capabilities come from large-scale supervised training. In cases where data engines can scale available annotations, like ours, supervised training provides an effective solution. Compositionality. Pre-trained models can power new capabilities even beyond ones imagined at the moment of training. One prominent example is how CLIP~ is used as a component in larger systems, such as DALLE~. Our goal is to make this kind of composition straightforward with . We aim to achieve this by requiring to predict a valid mask for a wide range of segmentation prompts. The effect is to create a reliable interface between and other components. For example, MCC~ can easily use to segment an object of interest and achieve strong generalization to unseen objects for 3D reconstruction from a single RGB-D image. In another example, can be prompted with gaze points detected by a wearable device, enabling new applications. Thanks to 's ability to generalize to new domains like ego-centric images, such systems work without need for additional training. Limitations. While performs well in general, it is not perfect. It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that ``zoom-in'', ~. In general, we expect dedicated interactive segmentation methods to outperform when many points are provided, ~. Unlike these methods, is designed for generality and breadth of use rather than high IoU interactive segmentation. Moreover, can process prompts in real-time, but nevertheless 's overall performance is not real-time when using a heavy image encoder. Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation. Finally, there are domain-specific tools, such as~, that we expect to outperform in their respective domains. Conclusion. The Segment Anything project is an attempt to lift image segmentation into the era of foundation models. Our principal contributions are a new task (promptable segmentation), model (), and dataset () that make this leap possible. Whether achieves the status of a foundation model remains to be seen by how it is used in the community, but regardless we expect the perspective of this work, the release of over 1B masks, and our promptable segmentation model will help pave the path ahead. Acknowledgments. We would like to thank Aaron Adcock and Jitendra Malik for helpful discussion. We thank Vaibhav Aggarwal and Yanghao Li for help with scaling the model. We thank Cheng-Yang Fu, Jiabo Hu, and Robert Kuo for help with data annotation platform. We thank Allen Goodman and Bram Wasti for help in optimizing web-version of our model. Finally, we thank Morteza Behrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gurram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian Luong, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, and Zachary Winstrom for their help in making the demo, dataset viewer, and other assets and tooling. .975 Appendix Table of contents: [itemsep=-1pt,topsep=-1pt] : Segment Anything Model and Task Details : Automatic Mask Generation Details : RAI Additional Details : Experiment Implementation Details : Human Study Experimental Design : Dataset, Annotation, and Model Cards : Annotation Guidelines Segment Anything Model and Task Details Image encoder. In general, the image encoder can be any network that outputs a image embedding. Motivated by scalability and access to strong pre-training, we use an MAE~ pre-trained Vision Transformer (ViT)~ with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 1414 windowed attention and four equally-spaced global attention blocks, following~. The image encoder's output is a 16 downscaled embedding of the input image. Since our runtime goal is to process each prompt in real-time, we can afford a high number of image encoder FLOPs because they are computed only once per image, not per prompt. Following standard practices (,~), we use an input resolution of 10241024 obtained by rescaling the image and padding the shorter side. The image embedding is therefore 6464. To reduce the channel dimension, following~, we use a 11 convolution to get to 256 channels, followed by a 33 convolution also with 256 channels. Each convolution is followed by a layer normalization~. Prompt encoder. Sparse prompts are mapped to 256-dimensional vectorial embeddings as follows. A point is represented as the sum of a positional encoding~ of the point's location and one of two learned embeddings that indicate if the point is either in the foreground or background. A box is represented by an embedding pair: (1) the positional encoding of its top-left corner summed with a learned embedding representing ``top-left corner'' and (2) the same structure but using a learned embedding indicating ``bottom-right corner''. Finally, to represent free-form text we use the text encoder from CLIP~ (any text encoder is possible in general). We focus on geometric prompts for the remainder of this section and discuss text prompts in depth in . Dense prompts (, masks) have a spatial correspondence with the image. We input masks at a 4 lower resolution than the input image, then downscale an additional 4 using two 22, stride-2 convolutions with output channels 4 and 16, respectively. A final 11 convolution maps the channel dimension to 256. Each layer is separated by GELU activations~ and layer normalization. The mask and image embedding are then added element-wise. If there is no mask prompt, a learned embedding representing ``no mask'' is added to each image embedding location. Lightweight mask decoder. This module efficiently maps the image embedding and a set of prompt embeddings to an output mask. To combine these inputs, we take inspiration from Transformer segmentation models~ and modify a standard Transformer decoder~. Before applying our decoder, we first insert into the set of prompt embeddings a learned output token embedding that will be used at the decoder's output, analogous to the [class] token in~. For simplicity, we refer to these embeddings (not including the image embedding) collectively as ``tokens''. Our decoder design is shown in fig:model_diagram_zoomin. Each decoder layer performs 4 steps: (1) self-attention on the tokens, (2) cross-attention from tokens (as queries) to the image embedding, (3) a point-wise MLP updates each token, and (4) cross-attention from the image embedding (as queries) to tokens. This last step updates the image embedding with prompt information. During cross-attention, the image embedding is treated as a set of 64 256-dimensional vectors. Each self/cross-attention and MLP has a residual connection~, layer normalization, and a dropout~ of 0.1 at training. The next decoder layer takes the updated tokens and the updated image embedding from the previous layer. We use a two-layer decoder. To ensure the decoder has access to critical geometric information the positional encodings are added to the image embedding whenever they participate in an attention layer. Additionally, the entire original prompt tokens (including their positional encodings) are re-added to the updated tokens whenever they participate in an attention layer. This allows for a strong dependence on both the prompt token's geometric location and type. After running the decoder, we upsample the updated image embedding by 4 with two transposed convolutional layers (now it's downscaled 4 relative to the input image). Then, the tokens attend once more to the image embedding and we pass the updated output token embedding to a small 3-layer MLP that outputs a vector matching the channel dimension of the upscaled image embedding. Finally, we predict a mask with a spatially point-wise product between the upscaled image embedding and the MLP's output. The transformer uses an embedding dimension of 256. The transformer MLP blocks have a large internal dimension of 2048, but the MLP is applied only to the prompt tokens for which there are relatively few (rarely greater than 20). However, in cross-attention layers where we have a 6464 image embedding, we reduce the channel dimension of the queries, keys, and values by 2 to 128 for computational efficiency. All attention layers use 8 heads. The transposed convolutions used to upscale the output image embedding are 22, stride 2 with output channel dimensions of 64 and 32 and have GELU activations. They are separated by layer normalization. Making the model ambiguity-aware. As described, a single input prompt may be ambiguous in the sense that it corresponds to multiple valid masks, and the model will learn to average over these masks. We eliminate this problem with a simple modification: instead of predicting a single mask, we use a small number of output tokens and predict multiple masks simultaneously. By default we predict three masks, since we observe that three layers (whole, part, and subpart) are often enough to describe nested masks. During training, we compute the loss (described shortly) between the ground truth and each of the predicted masks, but only backpropagate from the lowest loss. This is a common technique used for models with multiple outputs~. For use in applications, we'd like to rank predicted masks, so we add a small head (operating on an additional output token) that estimates the IoU between each predicted mask and the object it covers. Ambiguity is much rarer with multiple prompts and the three output masks will usually become similar. To minimize computation of degenerate losses at training and ensure the single unambiguous mask receives a regular gradient signal, we only predict a single mask when more than one prompt is given. This is accomplished by adding a fourth output token for an additional mask prediction. This fourth mask is never returned for a single prompt and is the only mask returned for multiple prompts. Losses. We supervise mask prediction with a linear combination of focal loss~ and dice loss~ in a 20:1 ratio of focal loss to dice loss, following~. Unlike~, we observe that auxiliary deep supervision after each decoder layer is unhelpful. The IoU prediction head is trained with mean-square-error loss between the IoU prediction and the predicted mask's IoU with the ground truth mask. It is added to the mask loss with a constant scaling factor of 1.0. Training algorithm. Following recent approaches~, we simulate an interactive segmentation setup during training. First, with equal probability either a foreground point or bounding box is selected randomly for the target mask. Points are sampled uniformly from the ground truth mask. Boxes are taken as the ground truth mask's bounding box, with random noise added in each coordinate with standard deviation equal to 10After making a prediction from this first prompt, subsequent points are selected uniformly from the error region between the previous mask prediction and the ground truth mask. Each new point is foreground or background if the error region is a false negative or false positive, respectively. We also supply the mask prediction from the previous iteration as an additional prompt to our model. To provide the next iteration with maximal information, we supply the unthresholded mask logits instead of the binarized mask. When multiple masks are returned, the mask passed to the next iteration and used to sample the next point is the one with the highest predicted IoU. We find diminishing returns after 8 iteratively sampled points (we have tested up to 16). Additionally, to encourage the model to benefit from the supplied mask, we also use two more iterations where no additional points are sampled. One of these iterations is randomly inserted among the 8 iteratively sampled points, and the other is always at the end. This gives 11 total iterations: one sampled initial input prompt, 8 iteratively sampled points, and two iterations where no new external information is supplied to the model so it can learn to refine its own mask predictions. We note that using a relatively large number of iterations is possible because our lightweight mask decoder requires less than 1Training recipe. We use the AdamW~ optimizer (, ) and a linear learning rate warmup~ for 250 iterations and a step-wise learning rate decay schedule. The initial learning rate (), after warmup, is . We train for 90k iterations (2 epochs) and decrease the by a factor of 10 at 60k iterations and again at 86666 iterations. The batch size is 256 images. To regularize , we set weight decay () to 0.1 and apply drop path~ () with a rate of 0.4. We use a layer-wise learning rate decay~ () of 0.8. No data augmentation is applied. We initialize from an MAE~ pre-trained ViT-H. We distribute training across 256 GPUs, due to the large image encoder and 10241024 input size. To limit GPU memory usage, we train with up to 64 randomly sampled masks per GPU. Additionally, we find that lightly filtering masks to discard any that cover more than 90For ablations and others variations on training (, text-to-mask ), we deviate from the default recipe above as follows. When training with data from the first and second data engine stages only, we augment the input with large-scale jitter~ with a scale range of [0.1, 2.0]. Intuitively, data augmentation may be helpful when training data is more limited. To train ViT-B and ViT-L, we use 180k iterations with batch size 128 distributed across 128 GPUs. We set = /, = 0.6/0.8, = 0.1, and = 0.6/0.4 for ViT-B/L, respectively. Automatic Mask Generation Details Here we discuss details of the data engine's fully automatic stage that was used to generate the released . Cropping. Masks were generated from a regular grid of 3232 points on the full image and 20 additional zoomed-in image crops arising from 22 and 44 partially overlapping windows using 1616 and 88 regular point grids, respectively. The original high-resolution images were used for cropping (this was the only time we used them). We removed masks that touch the inner boundaries of the crops. We applied standard greedy box-based NMS (boxes were used for efficiency) in two phases: first within each crop and second across crops. When applying NMS within a crop, we used the model's predicted IoU to rank masks. When applying NMS across crops, we ranked masks from most zoomed-in (, from a 44 crop) to least zoomed-in (, the original image), based on their source crop. In both cases, we used an NMS threshold of 0.7. Filtering. We used three filters to increase mask quality. First, to keep only confident masks we filtered by the model's predicted IoU score at a threshold of 88.0. Second, to keep only stable masks we compared two binary masks resulting from the same underlying soft mask by thresholding it at different values. We kept the prediction (, the binary mask resulting from thresholding logits at 0) only if the IoU between its pair of -1 and +1 thresholded masks was equal to or greater than 95.0. Third, we noticed that occasionally an automatic mask would cover the entire image. These masks were generally uninteresting, and we filtered them by removing masks that covered 95Postprocessing. We observed two error types that are easily mitigated with postprocessing. First, an estimated 4Automatic mask generation model. We trained a special version of for fully automatic mask generation that sacrifices some inference speed for improved mask generation properties. We note the differences between our default and the one used for data generation here: it was trained on manual and semi-automatic data only, it was trained for longer (177656 iterations instead of 90k) with large-scale jitter data augmentation~, simulated interactive training used only point and mask prompts (no boxes) and sampled only 4 points per mask during training (reducing from our default of 9 to 4 sped up training iterations and had no impact on 1-point performance, though it would harm mIoU if evaluating with more points), and finally the mask decoder used 3 layers instead of 2. examples. We show samples in fig:sa1bvisuals. For more examples, please see our https://www.segment-anything.com/dataset/index.htmldataset explorer. RAI Additional Details Inferring geographic information for . While the images in are not geo-tagged, each image has a caption describing its contents and where it was taken. We infer approximate image geo-locations from these captions using an Elmo-based named entity recognition model~. Each extracted location entity is mapped to every matching country, province, and city. Captions are mapped to a single country by first considering the matching countries, then provinces, and finally cities. We note that there are ambiguities and potential for biases with this method (, ``Georgia'' may refer to the country or the US state). As such, we use the extracted locations to analyze the dataset as a whole, but do not release the inferred locations. The captions will not be released publicly as required by the image provider. Inferring geographic information for COCO and Open Images. The COCO~ and Open Images~ datasets do not provide geo-locations. Following~, we retrieve geographic metadata using the Flickr API. We retrieved locations for 24Inferring income information. We use each image's inferred country to look up its income level using the levels defined by The World Bank~. We collapse the upper-middle and lower-middle levels into a single middle level. Fairness in segmenting people. To investigate 's fairness at segmenting people we use the More Inclusive Annotations for People (MIAP)~ test set annotations for Open Images~, which allows us to compare 's performance across perceived gender presentation and perceived age group. MIAP provides box annotations, while we need ground truth masks for this analysis. To get ground truth masks, we select each person-category mask from Open Images if its corresponding bounding box is within a 1Fairness in segmenting clothing. We extend our analysis from to clothing segmentation. We look at 's performance on clothing relative to the attributes of those wearing the clothes. We use all 6.5k ground truth masks from Open Images that have a category under the clothing superclass and reside within a person box from MIAP. In Table~ we compare performance across perceived gender presentation and age group. We find that is better at segmenting clothing on those who present predominantly masculine, with disjoint 95Experiment Implementation Details Zero-Shot Single Point Valid Mask Evaluation Datasets. We built a new segmentation benchmark to evaluate the zero-shot transfer capabilities of our model using a suite of 23 diverse segmentation datasets from prior work. A description of each dataset is given in Table~. For examples, see main text fig:benchmark_examples. This suite covers a range of domains including egocentric~, microscopy~, X-ray~, underwater~, aerial~, simulation~, driving~, and painting~ images. For efficient evaluation we subsampled datasets with more than 15k masks. Specifically, we randomly picked images so that the total number of masks in the sampled images was 10k. We blurred faces of people in all the datasets. Point sampling. Our default point sampling follows standard practice in interactive segmentation~. The first point is chosen deterministically as the point farthest from the object boundary. Each subsequent point is the farthest from the boundary of the error region between ground truth and the previous prediction. Some experiments (where specified) use a more challenging sampling strategy in which the first point is a random point, rather than a deterministically selected ``center'' point. Each subsequent point is selected as described above. This setting better reflects use cases in which the first point is not reliably near the center of the mask, such as prompting from eye gaze. Evaluation. We measure IoU between a prediction after point prompts and a ground truth mask, where and points are sampled iteratively with either of the strategies described above. The per-dataset mIoU is the per-mask IoU averaged across all objects in the dataset. Finally, we report the top-line metric by averaging the per-dataset mIoUs across all 23 datasets. Our evaluation differs from the standard interactive segmentation evaluation protocol which measures the average number of points needed to achieve Baselines. We use three recent strong interactive baselines: RITM~, FocalClick~, and SimpleClick~. For each, we use the largest models trained on the broadest datasets publicly released by the authors. For RITM, we use HRNet32 IT-M trained on the combination of COCO~ and LVIS~ introduced by the authors. For FocalClick, we use SegFormerB3-S2 trained on a ``combined dataset'' that includes 8 different segmentation datasets~. For SimpleClick, we use ViT-H448 trained on a combination of COCO and LVIS. We follow the suggested default strategies for data pre-processing (, data augmentations or image resizing) and do not change or adapt any parameters for our evaluation. In our experiments, we observe that RITM outperforms other baselines on our 23 dataset suite with 1 point evaluation. Therefore, we use RITM as the default baseline. When evaluating with more points we report results for all baselines. Single point ambiguity and oracle evaluation. In addition to IoU after points prompts, we report 's ``oracle'' performance at 1 point by evaluating the predicted mask that best matches ground truth from amongst 's three predictions (rather than using the one that itself ranks first, as we do by default). This protocol addresses possible single point prompt ambiguity by relaxing the requirement to guess the one right mask among several valid objects. Zero-Shot Edge Detection Dataset and metrics. We perform zero-shot edge detection experiments on BSDS500~. The ground truth for each image comes from the manual annotations of five different subjects. We report results on the 200 image test subset using the four standard metrics for edge detection~: optimal dataset scale (ODS), optimal image scale (OIS), average precision (AP), and recall at 50Method. For zero-shot transfer, we use a simplified version of our automatic mask generation pipeline. We prompt with a 1616 regular grid of foreground points, which yields 768 predicted masks (three per point). We do not filter by predicted IoU or stability. Redundant masks are removed by NMS. Then we apply a Sobel filter to the remaining masks' unthresholded probability maps and set values to zero if they do not intersect with the outer boundary pixels of a mask. Finally, we take a pixel-wise max over all the predictions, linearly normalize the result to [0,1], and apply edge NMS~ to thin the edges. Visualizations. In fig:more_edges, we show additional examples of zero-shot edge predictions from . These qualitative examples further illustrate how tends to output sensible edge maps, despite not being trained for edge detection. We see that the edges can align well with the human annotations. Although, as previously mentioned, since is not trained for edge detection it does not learn the biases of the BSDS500 dataset and often outputs more edges than are present in the ground truth annotations. Zero-Shot Object Proposals Dataset and metrics. We report the standard average recall (AR) metric for masks at 1000 proposals on the LVIS v1 validation set~. Since LVIS has high-quality masks for 1203 object classes, it provides a challenging test for object proposal generation. We focus on AR@1000 due to the open-world nature of our model, which will likely produce many valid masks outside even the 1203 classes in LVIS. To measure performance on frequent, common, and rare categories, we use AR@1000 but measured against a ground truth set containing just the corresponding LVIS categories. Baseline. We use cascade ViTDet-H as a baseline, the strongest model from~ by AP on LVIS. As noted in the main text, an object detector trained in-domain can ``game'' AR~ and is expected to be a stronger baseline than other models that focus on open-world proposals or segmentation~. To produce 1000 proposals, we disable score thresholding in the three cascade stages and as raise the maximum number of predictions per stage to 1000. Method. We use a modified version of 's automatic mask generation pipeline for zero-shot transfer. First, to make inference time comparable to that of ViTDet we do not process image crops. Second, we remove filtering by predicted IoU and stability. This leaves two tunable parameters to get 1000 masks per image: the input point grid and the NMS threshold duplicate mask suppression. We choose a 6464 point grid and an NMS threshold of 0.9, which produces 900 masks per image on average. At evaluation, if greater than 1000 masks have been proposed in an image, they are ranked by the average of their confidence and stability scores, then truncated to the top 1000 proposals. We hypothesize that 's ability to output multiple masks is especially valuable for this task, since recall should benefit from proposals generated at multiple scales from a single input point. To test this, we compare to an ablated version that only outputs a single mask instead of three (- single-output). Since this model produces fewer masks, we further increase the number of points sampled and NMS threshold to 128128 and 0.95, respectively, obtaining 950 masks per image on average. Additionally, single-output does not produce the IoU score used to rank masks for NMS in the automatic mask generation pipeline, so instead masks are ranked randomly. Testing suggests this has similar performance to more sophisticated methods of ranking masks, such as using the max logit value of the mask as a proxy for model confidence. Zero-Shot Instance Segmentation Method. For zero-shot instance segmentation, we prompt with the boxes output by a fully-supervised ViTDet-H on COCO and LVIS v1 validation splits. We apply an additional mask refinement iteration by feeding the most confident predicted mask, together with the box prompt, back to the mask decoder to produce the final prediction. We show zero-shot instance segmentations predicted on LVIS in fig:instanceseg. Compared to ViTDet, tends to produce higher quality masks with cleaner boundaries. We confirm this observation with human studies in . Note that as a zero-shot model, is not able to learn annotation biases in a dataset. For instance, we see that makes a valid modal prediction for the plate, whereas LVIS masks cannot contain holes by design so the plate is annotated amodally. Zero-Shot Text-to-Mask Model and training. We use the largest publicly available CLIP model~ ( ViT-L/14@336px) to compute text and image embeddings, which we normalize prior to use. To train , we use masks from the first two stages of our data engine. Moreover, we discard all masks with an area smaller than pixels. We train this model with large-scale jitter~ for 120k iterations with batch size 128. All other training parameters follow our default settings. Generating training prompts. To extract an input prompt we first expand the bounding box around each mask by a random factor from 1 to 2, square-crop the expanded box to maintain its aspect ratio, and resize it to 336336 pixels. Before feeding the crop to the CLIP image encoder, with 50Inference. During inference we use the CLIP text encoder without any modifications to create a prompt for . We rely on the fact that text and image embeddings are aligned by CLIP, which allows us to train without any explicit text supervision while using text-based prompts for inference. Probing the Latent Space of Finally, we perform an initial investigation to qualitatively probe the latent space learned by . In particular, we are interested in whether is able to capture any semantics in its representation even though is not trained with explicit semantic supervision. To do so, we compute mask embeddings by extracting an image embedding from from an image crop around a mask and its horizontally flipped version, multiplying the image embedding by the binary mask, and averaging over spatial locations. In fig:latent_visualization, we show 3 examples of a query mask and similar masks (in the latent space) in the same image. We observe that the nearest neighbors for each query show some, albeit imperfect, shape and semantic similarity. Although these results are preliminary, they indicate that the representations from may be useful for a variety of purposes, such as further data labeling, understanding the contents of datasets, or as features for downstream tasks. Human Study Experimental Design Here we describe details of the human study used to evaluate mask quality in and . The purpose of the human study is to address two limitations of using IoU to ground truth as a measure of predicted mask quality. The first limitation is that, for ambiguous inputs such as a single point, the model may be strongly penalized for returning a valid mask of a different object than the ground truth. The second limitation is that ground truth masks may include various biases, such as systematic errors in the edge quality or decisions to modally or amodally segment occluding objects. A model trained in-domain can learn these biases and obtain a higher IoU without necessarily producing better masks. Human review can obtain a measure of mask quality independent of an underlying ground truth mask in order to alleviate these issues. Models. For single-point evaluation, we use RITM~, single-output , and to test two hypotheses. First, we hypothesize that produces visually higher quality masks than baseline interactive segmentation models when given a single point, even when metrics such as IoU with ground truth do not reveal this. Second, we hypothesize that 's ability to disambiguate masks improves mask quality for single point inputs, since single output may return masks that average over ambiguous masks. For instance segmentation experiments, we evaluate cascade ViTDet-H~ and in order to test the hypothesis that produces visually higher quality masks, even if it obtains a lower AP due to the inability to learn specific annotation biases of the validation dataset. Datasets. For single-point experiments, we select 7 datasets from our set of 23 datasets, since the full suite is too large for human review. We choose LVIS v0.5~, VISOR~, DRAM~, IBD~, NDD20~, OVIS~, and iShape~, which provide a diverse collection of images, including scene-level, ego-centric, drawn, overhead, underwater, and synthetic imagery. Additionally, this set includes datasets both where outperforms RITM with IoU metrics and vice-versa. For instance segmentation experiments, we use the LVIS v1 validation set, allowing for direct comparison to ViTDet, which was trained on LVIS. Methodology. We presented masks generated by the models to professional annotators and asked them to rate each mask using provided guidelines (see for the complete guidelines). Annotators were sourced from the same company that collected manually annotated masks for the data engine. An annotator was provided access to an image, the predicted mask of a single model, and the input to the model (either a single point or single box) and asked to judge the mask on three criterion: Does the mask correspond to a valid object? Does the mask have a clean boundary? and Does the mask correspond to the input? They then submitted a rating from 1-10 indicating the overall mask quality. A score of 1 indicates a mask that corresponds to no object at all; a low score (2-4) indicates that the mask has huge errors, such including huge regions of other objects or having large areas of nonsensical boundaries; a middle score (5-6) indicates masks that are mostly sensible but still have significant semantic or boundary errors; a high score (7-9) indicates masks with only minor boundary errors; and a score of 10 is for masks with no visible errors. Annotators were provided with five different views, each designed to help identify different error types. For single point experiments, 1000 masks per dataset were selected randomly from the same subsets used for benchmarking zero-shot interactive segmentation (see for details on these subsets). The model input was the centermost point, calculated as the largest value of the distance transform from the edge of the mask. For instance segmentation experiments, 1000 masks were selected from the LVIS v1 validation set, and the model input was the LVIS ground truth box. In all experiments, masks with a size smaller than pixels were excluded from sampling, to prevent showing raters a mask that was too small to judge accurately. For both memory and display reasons, large images were rescaled to have a max side-length of 2000 before predicting a mask. In all experiments, the same inputs were fed to each model to produce a predicted mask. For comparison, the ground truth masks from each dataset were also submitted for rating. For single-point experiments, this gave 4000 total rating jobs per dataset (1000 masks each for RITM, single-output, , and ground truth); for instance segmentation experiments, it gave 3000 total jobs (ViTDet, , and ground truth). For each dataset, these jobs were inserted with random ordering into a queue from which 30 annotators drew jobs. In initial testing of the review study, we provided each job to five different annotators and found reasonable consistency in scores: the average standard deviation in score over the five annotators was 0.83. Additionally, the annotation company deployed quality assurance testers who spot checked a fraction of results for extreme departures from the guidelines. Thus for our experiments each job (, rating one mask in one image) was completed by only a single annotator. Average time spent per annotator per job was 90 seconds, longer than our initial target of 30 seconds, but still sufficiently fast to collect a large number of ratings on each of the 7 selected datasets. Results. fig:humanstudy:points shows histograms over ratings for each dataset in the single-point experiments. We run statistical tests for two hypotheses: (1) that gets higher scores than the baseline model (RITM or ViTDet) and (2) that gets higher scores than single-output . P-values are calculated via a paired t-test on the means of the model scores, which we supplement with a paired bootstrap test on 10k samples to find the 99For instance segmentation, fig:humanstudy:inst of the main text shows the histogram for ratings. To compare to COCO ground truth, we additionally include 794 ratings of COCO ground truth masks that were collected during our testing of the human review process. These masks were presented to raters using an identical setup as the LVIS results. For fair comparison, results for LVIS in fig:humanstudy:inst were subsampled to the same 794 inputs for each model and ground truth. For Table , the full 1000 ratings are used to run statistical tests, which show that 's mask quality improvement over ViTDet is statistically significant. Dataset, Annotation, and Model Cards In we provide a Dataset Card for , following~, in a list of questions and answers. Next, we provide a Data Annotation Card in for the first two stages of our data engine described in , following CrowdWorkSheets~, again as a list of questions and answers. We provide a Model Card following~ in Table~. Dataset Card for 7.58.4 Motivation For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The contributions of our dataset to the vision community are fourfold: (1) We release a dataset of 11M images and 1.1B masks, by far the largest segmentation dataset to date. (2) The dataset we release is privacy protecting: we have blurred faces and license plates in all images. (3) The dataset is licensed under a broad set of terms of use which can be found at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything. (4) The data is more geographically diverse than its predecessors, and we hope it will bring the community one step closer to creating fairer and more equitable models. Who created the dataset (, which team, research group) and on behalf of which entity (, company, institution, organization)? The dataset was created by the FAIR team of Meta AI. The underlying images were collected and licensed from a third party photo company. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. Meta AI funded the creation of the dataset. Any other comments? No. Composition What do the instances that comprise the dataset represent (, documents, photos, people, countries)? Are there multiple types of instances (, movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. All of the instances in the dataset are photos. The photos vary in subject matter; common themes of the photo include: locations, objects, scenes. All of the photos are distinct, however there are some sets of photos that were taken of the same subject matter. How many instances are there in total (of each type, if appropriate)? There are 11 million images. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (, geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (, to cover a more diverse range of instances, because instances were withheld or unavailable). The dataset is composed of images licensed from a photo provider. The dataset contains all instances licensed. The images are photos, not artwork, although there are a few exceptions. The dataset includes all generated masks for each image in the dataset. We withheld 2k randomly selected images for testing purposes. What data does each instance consist of? ``Raw'' data (, unprocessed text or images) or features? In either case, please provide a description. Each instance in the dataset is an image. The images were processed to blur faces and license plates to protect the identities of those in the image. Is there a label or target associated with each instance? If so, please provide a description. Each image is annotated with masks. There are no categories or text associated with the masks. The average image has 100 masks, and there are 1.1B masks in total. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (, because it was unavailable). This does not include intentionally removed information, but might include, , redacted text. Yes. Each image is accompanied by a short caption that describes the content and place of the photo in a free form text. Per our agreement with the photo provider we are not allowed to release these captions. However, we use them in our paper to analyze the geographical distribution of the dataset. Are relationships between individual instances made explicit (, users' movie ratings, social network links)? If so, please describe how these relationships are made explicit. No, there are no known relationships between instances in the dataset. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. Errors: The masks are generated by a segmentation model, so there may be errors or inconsistencies in the masks. Redundancies: While no two images are the same, there are instances of images of the same subject taken close together in time. Is the dataset self-contained, or does it link to or otherwise rely on external resources (, websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (, including the external resources as they existed at the time the dataset was created); c) are there any restrictions (, licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset is self-contained. Does the dataset contain data that might be considered confidential (, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description. No. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. We have two safety measures to prevent objectionable content: (1) Photos are licensed from a photo provider and had to meet the terms of service of the photo provider. We requested that all objectionable content be filtered from the images we licensed. (2) If a user observes objectionable image(s) in the dataset, we invite them to report the image(s) at mailto:segment-anything@meta.comsegment-anything@meta.com for removal. Despite the measures taken, we observe that a small portion of images contains scenes of protests or other gatherings that focus on a diverse spectrum of religious beliefs or political opinions that may be offensive. We were not able to produce a filtering strategy that removes all such images and rely on users to report this type of content. Does the dataset identify any subpopulations (, by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. The dataset does not identify any subpopulations of the people in the photos. Is it possible to identify individuals (, one or more natural persons), either directly or indirectly (, in combination with other data) from the dataset? If so, please describe how. No. Images were subjected to a face blurring model to remove any personally identifiable information. If a user observes any anonymization issue, we invite them to report the issue and the image id(s) at mailto:segment-anything@meta.comsegment-anything@meta.com. Does the dataset contain data that might be considered sensitive in any way (, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. The dataset contains scenes of protests, or other gatherings that may suggest religious beliefs, political opinions or union memberships. However, the faces of all people in the dataset have been anonymized via facial blurring, so it is not possible to identify any person in the dataset. Any other comments? No. Collection Process How was the data associated with each instance acquired? Was the data directly observable (, raw text, movie ratings), reported by subjects (, survey responses), or indirectly inferred/derived from other data (, part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The released masks associated with each image were automatically inferred by our segmentation model, . The masks that were collected using model-assisted manual annotation will not be released. Quality was validated as described in . What mechanisms or procedures were used to collect the data (, hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated? The images in the dataset are licensed from an image provider. They are all photos taken by photographers with different cameras. If the dataset is a sample from a larger set, what was the sampling strategy (, deterministic, probabilistic with specific sampling probabilities)? We withheld 2k randomly selected images for testing purposes. The rest of the licensed images are included in the dataset. Who was involved in the data collection process (, students, crowdworkers, contractors) and how were they compensated (, how much were crowdworkers paid)? The released masks were automatically inferred by . For details on our model-assisted manual annotation process see our Data Annotation Card in . Note these masks will not be released. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (, recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. The licensed photos vary in their date taken over a wide range of years up to 2022. Were any ethical review processes conducted (, by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. If the dataset does not relate to people, you may skip the remaining questions in this section. We underwent an internal privacy review to evaluate and determine how to mitigate any potential risks with respect to the privacy of people in the photos. Blurring faces and license plates protects the privacy of the people in the photos. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (, websites)? We licensed the data from a third party photo provider. Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. The images are licensed from a third party who provided appropriate representations regarding the collection of any notices and consents as required from individuals. In addition, all identifiable information ( faces, license plates) was blurred. Under the terms of the dataset license it is prohibited to attempt to identify or associate an image with a particular individual. Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. The images are licensed from a third party who provided appropriate representations regarding the collection of any notices and consents as required from individuals. In addition, all identifiable information ( faces, license plates) was blurred from all images. For avoidance of doubt, under the terms of the dataset license it is prohibited to attempt to identify or associate an image with a particular individual. If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). We invite users to report at mailto:segment-anything@meta.comsegment-anything@meta.com for image(s) removal. Has an analysis of the potential impact of the dataset and its use on data subjects (, a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. To eliminate any potential impact on people whose photos are included in the dataset, identifiable information (faces, license plates) has been blurred. Any other comments? No. Preprocessing / Cleaning / Labeling Was any preprocessing / cleaning / labeling of the data done (, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section. We resized the high-resolution licensed images such that the shorter side is 1500 pixels and only processed the images to remove any identifiable and personal information from the photos (faces, license plates). Was the ``raw'' data saved in addition to the preprocessed/cleaned/labeled data (, to support unanticipated future uses)? If so, please provide a link or other access point to the ``raw'' data. No, as we removed the data for safety reasons and to respect privacy, we do not release the unaltered photos. Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point. We used the RetinaFace~ model (https://github.com/serengil/retinafacehttps://github.com/serengil/retinaface) to detect faces. The model used to blur license plates has not been made public. Uses Has the dataset been used for any tasks already? If so, please provide a description. The dataset was used to train our segmentation model, . Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. No. However, all users of the dataset must cite it, so its use is trackable via citation explorers. What (other) tasks could the dataset be used for? We intend the dataset to be a large-scale segmentation dataset. However, we invite the research community to gather additional annotations for the dataset. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (, stereotyping, quality of service issues) or other risks or harms (, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms? We have an analysis of the approximate geographic and income level coverage of our dataset in . While we believe our dataset to be more representative than most of the publicly existing datasets at this time, we acknowledge that we do not have parity across all groups, and we encourage users to be mindful of potential biases their models have learned using this dataset. Are there tasks for which the dataset should not be used? If so, please provide a description. Full terms of use for the dataset including prohibited use cases can be found at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything. Any other comments? No. Distribution Will the dataset be distributed to third parties outside of the entity (, company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. The dataset will be available for the research community. How will the dataset will be distributed (, tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)? The dataset is available at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything. When will the dataset be distributed? The dataset will be released in 2023. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. Yes. The license agreement and terms of use for the dataset can be found at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything. Users must agree to the terms of use before downloading or using the dataset. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. Full terms of use and restrictions on use of the dataset can be found at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. The license and restrictions on use of the SA-1B dataset can be found at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything. Any other comments? No. Maintenance Who will be supporting/hosting/maintaining the dataset? The dataset will be hosted at https://ai.facebook.com/datasets/segment-anythinghttps://ai.facebook.com/datasets/segment-anything and maintained by Meta AI. How can the owner/curator/manager of the dataset be contacted (, email address)? Please email mailto:segment-anything@meta.comsegment-anything@meta.com. Is there an erratum? If so, please provide a link or other access point. No. Will the dataset be updated (, to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (, mailing list, GitHub)? To aid reproducibility of research using , the only updates will be to remove reported images. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. There are no limits on data retention. We took measures to remove personally identifiable information from any images of people. Users may report content for potential removal here: mailto:segment-anything@meta.comsegment-anything@meta.com. Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers. No, as the only updates will be to remove potentially harmful content, we will not keep older versions with the content. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description. We encourage users to gather further annotations for . Any users who generate annotations will be liable for hosting and distributing their annotations. Any other comments? No. Data Annotation Card 7.58.4 Task Formulation At a high level, what are the subjective aspects of your task? Segmenting objects present in an image is inherently a subjective task. For instance, one annotator may segment two boots as one mask, whereas another may segment each boot separately. Depending on annotators's skills, the quality of the mask and the number of masks per image are different between annotators. Despite these subjective aspects of the task, we believed efficient annotation was possible as the data was annotated in a per-mask fashion with the main focus on the diversity of the data rather than completeness. What assumptions do you make about annotators? Our annotators worked full time on our annotation task with very small attrition rate. This made it possible to train the annotators providing feedback and answering their questions on a regular basis. Specifically: (1) By giving a clear understanding of the goals of this work and providing clear guidelines, including visuals and video recordings of the tasks, annotators had enough context to understand and perform the tasks reasonably. (2) Sharing objectives and key results and meeting weekly with annotators increased the likelihood that annotators improved annotation quality and quantity over time. How did you choose the specific wording of your task instructions? What steps, if any, were taken to verify the clarity of task instructions and wording for annotators? As our task was annotating images, the annotation guidelines included visual examples. Our research team completed 30 annotation tasks to identify any obvious challenges using the annotation tool, collectively decide how to handle complex cases, and refine the guidelines. The research team met with the annotators weekly for feedback sessions. Videos of the research team performing the task were shared live with the annotators, followed by QA sessions. Annotators were able to give feedback on unclear aspects, both during the feedback session and asynchronously. What, if any, risks did your task pose for annotators and were they informed of the risks prior to engagement with the task? No identified risks. Images were filtered for objectionable content prior to the annotation phase. What are the precise instructions that were provided to annotators? We provide only high-level instructions: Given an image, we aim at segmenting every possible object. Annotators generate a mask for every potential object they can identify. An object can be segmented using our interactive segmentation tool either by using corrective foreground/background clicks to add/remove parts of the mask or by drawing a bounding box around the object. Masks can be refined using pixel-precise tools. Selecting Annotations Are there certain perspectives that should be privileged? If so, how did you seek these perspectives out? We chose to work with annotators that have worked on other vision annotation tasks before. Are there certain perspectives that would be harmful to include? If so, how did you screen these perspectives out? No. Were sociodemographic characteristics used to select annotators for your task? If so, please detail the process. No. If you have any aggregated socio-demographic statistics about your annotator pool, please describe. Do you have reason to believe that sociodemographic characteristics of annotators may have impacted how they annotated the data? Why or why not? We worked with 130 annotators. The annotators were all based in Kenya. We do not believe sociodemographic characteristics of annotators meaningfully impacted the annotated data. Consider the intended context of use of the dataset and the individuals and communities that may be impacted by a model trained on this dataset. Are these communities represented in your annotator pool? The Segment Anything 1B () dataset is to be used for research purposes only. The dataset is one of the most geographically diverse segmentation dataset, as discussed in . In addition, we analyze the responsible AI axes of a model trained on the dataset in . Platform and Infrastructure Choices What annotation platform did you utilize? At a high level, what considerations informed your decision to choose this platform? Did the chosen platform sufficiently meet the requirements you outlined for annotator pools? Are any aspects not covered? We used a proprietary annotation platform. What, if any, communication channels did your chosen platform offer to facilitate communication with annotators? How did this channel of communication influence the annotation process and/or resulting annotations? We manually reviewed annotations and shared feedback with the annotators on a weekly basis. We communicated common mistakes or inconsistencies and the corresponding corrections. In addition, the annotators were given feedback for improvements daily by the annotation QA team. Outside the weekly feedback sessions, annotators had access to a spreadsheet and chat group to facilitate communication with the research team. This process greatly improved the average speed and quality of the annotations. How much were annotators compensated? Did you consider any particular pay standards, when determining their compensation? If so, please describe. Annotators were compensated with an hourly wage set by the vendor. The vendor is a Certified B Corporation. Dataset Analysis and Evaluation How do you define the quality of annotations in your context, and how did you assess the quality in the dataset you constructed? Annotators were first placed into training. They followed a 1-day training session led by the vendor and then were asked to annotate a large number of examples from a training queue. Annotators graduated from training to production after the vendor QA team, in collaboration with the research team, manually spot-checked the annotator’s masks to ensure quality. On average, annotators spent one week in training before graduating. Production quality assessment followed a similar process: the vendor QA team and the research team manually reviewed the annotations weekly, sharing feedback weekly. Have you conducted any analysis on disagreement patterns? If so, what analyses did you use and what were the major findings? Did you analyze potential sources of disagreement? We pointed out common mistakes during weekly meetings with the annotators. How do the individual annotator responses relate to the final labels released in the dataset? The annotations were only used to train early versions of the model and we do not currently plan to release them. Dataset Release and Maintenance Do you have reason to believe the annotations in this dataset may change over time? Do you plan to update your dataset? No, except to remove objectionable images. Are there any conditions or definitions that, if changed, could impact the utility of your dataset? We do not believe so. Will you attempt to track, impose limitations on, or otherwise influence how your dataset is used? If so, how? The SA-1B dataset will be released under a license agreement allowing use for certain research purposes and protections for researchers. Researchers must agree to the terms of the license agreement to access the dataset. Were annotators informed about how the data is externalized? If changes to the dataset are made, will they be informed? No, we do not plan to release the manual annotations at the moment. Is there a process by which annotators can later choose to withdraw their data from the dataset? If so, please detail. No. [2][t].24 -6mm* #21mm1.5mm [3][t].24 [b] -6mm* #31mm1.5mm [2][t].24 [b]33.6#22mm -6mm* #11mm1.5mm Annotation Guidelines We provide the complete guidelines given to annotations for the human review of mask quality in fig:guidelines_a and fig:guidelines_b.",
    "idea_abstract": "We propose the Segment Anything project, a new task and model for image segmentation that enables zero-shot transfer to new image distributions and tasks. Our model is designed to be promptable and can be fine-tuned for various applications. We introduce a large-scale dataset for image segmentation, which we believe will facilitate research into foundation models for computer vision.",
    "proposal": "1. Title: Building a Foundation Model for Image Segmentation\n\n2. Problem Statement: Image segmentation is a fundamental task in computer vision that involves partitioning an image into its constituent parts or objects. However, existing methods often require large amounts of annotated data and are limited in their ability to generalize to new tasks and image distributions. This problem is interesting and important because it has numerous applications in fields such as autonomous driving, medical imaging, and robotics.\n\n3. Motivation: The success of large language models in natural language processing has inspired the development of foundation models in computer vision. These models are pre-trained on broad data and can adapt to a wide range of downstream tasks via prompt engineering. However, existing foundation models for computer vision are limited in their ability to generalize to new tasks and image distributions. Our goal is to build a foundation model for image segmentation that can perform zero-shot and few-shot learning for new datasets and tasks.\n\n4. Proposed Method: We propose a promptable segmentation task, where the goal is to return a valid segmentation mask given any segmentation prompt. We use a simple design that satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts"
  },
  {
    "rank": 8,
    "title": "Judging LLM-as-a-Judge with MTBench and Chatbot Arena",
    "arxiv": "2306.05685v2",
    "paper_tar": "tar_files/2306.05685v2.tar.gz",
    "abstract": "",
    "content": "article [preprint]neurips_data_2023 graphicx subcaption [utf8]inputenc [T1]fontenc hyperref url booktabs amsfonts nicefrac microtype xcolor booktabs multirow cleveref enumitem listings float noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=1.5em Vicuna MT-bench yes [1]yesred TODO: #1 [1]yesblue (Joey: #1) [1]yesblue Lianmin: #1 [1]yesblue Ion: #1 [1]yesblue Hao: #1 [1]yesblue Dacheng: #1 [1]yescyan Wei-Lin: #1 [1]yespurple Siyuan: #1 Judging LLM-as-a-judge with MT-Bench and Chatbot Arena Lianmin ZhengJoint first authors. This paper is an extended version of our earlier blog post~. Wei-Lin Chiang Ying Sheng Siyuan Zhuang Zhanghao Wu Yonghao Zhuang Zi Lin Zhuohan Li Dacheng Li Eric. P Xing Hao Zhang Joseph E. Gonzalez Ion Stoica UC Berkeley UC San Diego Carnegie Mellon University Stanford MBZUAI",
    "idea_abstract": "Question:\nWhat is the square root of 1776 to the nearest integer?\nAnswer:\n42",
    "proposal": "Here is the rewritten abstract in the format of an idea abstract:\n\n1. **Evaluating Large Language Models as Judges**\n\n2. Problem Statement: Evaluating the performance of large language models as judges in various tasks is a crucial challenge in natural language processing and artificial intelligence. The ability to accurately assess the quality of text generated by these models is essential for their deployment in real-world applications, such as content moderation, language translation, and text summarization. However, existing evaluation methods often rely on human judgments, which can be time-consuming, expensive, and prone to bias.\n\n3. Motivation: Current methods for evaluating LLMs as judges are limited in their ability to capture the nuances of human judgment and may not accurately reflect the performance of these models in real-world scenarios. This is because human evaluators may have varying levels of expertise, and their judgments can be influenced by personal biases and contextual factors. Moreover, existing evaluation metrics often focus on specific aspects of model performance, such as fluency or coherence, without considering the broader context in which the model will be used.\n\n4. Proposed Method: We propose a novel approach to evaluating LLMs as judges using a combination of automated evaluation tools and human feedback. Our method involves using a benchmarking platform, MT-Bench"
  },
  {
    "rank": 9,
    "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
    "arxiv": "2302.04023v2",
    "paper_tar": "tar_files/2302.04023v2.tar.gz",
    "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\\",
    "content": "=1 [11pt]article ACL2023 times latexsym lipsum cleveref amssymb pifont adjustbox multicol multirow graphicx microtype footmisc ctable [normalem]ulem longtable pdflscape CJKutf8 array *[1] 55 [labelfont=bf]caption subcaption mathrsfs bbding pifont soul [T1]fontenc [utf8]inputenc microtype inconsolata algorithm algpseudocode amssymb xurl greencustomHTML008000 [1]greencustom#1 [1]gray#1 [1]greencustom#1 [1]greencustom#1 [1]orange[Note: #1] [1]orange#1 [1]greencustom#1 [1]red[Pascale: #1] [1]red#1 [1]greencustom#1 A Multitask, Multilingual, Multimodal Evaluation of ChatGPT Reasoning, Hallucination, and Interactivity Yejin Bang~Samuel Cahyawijaya~Nayeon Lee~Wenliang Dai~Dan Su~Bryan Wilie Holy Lovenia~Ziwei Ji~Tiezheng Yu~Willy Chung~Quyet V. Do~Yan Xu~Pascale FungCentre for Artificial Intelligence Research (CAiRE)Hong Kong University of Science and Technology yjbang@connect.ust.hk, pascale@ece.ust.hk This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8Introduction ChatGPT is a successor of the large language model (LLM) InstructGPT~ with a dialog interface that is fine-tuned using the Reinforcement Learning with Human Feedback (RLHF)~ approach.https://beta.openai.com/docs/ model-index-for-researchers In the last couple of months, ChatGPT has gathered close to 1 million user base and is being used by businesses and consumers alike for a myriad of mostly textual tasks. One reason for its unprecedented popularity is that ChatGPT, through its scale and via RLHF, has shown impressive abilities in many areas of NLP as well as emergent abilities such as code generation and multimodal generation. Another reason is that its dialog interface allows users to interact with the underlying large language model more effectively and efficiently via interactive chats that are akin to multi-turn prompt engineering. However, despite its powerful abilities, anecdotal reports on ChatGPT have consistently shown significant remaining challenges - for example, it fails in some elementary mathematical~ and commonsense reasoning tasks~; it hallucinates with human-like fluency and eloquence on things that are not based on truth~; and as a general-purpose language model trained from everything on the web, its language coverage is questionable~. OpenAI has listed many limitations of ChatGPT on its website. CEO tweeted that ``It's a mistake to be relying on [ChatGPT] for anything important right now''~. Many researchers have argued that, despite appearances, LLMs like ChatGPT are only good at language abilities, not actual reasoning~. Consequently, it is not clear what people can or cannot use it for despite its popularity. For users and researchers alike, it would be beneficial to have a sense of confidence in its reliability in various NLP/AI tasks. Previous works have discussed the ethical implications or concerns associated with ChatGPT (and other LLMs)~. However, there has not been much technical evaluation of the strengths and limitations of ChatGPT. To fill this gap, we conduct experiments on ChatGPT with samples from standard public test sets on major NLP tasks such as question answering, reasoning, summarization, machine translation, automatic post-editing, sentiment analysis, language identification, and task-oriented dialogue (dialogue state tracking response generation) and misinformation detection. We evaluate its multilingual performance as well as vision-language multimodal abilities. With additional experiments, we also quantitatively evaluate its primary limitations in reasoning and hallucination. In addition, we conduct experiments to test its multi-turn interactivity as a means for better prompt engineering. We hope to provide insights to users of ChatGPT on the above-mentioned strengths and limitations, as well as how they can improve outcomes with interactivity. (Note that we are not able to quantitatively evaluate the RLHF aspect of ChatGPT without access to the user log. We hope OpenAI will publish this work and one can carry out such evaluations in the future in collaboration with OpenAI.) The following are the major insights we have gained from the evaluations: Multitask, Multimodal, and Multilingual For 9/13 NLP datasets, ChatGPT outperforms previous LLMs with zero-shot learning. It even outperforms fully fine-tuned task-specific LMs on 4 different tasks. In other cases, ChatGPT is on par or slightly lower than fully fine-tuned for specific NLP tasks; ChatGPT fails to generalize to low-resource and extremely low-resource languages (e.g., Marathi, Sundanese, and Buginese). There is an overall performance degradation in low-resource languages, especially in non-Latin scripts in the case of translation; its weakness lies in generation rather than understanding part of the translation process; ChatGPT enables a code intermediate medium to bridge vision and language, even though the multi-modality ability is still elementary compared to vision-language models. Reasoning We tested 10 different reasoning categories with 634 samples in total. Based on our experiments, ChatGPT shows more weakness in inductive reasoning than in deductive or abductive reasoning. ChatGPT also lacks spatial reasoning while showing better temporal reasoning. ChatGPT also lacks mathematical reasoning, which aligns with recent findings by mathchatgpt. Further, we found that ChatGPT is relatively better at commonsense reasoning than non-textual semantic reasoning. Finally, while ChatGPT shows acceptable performance in causal and analogical reasoning, it is bad at multi-hop reasoning capability as similar to other LLMs' weakness in complex reasoning~. Hallucination Similar to other LLMs , ChatGPT suffers from the hallucination problem. It generates more extrinsic hallucinations -- factual statements that cannot be verified from the source, from its parametric memory across all tasks since it does not possess the access to external knowledge bases. Interactivity One of the primary differentiating factors of ChatGPT from its predecessors is its multi-turn dialog interactivity. This enables ChatGPT to perform multiple tasks within a dialog session. There is also significant performance improvement (8Organization of This Paper: We first provide an overview of ChatGPT and related work (). Then, we provide evaluation results on ChatGPT on various application test sets, on multilingual test sets, and on a new multimodal task in . We then explore the three main strengths and weaknesses of ChatGPT, namely reasoning (), hallucination () and interactivity () in the subsequent three sections. Finally, we discuss and give a conclusion on our findings of ChatGPT. Background and Related Work Large Pretrained Models Large Language Models (LLMs) are language models with parameter sizes over a hundred billion, beginning with the introduction of GPT-3. Examples of LLMs include, but are not limited to, GPT-3, Gopher , Megatron , GPT-Jurassic , OPT-175B~. Beyond fine-tuning models with task-specific data, LLMs have shown robustness and generalizability through zero-shot and few-shot learning with examples. Scaling up the models unlocked new, emergent abilities that were not observed with smaller models . Prompts are used to probe the LLMs to generate the target outcome by sampling the language distribution. To enable the LLMs to demonstrate their abilities, sophisticated prompt engineering~ is required. However, previous LLMs only allow one-time probing, which means the target outcome varies a great deal with minor changes in the prompt instruction. Whereas scaling up LLMs improve generalizability, generic LLMs may fall short in specific applications. Despite its name, ChatGPT has not been primarily used as a chatbot. Its dialog ability serves as the user interface to the underlying LLM. We nevertheless refer to other dialog systems here in this paper. A number of large pre-trained dialogue models have been created, following the pre-train-then-finetune paradigm. LaMDA~ is a large-scale conversational model, fine-tuned from an LLM with a parameter size of 134 billion. Blenderbot 3.0 , scaled up to 175 billion parameter size, is also introduced with similar abilities as LAMDA. Both models are pre-trained on public dialogue and other public web documents and then fine-tuned with manually curated dialogue data. They also have access to external knowledge sources for information retrieval, thus they have shown an excellent ability for fluent and natural dialogue generation as well as information retrieval. However, the aforementioned large dialogue models suffer from catastrophic forgetting of the knowledge obtained from the pre-training. Models after fine-tuning show stable and strong performance on specific tasks, but they only preserve the knowledge learned from the task-specific data while losing the generalization ability. ChatGPT, on the other hand, was trained on a large-scale conversational-style dataset constructed from web documents directly~, which unifies the pre-training and fine-tuning data format. Thus, ChatGPT is able to preserve the knowledge from pre-training and produce informative outputs without access to external knowledge sources. ChatGPT Compared to existing LLMs, ChatGPT has unique characteristics. First, it has the ability to interact with users in a conversation-like manner, while retaining its accumulated knowledge and generalization ability gained from pre-training. This is achieved by pre-training ChatGPT on a large-scale conversational-style dataset, that is constructed by transforming a large-scale instruction-tuning corpus used for building InstructGPT into a conversational format, then fine-tuning the model based on a reward model to further improve the generation quality and align the generation with human preference. ChatGPT should be considered a generic language model which can be probed in a conversational manner. The biggest advantage of such conversational interaction is that, unlike previous LLMs, ChatGPT can intelligently ``answer follow-up questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests''~. Second, ChatGPT is trained with a better human-aligned objective function via Reinforcement Learning from Human Feedback (RLHF)~. Conventional natural language generation models, including dialogue models, are trained with maximum likelihood estimation (MLE) and might not be aligned with human preferences. For instance, for dialogue systems, humanness, engagement, and groundedness are some examples of essential criteria for success. Such discrepancy between training objectives and evaluation metrics becomes a bottleneck to performance improvement. By using RLHF, ChatGPT aligns more closely with human preferences in generating text than by using MLE. As ChatGPT has become available to public users through an easily accessible UI, there have been many discussions from a wide range of communities, not just from AI or NLP, but also from other disciplines. A line of discussion is the specific emergent ability and strength of ChatGPT in more technical perspectives. conducts linguistic analyses and human evaluations of ChatGPT's writing against human experts with their proposed corpus named Human ChatGPT Comparison Corpus and found that ChatGPT responses are strictly focused on the given question, more formal, objective, and less emotional. also studies ChatGPT's generated medical advice if it passes the Turing test. investigate mathematical capabilities of ChatGPT on both publicly available and hand-crafted datasets, including graduate-level mathematics, and show that ``significantly below those of an average mathematics graduate student.'' There are many investigations of ChatGPT's understanding and potential applications in different fields such as law~, medical domain~ and finance . conduct a case study of the application of ChatGPT on simplified radiology reports. Another important line of discussion is the ethical concerns over the use of ChatGPT. The most active discussion is over the use of academic writing and exam integrity . OpenAI also discusses the misuse of LM for disinformation and remedies. zhuo2023exploring study AI ethics of ChatGPT in criteria of bias, reliability, robustness, and toxicity. LLM benchmark and evaluation With the advancement of LLMs' generalization ability, there have been efforts to understand their capabilities, limitations, and risks. Recently, several benchmarks with a collection of a large number of NLP datasets, such as BIG-Bench and AI LM Harness , have been introduced. Moreover, HELM is proposed to conduct a holistic evaluation of LLMs that considers scenarios and metrics with a top-down approach. In this work, we instead focus on specific limitations and unique findings of ChatGPT that had not been discussed with previous LLMs. There is difficulty to evaluate ChatGPT with the whole test set from such benchmarks due to limited access to ChatGPT. There are also other works that discuss LLMs' emergent abilities through thorough surveys or case studies. thoroughly studies LLMs capabilities by distinguishing formal and functional linguistic competence with reference to cognitive science, psychology, and NLP to clarify the discourse surrounding LLMs' potential. Other works focus on more specific abilities such as mathematical skills , reasoning . Also, there have been overviews of existing LLMs Multitask, Multilingual, and Multimodal Evaluations of ChatGPT Evaluating the Multitask Ability of ChatGPT We take the average from the state-of-the-art zero-shot performance in CNN and DM from~. ChatGPT has become very well-known in such a short period of time to general public users, not just those who are in AI, machine learning, and NLP communities who might be more familiar with LLMs. One of the main reasons is that, in addition to media reports, innumerable use cases of ChatGPT are shared by both non-academic and academic users online~. There have been debates and panels on whether ChatGPT is approaching Artificial General Intelligence (AGI), as it seems to be able to carry out a multitude of tasks without specific fine-tuning~. On the other hand, there has also been as much sharing of its failures in simple tasks . Instead of relying on anecdotal examples, we first evaluate ChatGPT's performance in various standard NLP tasks in a zero-shot manner to obtain a basic/better understanding of its multi-task ability. We compile results from the existing literature on ChatGPT and compare them with the state-of-the-art fully-fine-tuned and zero-shot models across multiple tasks. We evaluate ChatGPT performances on 21 datasets covering 8 tasks, i.e., summarization, machine translation, sentiment analysis, questions answering, task-oriented dialogue, open-domain knowledge-grounded dialogue, and misinformation detection tasks. For ChatGPT, we sample testing cases from existing standard test sets for each task with a sample size ranging from 30 to 200 samples per task. Multitask Generalization of ChatGPT The result of the multitask evaluation is shown in~tab:multitask-performance. ChatGPT is shown to achieve remarkable zero-shot performances on multiple tasks, surpassing previous state-of-the-art zero-shot models on 9 out of 13 evaluation datasets with reported zero-shot LLMs performance. In most tasks, especially task-oriented and knowledge-grounded dialogue tasks, task-specific fully-fine-tuned models outperform ChatGPT. Compared to the latter, ChatGPT yields lower performance in most tasks while still surpassing the performance on 4 evaluation datasets. Furthermore, from the evaluation results, we also observe several limitations of ChatGPT, e.g., 1) limited language understanding and generation capabilities on low-resource languages, 2) lacking reasoning ability as shown from the results in QA, and 3) performing task-oriented and knowledge-grounded dialogue tasks. More detailed experimental setup and analysis for each task are shared in the next subsections, i.e., : Experiment details and result and : ChatGPT on Dialogue System. We also provide the complete list of all the datasets used in our evaluation in Appendix~. ChatGPT on Summarization, MT, Sentiment Analysis, QA, and Misinformation Detection Summarization We test on 100 samples from two common summarization datasets: half from SAMSum~, a dialogue summarization dataset, and another half from CNN/DM~, news summarization datasets. The large version of Bart~ model fine-tuned on both datasets is conducted for comparison. Moreover, OpenAI's text-davinci-002 is used as the previous SOTA zero-shot model. We calculate ROUGE-1 scores for evaluating the generated summary. As is shown in Table~, ChatGPT achieves a similar zero-shot performance with text-davinci-002, which is expected since they evolved from the same GPT3 pre-trained checkpoint. However, the fine-tuned Bart still outperforms zero-shot ChatGPT by a large margin. Furthermore, we evaluate the ChatGPT's unique interaction capabilities in . Machine Translation We evaluate the machine translation ability of ChatGPT on both high-resource and low-resource languages using the ChrF++ metric~. Specifically, we incorporate 8 high-resource languages, i.e., French (fra), Spanish (spa), Chinese (zho), Arabic (ara), Japanese (jpn), Indonesian (ind), Korean (kor), and Vietnamese (vie), and 4 low-resource languages, i.e., Javanese (jav), Sundanese (sun), Marathi (mar), and Buginese (bug) for our evaluation.~ For each language pair, we sample 30 EngXXX parallel sentences from the FLORES-200 dataset~. The result of our experiment suggests that ChatGPT can well perform XXXEng translation, but it still lacks the ability to perform EngXXX translation. Sentiment Analysis Sentiment analysis has been widely explored for both high-resource and low-resource languages~. We explore the sentiment analysis ability of ChatGPT through 4 languages with diverse amounts of resources in NusaX~: English (eng), Indonesian (ind), Javanese (jav), and Buginese (bug). For each language, we sample 50 sentences from the corresponding dataset for our experiment and measure the macro F1 score as the evaluation metric. We compare the results with two baselines, i.e., supervised state-of-the-art performance from and zero-shot multilingual LLM from . ChatGPT outperforms the previous state-of-the-art zero-shot model by a large margin except for the Buginese, where it performs on par. This shows that ChatGPT still has a limited understanding of extremely low-resource languages. Question Answering Since Question Answering (QA) is a broad topic, we classify QA datasets into different categories based on the knowledge/reasoning type required to do the task, e.g commonsense reasoning, spatial reasoning, temporal reasoning, etc., to have a clearer analysis on ChatGPT's abilities. For each category, we select several datasets, and for each dataset, we sample 30 instances and test ChatGPT on the subset. Details on the dataset will be described in which subsection of . Furthermore, we inspect the rationales provided by ChatGPT that it used to come up with the answers. Some of them will be discussed in detail in the corresponding section (). Based on our experiment results, ChatGPT outperforms the existing zero-shot and some of the fine-tuned state-of-the-art performance on question answering. Furthermore, ChatGPT achieves near-perfect scores on three tasks, i.e., bAbI task 15, EntailmentBank, and Pep-3k. Misinformation Detection We test ChatGPT's ability to detect misinformation with the test sets that consist of scientific and social claims related to COVID-19~ with 100 samples. We take half from scientific (covid-scientific) and another half from social (covid-social) sets. We evaluate the accuracy of the veracity by manually checking the generated text. ChatGPT could detect misinformation 92ChatGPT on Dialogue Tasks Given that ChatGPT has the ability to generate conversation-like responses, it is interesting to test their ability in response generation in different dialogue settings: 1) Knowledge-Grounded Open-Domain Dialogue and 2) Task-Oriented Dialogue. Knowledge-Grounded Open-Domain Dialogue Open-domain dialogue systems interact with humans with generated responses automatically and aim to provide users with an engaging experience. To boost informativeness, these systems leverage external knowledge, including structured knowledge such as knowledge graphs~ and unstructured knowledge such as free text~. To quantitatively measure ChatGPT's performance on knowledge-grounded dialogue, we apply it to 50 samples randomly selected from the test set of OpenDialKG~, which contains open-ended dialogues grounded on a knowledge path. We use the following instruction for this KGD task: ``Can we try dialogue generation? I will give you turns, and you can generate the next turn, but only one. n n You can also consider the knowledge of XXX for your reference in the dialogue.'' According to human judgment, the responses from ChatGPT are of high quality with fluent response generation as well as incorporating the provided knowledge in the response. However, the automatic evaluation results in Table~ are relatively low compared with GPT2~, which is fine-tuned on this dataset. Specifically, ChatGPT obtains a 4.05 BLEU and an 18.62 ROUGE-L score as the generated responses tend to be longer than the golden answers. For FeQA, which measures the generated response's faithfulness to the input source, ChatGPT gets 15.03 since some generated responses include content from its parametrized knowledge injected during pre-training. Task-Oriented Dialogue In task-oriented dialogue (TOD), a model needs to fulfill a specific objective by interacting in natural language with the user. This task is often split into three modules: natural language understanding with belief state tracking, decision-making through dialogue policies, and response generation -- a modular approach that handles each of these steps with different models. Besides, unified approaches are starting to show increasingly strong performances . Although ChatGPT seems more appropriate for open-domain dialogue tasks, we investigate and discuss how ChatGPT's emergent abilities and interactivity could potentially be leveraged for TOD as well. We explore two setups A) modular approach: testing both dialogue state tracking and response generation using oracle actions; B) unified approach: a direct approach to simulate the TOD interaction while leveraging information in a structured database. We provide an example of the modular and unified approaches in Appendix~. Setup A: Modular Approach We investigate ChatGPT’s ability for both dialogue state tracking and response generation in 50 dialogue turn samples taken from MultiWOZ2.2 . In detail, we ask the model to provide the belief state as domain-intent: [slot1, value1], … in the prompt following previous zero-shot and few-shot approaches, and provide an exhaustive list of domain-intent-slot-value for the given dialogue. For the response generation, we provide only the oracle dialogue actions (e.g. 'Hotel-Inform':['area', 'centre']), and ask ChatGPT to generate a TOD response given the dialogue history. We assess DST with joint goal accuracy (JGA), the ratio of dialogue turns where the predicted dialogue state is exactly the ground truth, and response generation with BLEU and inform rate(As shown in table , the performance for DST is mediocre with a JGA of 24.4Setup B: Unified Approach We explore ChatGPT’s ability to simulate a TOD interaction in an end-to-end manner by providing nothing more than a structured database and giving the instruction ``Use the following knowledge base to complete the task of recommending a restaurant as a task-oriented dialogue system''. In this setup, we could investigate whether ChatGPT is able to complete basic retrieval queries and respond to users' requests such as ``Give me some restaurants that serve Italian food\" or \"I would prefer cheap options please''. However, there are several limitations that we could investigate as follow. Long-term Multi-turn Dependency: ChatGPT cannot keep the belief state across multiple turns within the interaction. For instance, asking for Italian food will overwrite the previous turn’s belief state by asking for restaurants with a rating of 3 or higher. However, if the user explicitly asks to recall the earlier preferences, ChatGPT is able to correct the retrieved information and incorporate the previous belief state. This is interesting as it shows that the information previously given in multi-turn is still usable, but needs to be called explicitly. Basic Reasoning Failure: ChatGPT's response tends to be wrong if the query introduces a basic level of reasoning such as when it is asked for ``recommendation for restaurants with European food'' (ChatGPT has to filter the types of cuisine which are based on countries) or ``recommendation for restaurants with a rating of 3 or higher'' (ChatGPT needs to understand rating 3, 4 and 5). Even with a basic knowledge base, ChatGPT fails to answer correctly 66 Extrinsic Hallucination: ChatGPT tends to generate hallucinated information beyond the given knowledge. This is especially harmful in TOD as ChatGPT will sometimes hallucinate some prices for hotel booking, or availability for restaurants. Evaluating Multilinguality of ChatGPT Training data size affects language understanding and generation quality of LMs~. As an LLM, the same premise also applies to ChatGPT, and the question is to what extent. We investigate this question through a series of experiments by analyzing 1) the language understanding capability using two different tasks, i.e, language identification (LID) and sentiment analysis, and 2) the language generation capability through machine translation using English as the pivot language. Based on the percentage of data in the CommonCrawl) is the primary source of language pre-training data used in GPT3, we group languages into 3 categories, i.e., high-resource (>1Language Understanding We propose a framework for investigating the language understanding ability of ChatGPT through 3 languages from different language categories in NusaX~, i.e. English (eng), Indonesian (ind), Javanese (jav). In addition, we incorporate an extremely low-resource language from NusaX, i.e., Buginese (bug), which is not even listed on CommonCrawl since the LID used in CommonCrawl, i.e., CLD2~, does not support Buginese (bug). We sample 50 sentences per language from the corresponding dataset for our experiment. ChatGPT fails to generalize to extremely low-resource languages As shown in~tab:nlu_result, ChatGPT achieves 84ChatGPT understands sentences in low-resource languages but lacks the ability to identify the language ChatGPT correctly classified the languages for English and Indonesian 100ChatGPT displays better human-preferred responses As shown in tab:chat-gpt-nlu, ChatGPT lets the user know that its prediction is uncertain when it does not completely understand the language and also provides broader information regarding the language, such as location and tribe of which the predicted language is spoken. This fact provides evidence regarding the benefit of using the RLHF approach compared to other training approaches for aligning LLMs with human preferences. Language Generation We assess the multilingual language generation ability of ChatGPT through machine translation. ChatGPT has been shown to be competitive compared to commercial translation products for high-resource languages~. Specifically, we choose 2 languages from each language category, i.e., French (fra), Chinese (zho), Indonesian (ind), Korean (kor), Javanese (jav), and Sundanese (sun) from the FLORES-200 dataset~. For each language, we sample 30 English-XXX parallel sentences and perform two directions of translation using English as the pivot language. The correctness of the translation results is manually validated by a native speaker of the corresponding language. ChatGPT performs worse on low-resource languages As shown in tab:chat-gpt-nlg, similar to other LLMs~, ChatGPT produces better English translation quality from high-resource languages, such as French and Chinese. While for low-resource languages, such as Javanese and Sundanese, ChatGPT tends to generate several mistranslated words/phrases and sometimes even hallucinate some objects. Moreover, we also observe that sometimes ChatGPT translates the English sentence into a different but related language other than the requested target language (see ). This fact suggests that the generalization of LLMs, including ChatGPT, to low-resource languages, remains an open challenge. ChatGPT understands non-Latin scripts better than it can generate them Despite being high-resource and medium-resource languages, the translation from English to Chinese and Korean is much inferior to the other languages with Latin scripts, i.e., French or Indonesian. Similarly, prior works focusing on transliteration~ have shown the effectiveness of utilizing Latin scripts over other scripts, e.g., Cyrillic, Georgian, Arabic, etc, especially for low-resource languages. Interestingly, this problem of using non-Latin scripts is less severe for translation from Chinese and Korean to English, which suggests that ChatGPT can better neutralize the effect of non-Latin scripts as source languages~, but it still lacks the ability to generate non-Latin script languages. Evaluating Multimodality of ChatGPT Since ChatGPT is a purely text-prompted language model, it is unlikely to explore its multimodal capabilities with visual inputs like contemporary vision-language works . Hence, various ways to interact with ChatGPT and generate output data with multiple modalities have been explored in the research community. For example, as shown in Figure~, ChatGPT can generate a well-formed and suitable intermediate representation in code format in order to synthesize images given the dialogue context and user prompts. Thanks to the code understanding and generation ability of ChatGPT, we believe programming codes can serve as the intermediate medium to bridge vision and language . Given textual prompts, ChatGPT can generate code representations of visual images using the SVG (Scalable Vector Graphics) format or APIs such as the HTML Canvas element and the Python Turtle graphics. In this way, even though the generated images are symbolic and their quality is not comparable to the ones generated by modern text-to-image models~, it is worth exploring due to three reasons. Firstly, it helps us investigate the visual understanding and reasoning abilities of ChatGPT, which can be seen as an emergent skill after the very large-scale pre-training on text and code data. Furthermore, representing images with code is a more explainable way to understand the model's behaviors and rationales in text-to-image generation. Third, it is a natural way to evaluate ChatGPT's ability on multi-turn interaction by asking for post-editing and corrections of the generated images. Flag Drawing Task To systematically evaluate the image generation ability of ChatGPT through code generation, we design a national flag drawing task. This is a unique task showing how ChatGPT's textually described knowledge (language) converts into the drawing (vision) through the SVG (code), using multi-turn interactions in the dialogue. Task Formulation The flag-drawing task contains three steps. Firstly, we ask ChatGPT to illustrate the appearance of the flag using the prompt ``Describe how the <NATION> flag looks like''. Next, based on the description, we ask ChatGPT to generate the SVG code of that flag by prompting ``Generate a code snippet to represent that flag in SVG format''. Finally, if the generated image contains errors, we iteratively ask ChatGPT to fix them. There are four types of errors, including 1) layout, 2) color, 3) missing components, and 4) shape/size. In each round of fixing, we ask ChatGPT to revise only one type of error with the prompt ``<ERROR DESCRIPTION>. Revise the image''. We terminate the conversation once the generated flag becomes perfect or we have already passed two rounds of fixing. We uniformly collect 50 national flags from different continents and conduct the flag-drawing task on ChatGPT. The full results are shown in Appendix~. The generated flag images are evaluated by the aforementioned four error types as criteria. We further assess the image quality with five grades, A E, which indicate zero to four (or above) errors with an increment of one. We assign grades to each round so that we can assess the number of improvements and degradation through conversational interactions (post-editing). An overview of the result evaluation is provided in Table~. Findings ChatGPT is capable of drawing, yet better with a self-generated textual description. As demonstrated in Table~ and Appendix~, by following the task formulation, ChatGPT can generate plausible national flags using the SVG format. To better understand the behavior of ChatGPT, we perform an ablation study by removing the description generation step. As illustrated by Figure~, the performance drops dramatically without first prompting the textual flag description, which is generated by ChatGPT itself. Quantitatively, the proportion of E-graded images increases from 32ChatGPT is an elementary illustrator. Among the four error types, the majority lies in the shape/size error, which happens 68For the other three error types (layout, color, missing components), they appear 34First, there might not be sufficient training data in such a pattern. To draw sophisticated shapes, the <path> tag in SVG is generally used, but it might not be commonly seen in the pre-training code data, thus leading to ChatGPT being incapable of creating complex shapes. Second, in the textual flag description generated at the initial step, the illustration of a sophisticated shape is written in a conceptual and high-level manner. There are no detailed instructions or rules for the model to precisely draw the shape. For example, in the description of the Canadian flag, it only says ``a red maple leaf in the center'', making it nearly impossible to draw the leaf correctly without seeing it before. This is also a natural defect of text-only language models as they never see actual visual data and textual data is usually conceptual. Reasoning Evaluations of ChatGPT Reasoning is one of the most actively discussed and debated abilities of LLMs as scaling the model parameter size also increases the implicit knowledge in LLMs~. mahowald2023dissociating eloquently argues that \"language ability does not equal to thinking\" or \"reasoning\" in LLMs, and that LLMs have poor reasoning skills despite possessing human-level language skills. In the NLP literature, evaluating a model's reasoning often means evaluating its various skills in arithmetic, commonsense, and symbolic reasoning in different NLP tasks that require such skills~. This is in line with the anecdotal experience of users with ChatGPT -- some of the examples demonstrate surprisingly good ``reasoning'' abilities compared to previously introduced LLMs but at the same time ChatGPT fails in very simple reasoning problems . In this paper, we investigate the reasoning ability of ChatGPT in a more fine-grained manner, which includes deductive, inductive, abductive, analogical, causal, multi-hop, temporal, and spatial reasoning, via question answering tasks. We first categorize available QA tasks into each category by avoiding overlap (i.e., choosing a test set that requires mainly one specific category of reasoning) as shown in Table . We share experimental results on each of the categories in the following subsections : logical reasoning (inductive, deductive, and abductive), : non-textual semantic reasoning (temporal, mathematical and spatial), commonsense reasoning, and : causal, multi-hop and analogical reasoning. On all reasoning tasks, we manually check the accuracy of the answer as well as verify the rationales and explanations generated by ChatGPT. The composed result for all reasoning tasks is shown in Appendix~. We further discuss each reasoning task in the following sections. Logical Reasoning Inductive, deductive, and abductive reasoning are common forms of logical reasoning, a process of deriving a conclusion or judgment based on given evidence or past experience and observations . Inductive and deductive are categorized by ``a degree to which the premise supports the conclusion'' based on logic and philosophy . Inductive reasoning is based on ``observations or evidence'' while deductive is based on ``truth of the premises'' (i.e., necessarily true inference) . Another way to categorize is based on the ``direction of reasoning'' -- deductive is from premise to conclusion while abductive is from conclusion to the most probable premise that supports the conclusion . Deductive vs. Inductive Reasoning Deductive reasoning involves processes of driving specific conclusions based on more general premises. On the contrary, inductive reasoning involves specific observation of patterns, processing them on increasingly abstract cycles of hypothetico-deductive reasoning to draw a more general conclusion . Comparing the two types of reasoning, deduction requires less ``guessing'' from the perspective of ChatGPT, as induction requires figuring out rules~. The former can be viewed as top-down while the latter is bottom-up. We explore ChatGPT's ability of inductive and deductive reasoning in two different levels: 1) basic and 2) advanced. Basic-level tasks are the prerequisites to probe reasoning. While solving these tasks does not necessarily indicate full reasoning capability, if ChatGPT fails on any of these tasks, then there are likely real-world tasks that it will fail on too if they require similar reasoning mechanisms. Consequently, the advanced-level tasks are there to probe those capabilities in real-world tasks where the noises are present, and solving them requires a more systematic generalization. Additionally, we choose tasks that do not require or are dependent on external knowledge and the solution could be only derived by premises to focus on dissecting the capability of each reasoning mechanism. ChatGPT is a lazy reasoner that suffers more with induction We first investigate basic reasoning skills with bAbI tasks , 30 examples each from task 15 (inductive) and task 16 (deductive). Each test example includes a list of premises to derive inference for a certain question. Interestingly, when ChatGPT was asked to answer a question given premises without any prompt engineering, it performs poorly in inductive reasoning (0 out of 30) while it achieves much better performance in deductive (19 of 30). ChatGPT answers ``It is not specified what <attribute> <entity> is.'' for most of the time when it was asked a question requiring inductive reasoning. However, when ChatGPT is explicitly asked for reasonable inference with a prompt ``Based on the given facts, do a reasonable inference on this question using inductive reasoning:'', its ability for inductive reasoning increases to 20 out of 30. Yet, it is still not as good as in deduction as the same prompt engineering also helps increases its ability for deductive reasoning to 28 out of 30. When we repeat the analysis on the advanced-level tasks, specifically on CLUTRR~ for induction and EntailmentBank for deduction~, the same conclusion holds based on our experiment. We could derive similar insight as ChatGPT only correctly answered for half of the time while it could make inferences deductively well for 90Abductive Reasoning Abductive reasoning is the inference to the most plausible explanation given observations. For instance, ``if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess''~. We test ChatGPT's language-based abductive reasoning ability with 30 samples from NLI dataset , which requires the model to select the most plausible explanation given the conclusion. Based on our test, it could achieve 86.7Non-textual semantic reasoning It is often investigated in public sharing about ChatGPT errors/ failure instances that it lacks the reasoning ability that required non-text semantic understanding such as mathematical, temporal and spatial reasoning. In this section, we investigate the non-text semantic reasoning capabilities of ChatGPT. Mathematical reasoning Mathematical capabilities or numerical reasoning has been frequently mentioned to be lacking for LLMs, not only ChatGPT . mathchatgpt test ChatGPT's capability with publicly available datasets as well as the human-curated dataset, which consists of 728 prompts. The shared findings for ChatGPT's mathematical capabilities include 1) ChatGPT often understands the question but fails to provide correct solutions; 2) it shows inconsistent poor performance on graduate-level advanced mathematics; 3) it has a great ability to search for mathematical objects. We also test separately on MATH dataset. Not surprisingly, it could only score 23.33Temporal reasoning Temporal reasoning is mentioned a few times in the literature but is less common than others. It tests the understanding of the time duration of and the relation between events. For this category, we conduct experiments on the dataset TimeDial , which solely requires temporal reasoning. We follow the format of the task in the BIG-bench benchmark ~, which is multiple-choice (single correct answer), Overall, ChatGPT correctly answers 86.67Spatial Reasoning Spatial reasoning is using an understanding of spatial relations among different objects and spaces. For spatial reasoning, we utilize two existing datasets: SpartQA and StepGame , which compose of story-question pairs about k relations of k+1 (where k is up to 10) entities written in natural language. ChatGPT is asked to answer spatial relations between two entities based on the provided descriptions of different entities. ChatGPT falls short of the spatial reasoning tasks, as shown in Table , with overall success rates of 43.33To understand spatial reasoning ability at a more elementary level, we test with less complicated examples from StepGame which we refer to as StepGame (Basic). It does not involve multi-hop reasoning but purely spatial relation between two entities. (e.g, ``C is sitting at the top position to Y. What is the relation of the agent Y to the agent C?''). We test for basic spatial relations with 8 labels from StepGame left, right, above, below, lower-left, lower-right, upper-left, upper-right. When we test on StepGame (Basic), ChatGPT scores higher (63.33We investigate the errors that it often fails to understand clock direction (e.g., ``W is at K's 3 o'clock'') and diagonal spatial relations. We further analyze the results by breaking down the test examples of StepGame (Basic) into two comparisons: i) types of directions (basic cardinal vs. diagonal) and ii) ways of spatial description for cardinal directions (basic cardinal vs. clock-position cardinal). We take 20 more samples for each category (basic cardinal, diagonal, clock-position cardinal) and tested them as illustrated in Table . ChatGPT poorly infers with clock-position description. Although it is a simple cardinal direction, ChatGPT could only correctly answer for 5 samples (25 ChatGPT is worse at the diagonal position. It correctly answers around half of the time (55Commonsense Reasoning Commonsense reasoning is the understanding and reasoning about everyday concepts and knowledge that most people are familiar with, to make judgments and predictions about new situations~. Recent work has shown that LLMs perform impressively well on commonsense reasoning benchmarks~. However, ~bhargava2022commonsense also point out that the reasoning tasks underlying these benchmarks are still far from being solved, since most existing studies primarily report the performance of the models, without a detailed examination of the quality of the rationales produced. To evaluate ChatGPT's capability on commonsense reasoning, we first test it on two widely used benchmark datasets CommonsenseQA~ and PiQA~. CommonsenseQA focuses on general commonsense question answering such as \"Where is a business restaurant likely to be located?\", and PiQA is about physical commonsense reasoning: given a sentence such as \"When boiling butter, when it’s ready, you can \", the goal is to fill in the blank with one of two answer options, \"Pour it onto a plate\" and \"Pour it onto a jar\". We use the validation split for both of the datasets since there are no labels provided on the test set that we retrieve. We also further probe ChatGPT by evaluating a more challenging commonsense reasoning dataset in a more comprehensive way. We use Pep-3k~, which requires the model to recognize plausible but possibly novel events, such as \"man swallow paintball\". Each instance in the Pep-3k is an s-v-o predicate, and the task is to judge if the predicate is plausible or not. But instead of evaluating ChatGPT's performance only based on the binary judgment, we also check if the answer contains relevant rationales (explanations) that lead to its judgment. ChatGPT shows surprisingly good commonsense reasoning capability in our evaluation tasks, perhaps due to its large parametric memory. We sample 30 instances from each of the test sets. For the Pep-3k samples, we prepend the s-v-o predicate with \"Please judge if this predicate is (likely) plausible or implausible:\" to prompt ChatGPT. We show the results in Table~. As we see, ChatGPT performs quite well on the three datasets in terms of answer accuracy, which matches our anticipation. Furthermore, as we also check the rationales in ChatGPT's answer when evaluating Pep-3k samples, we can see that ChatGPT does quite well not only in terms of answer accuracy but also in generating reasonable reasoning procedures to support its answer. We show a concrete example in Table~. As we can see, ChatGPT's answer explains well what kinds of materials are usually cut through with knives (i.e., food, paper, or wood). Then, it reasons why rocks cannot be chopped with a knife by explaining `rocks are much harder than these materials.' While our findings are based on 30 samples from each dataset, we see the potential in ChatGPT's commonsense reasoning capability, and further large-scale investigation is worth exploring. Causal, Multi-Hop, and Analogical Reasoning Causal Reasoning Causal reasoning is the process of identifying the relationship between causes/actions and effects/changes (i.e., causality) . We test ChatGPT on 30 samples of human-annotated explainable CAusal REasoning dataset (E-CARE) and it could score 24 samples correctly (80Multi-hop Reasoning To be able to reason over a larger context, a system has to perform multi-hop reasoning over more than one piece of information to arrive at the answer . We test ChatGPT's multi-hop reasoning capability on 30 samples of HotpotQA dataset and we find that ChatGPT has difficulty performing with such capability, only answering 8 samples correctly, although the questions posed are only 2-hops. It is worth noting that ChatGPT oftentimes generates the answer in a short passage of explanations, thus we evaluate manually each of the ChatGPT responses to check its accuracy. This aligns with the findings that LLMs are also limited in several ways, and fail to produce accurate predictions due to their inability to accomplish complex reasoning, such as solving tasks that require multi-hop reasoning . Analogical Reasoning Analogical reasoning is a way of thinking that relies upon an analogy, comparing two or more objects or systems of objects to drive a conclusion. We test with 30 samples from~ and evaluate based on human evaluation, to see if the generated answer match with/contain the gold answer. ChatGPT could correctly answer all 30 examples, which may reveal that ChatGPT has a good capability in analogical reasoning skills. Factuality and Hallucination Evaluations in ChatGPT LLMs are known to be susceptible to generating nonfactual, untruthful information, which is referred to as hallucination . Many anecdotal witnesses show ChatGPT also seems to suffer from the same problem as other LLMs. To evaluate this aspect of ChatGPT, we first explore existing fact-checking test sets and QA tasks that required knowledge (). We illustrate the challenge of hallucination in ChatGPT by sharing hallucination examples from different NLP tasks (). Factuality in ChatGPT We first test ChatGPT's ability to detect misinformation with the test sets that consist of scientific and social claims related to COVID-19~. We take 50 samples each for scientific (covid-scientific) and social (covid-social) sets. ChatGPT is able to detect misinformation 92Interestingly, for more societal-related claims, 40However, it cannot avoid the criticism that parameterized knowledge is obtained by better memorization as it still shows worse performance in questions designed to cause imitative falsehoods. For instance, ``Does someone share more genes with their siblings or parents?'' and ``What are the health benefits of pink Himalayan salt?'' We test on 66 test samples from TruthfulQA , which tests the extent of LLMs to mimic humans falsehood, and 35.38Hallucination in ChatGPT There exist two categories of hallucination~. Intrinsic hallucinations that refers to the LLM generation that contradicts the source/input content. Extrinsic hallucinations that refers to the LLM generations that cannot be verified from the source/input content (i.e., output that can neither be supported nor contradicted by the source). In Table , we share examples of these hallucination types detected from different task explorations. With the setting of tasks we test, we often find extrinsic hallucinations, including both untruthful and factual ones, across various tasks such as Machine Translation, Question answering. The intrinsic hallucinations are barely found as discussed in tasks about summarization and knowledge-grounded open-domain dialogue. For instance, in the abstractive summarization task, in which neural models usually suffer from intrinsic hallucination, ChatGPT’s generated summarisation did not include any intrinsic hallucination examples based on our experiments. It rather shows a factual extrinsic hallucination, for instance, ChatGPT could correctly paraphrase ``Britain and five other countries’’ from source input into ``P5+1 (US, UK, France, China, Russia, and Germany),’’ which is assessed to be factual. We could also observe an interesting intrinsic hallucination for our proposed multi-modal task, the flag drawing task. ChatGPT is first asked to generate a description of how the flags look before it is asked to generate code for the flag. Although it generates the correct description as ``The flag of Mexico consists of three vertical bands [...]'', the final drawing (SVG code) consists of horizontal bands. However, extrinsic hallucinations often happen, including both untruthful and factual ones. In the question-answering task, we often find extrinsic hallucination to be non-factual which harms the final performance. For instance, in the question of asking for the relationship among entities, although step kindship is never mentioned in the question, ChatGPT answers the question with step kinship, as illustrated in Table . We could also observe that ChatGPT's weakness with extrinsic hallucination also degrades machine translation. When it is asked to translate the text ``Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes.'' into Korean, it contains a piece of information that was not found in the source, UTF8mj``저주파 치료'' (transcutaneous electrical nerve stimulation) in the translated text. Evaluating Interactivity in ChatGPT ChatGPT has a built-in interactive ability thanks to conversational data fine-tuning and RLHF. We further delve into the benefit of exploiting this interactive ability of ChatGPT in three NLP tasks, i.e., 1) summarization, 2) machine translation, and 3) multimodal generation. Our experiments demonstrate the potential of employing multi-turn interaction to refine the quality of the generated responses and improve the task performance of ChatGPT. Interactivity on Summarization Summarization models aim to extract essential information from documents and to generate short, concise, and readable text~. Recently, show that zero-shot prompting with GPT-3~ performs better than the state-of-the-art fine-tuning model~ on human evaluation. One main advantage of ChatGPT over GPT3 is that it interacts in a conversational way. Therefore, we study the interactivity of ChatGPT, especially in real-world applications, people may want to improve the summary based on the previously generated summary. In detail, we investigate ChatGPT's ability to control the length of summaries through multi-turn interaction. To run experiments, we randomly sample 50 documents from a dialogue summarization dataset called SAMSum~ and conduct a two-turn iterative prompt approach. Given an input dialogue as the context, we first input the prompt ``Summarize the above dialogue'' to the ChatGPT. However, ChatGPT usually generates an overly long summary, sometimes even longer than the input conversation itself. To refine the summary, we simply input another prompt -- ``Please make the summary shorter'' after the first response. According to the second prompt, GhatGPT could provide a much shorter summary than the first response. In order to quantify the experimental results, we calculate the ROUGE-1 scores among the first summary and the second summary. Experimental results show that with the second length control prompt, the refined summaries achieve 7.99, 1.64, and 5,19 gains on ROUGE-1, ROUGE-2, and ROUGE-L, respectively. Figure~ shows an example of how multi-turn interaction helps to control the length of the summary. Interactivity on Machine Translation One of the capabilities of ChatGPT is to perform text translation from one language to another. With the interactivity of ChatGPT, we explore the possibility of performing a combined machine translation and automatic post-editing tasks to improve the translation quality of ChatGPT. We explore this capability on translation from English to the target language since the translation quality from high-resource and medium-resource languages to English of ChatGPT is near perfect (see ). For the experiment, we adapt the dataset used in which samples 30 parallel sentences from 6 language pairs in NusaX~, i.e., Chinese (zho), French (fra), Indonesian (ind), Korean (kor), Javanese (jav), and Sundanese (sun). We experiment with a multi-turn approach, where we first query ChatGPT to translate to the target language using ``What is [TARGETLANGUAGE] translation of the following sentence? n n[INPUTSENTENCE]'' as the prompt template, and then query for the post-editing using the following prompt template: ``Could you perform a post-editing to ensure the meaning is equivalent to \"[INPUTSENTENCE]\"?''. The post-editing results are manually validated by a native speaker in the corresponding language to validate: 1) whether the post-edited sentence is better than the translation one, and 2) whether the post-edited sentence is the correct translation of the given English sentence. As shown in~fig:post-edit, despite the translation and post-editing being done using a single ChatGPT model, the multi-turn approach method helps to improve the correctness of the translation by making partial corrections or even full corrections in some cases. This result reflects that performing automatic post-editing through interactive LLMs, such as ChatGPT, yields consistently better translation results compared to a single-turn machine translation, which is especially useful for translation in low-resource languages. We provide per-language examples of the machine-translated and post-edited sentences in~app:ape-examples. To further strengthen our hypothesis, we conduct an additional experiment on the automatic post-editing (APE) shared task dataset on WMT 2022~, which focuses on EnglishMarathi post-editing task. Marathi (mar) is also a low-resource language with 0.02As shown on~tab:ape-mar, the single-turn translation without post-editing produces a slightly better evaluation score on the Marathi language, but the multi-turn with post-editing consistently yields better evaluation performance on the back-translated English text on all metrics. This suggests that post-editing enables the translation results to be closer to the actual meaning of the source text. Nevertheless, the translation to the Marathi language is much worse compared to the baseline MT provided from the APE 2022 shared task~ which further supports the limitations of ChatGPT on generating sentences in low-resource and non-Latin script languages. Interactivity on Multimodal Generation The multi-turn interaction ability of ChatGPT enables the refinement of text-to-image generation. It is one of the most natural ways for humans to create artwork or product designs by requesting an AI tool iteratively. For example, Figure~ shows the process of creating an interesting painting by prompting ChatGPT with varied requirements through multiple turns. To quantitatively study how this ability impacts text-to-image generation, as mentioned in the task formulation of the flag drawing, we conduct at most three rounds of post-editing. As shown in Figure~, in the first round of generation, ChatGPT rarely generates errorless SVG images except for some relatively simple flags (e.g., Nigerian and German). In subsequent rounds of the generation, we see a clear boost in the overall quality of the generated flag images by asking ChatGPT to fix errors based on its own description. We observe that 34Meanwhile, there are also 6Conclusion and Discussion Multitask, Multilingual, Multimodal ChatGPT outperforms multiple state-of-the-art zero-shot LLMs on various tasks and even surpasses fine-tuned models on some tasks. Although ChatGPT performs well in most of the tasks, there are still some failure cases on each task (). In the summarization task, ChatGPT sometimes generates a summary that is even longer than the input document. In the machine translation task, ChatGPT sometimes produces an incorrect translation for some words, making the meaning slightly shifted. Therefore, dealing with these special cases is a complex but important task. In terms of multilinguality, ChatGPT achieves strong performance in many high-resource and medium-resource languages. Nevertheless, ChatGPT still lacks the ability to understand and generate sentences in low-resource languages (). The performance disparity in low-resource languages limits the diversity and inclusivity of NLP~. Additionally, ChatGPT also lacks the ability to translate sentences in non-Latin script languages (), despite the languages being high-resource. This raises the question of language representation in ChatGPT. Research on shared representation for non-Latin scripts~ is needed. In terms of multimodality, it is very natural to have visual information (images or videos) in the form of dialogue~ in real applications, which may be provided by the user or generated by the model. The visual information also serves as part of the context for subsequent turns. Can textual models like ChatGPT switch to a multimodal backbone? Through our flag drawing experiments, we find that ChatGPT is able to translate visual concepts and structures to basic code formats (e.g., circle SVG element), which define the exact shape, orientation, color, and placement of the objects. Given this structured way of generating an image, one of the research questions is: if a model learns an image as a composition of basic shapes, would it help a model understand the abstraction of visual concepts and structures~? Moreover, would it produce more interpretable results for the users? Reasoning The highly impressive performance of ChatGPT has sparked interest in expanding its usage beyond traditional NLP tasks into more complex domains requiring sophisticated reasoning such as problem-solving, decision-making, and planning. Our evaluation of its reasoning abilities shows that they are not reliable. Specifically, our findings indicate that ChatGPT exhibits a tendency to be a lazy reasoner and that its capabilities are inconsistent across various reasoning abilities. In terms of logical reasoning, ChatGPT performs better deductive and abductive reasoning compared to inductive reasoning. However, as a language model, ChatGPT still lacks the ability to answer non-textual semantic reasoning tasks, such as mathematical, temporal, and spatial reasoning. Instead, many suggest pairing ChatGPT with another computational model, such as Wolfram, to solve each specific set of problems. In that combination, ChatGPT parses natural language input into programming language code snippets, then the computational model will execute the code to return results. In this way, the strength of ChatGPT is maximized while the weakness is mitigated. Meanwhile, ChatGPT surprisingly excels in commonsense, causal, and analogical reasoning. We suspect that all this knowledge has been encoded in the parametric memory of ChatGPT. Nevertheless, ChatGPT lacks the ability to perform multi-hop reasoning which suggests that, like other LLMs, ChatGPT possesses a limited ability to accomplish complex reasoning tasks. To support the further expansion of its use cases, it is necessary to prioritize the development of systems with robust complex reasoning capabilities, which should also be facilitated by the creation of more comprehensive benchmarks for assessing these abilities, particularly when multiple abilities are required to complete the tasks. Factuality and Hallucinations Although powerful, ChatGPT, like other LLMs, still makes things up . To ensure factuality, it is possible to build LLMs with an interface to an external knowledge source, like Blenderbot 3.0~, RETRO~, and LaMDa~. In this manner, factual information LLMs can be updated independently and easily in the knowledge base, without fine-tuning the whole LLM. However, how to balance the generative power of its parametric memory with external knowledge sources is an active research area~ Meanwhile, there are many forms of hallucinations from LLMs that are not necessarily counterfactual but still undesirable. The RLHF process of ChatGPT can ensure human feedback to mitigate undesirable responses. However, researchers need to work on coming up with more automatic and scalable methods to detect and mitigate hallucinations and other undesirable artifacts of LLMs. Interactivity Compared with the previous LLMs, the interactive ability of ChatGPT has made a leap according to both qualitative and quantitative measures. Based on our evaluation, through interactivity, we can improve the performance of ChatGPT by 8The conversational ability and multi-turn interaction of ChatGPT make it natural for people to use it as a dialog system. We carry out the very difficult task of using ChatGPT as a task-oriented dialog system with structured knowledge given in the prompt to perform. Whereas ChatGPT shows strong performance in various modules, challenges remain for us to use ChatGPT as a fully task-oriented dialog system, due to the lack of controllability and knowledge grounding in its response. The interactivity inadvertently enables the user to ``jail-break'' ChatGPT to carry out harmful actions. For example, a user could ask ChatGPT to turn off its safety layer, causing potential damage~. Responsible Generative AI Responsible design and usage of LLMs including ChatGPT is an important and pressing challenge today. There are common issues with these models, such as fairness, toxicity, demographic bias, and safety, that need to be addressed. In the case of ChatGPT, OpenAI constructs safety layers and uses RLHF and potentially other means to filter out undesirable system responses. This process is resource intensive and opaque to the public. We hope to see a more open discussion and sharing of responsible design of LLMs from various organizations including OpenAI in the future. Flag Drawing Task Results We provide the detailed results of the flag drawing task described in in Figure~. InstructGPT for Multimodality We show an example of a multi-turn flag drawing of InstructGPT in~fig:multimodal_instructgpt. Similar to ChatGPT, InstructGPT can revise the generated flag image in each turn, although the generation quality is still elementary. left=1cm,bottom=1.5cm,right=1cm,top=1cm List of Evaluation Datasets We provide a detailed list of all the datasets used in our experiment on~tab:datasets-complete. p0.1|p0.05|p0.4|p0.12|p0.12|p0.12 Dataset & Task & Description & Reference & Test Size & ChatGPT Eval National Flag Drawing & IG & National Flag Drawing is a designed synthetic dataset which is used to evaluate the multimodal understanding of LLMs. The instruction for the National Flag Drawing is as follow: given a nation, draw the corresponding national flag and revise it based on the follow-up correction requests. & Curated by authors of this paper & 50 & 50 CNN/DM & SUM & The CNN/DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine-reading and comprehension and abstractive question answering. & & 11490 & 50 SAMSum & SUM & SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. & & 819 & 50 FLoRes-200 & MT & FLoRes is a benchmark dataset for machine translation between English and four low resource languages, Nepali, Sinhala, Khmer and Pashto, based on sentences translated from Wikipedia. & & 1012 per language (200 languages) & 30 per language (12 languages) NusaX & SA & NusaX is a high-quality multilingual parallel corpus that covers 12 languages, Indonesian, English, and 10 Indonesian local languages, namely Acehnese, Balinese, Banjarese, Buginese, Madurese, Minangkabau, Javanese, Ngaju, Sundanese, and Toba Batak. & & 400 & 50 bAbI task 15 & QA & This basic deduction bAbI tasks is taken from the (20) QA bAbI tasks that a set of proxy tasks that evaluate reading comprehension via question answering. The tasks measure understanding in several ways: whether a system is able to answer questions via simple deduction. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. & & 1000 & 30 bAbI task 16 & QA & This basic induction bAbI tasks is taken from the (20) QA bAbI tasks that a set of proxy tasks that evaluate reading comprehension via question answering. The tasks measure understanding in several ways: whether a system is able to answer questions via simple induction. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. & & 1000 & 30 EntailmentBank & QA & ENTAILMENTBANK, the first dataset of multistep entailment trees for QA, to support entailment-based explanation. ENTAILMENTBANK contains two parts: 1,840 entailment trees, each tree showing how a question-answer pair (QA) is entailed from a small number of relevant sentences (e.g., Figure 1); and a general corpus C, containing those and other sentences of domain-specific and general knowledge relevant to the QA domain. & & 340 & 30 CLUTRR & QA & CLUTRR (Compositional Language Understanding and Text-based Relational Reasoning), a diagnostic benchmark suite, is first introduced in (https://arxiv.org/abs/1908.06177) to test the systematic generalization and inductive reasoning capabilities of NLU systems. The CLUTRR benchmark allows us to test a model’s ability for systematic generalization by testing on stories that contain unseen combinations of logical rules, and test for the various forms of model robustness by adding different kinds of superfluous noise facts to the stories. & & 1146 & 30 NLI & QA & Abductive Natural Language Inference (NLI) is a new commonsense benchmark dataset designed to test an AI system’s capability to apply abductive reasoning and common sense to form possible explanations for a given set of observations. Formulated as a binary-classification task, the goal is to pick the most plausible explanatory hypothesis given two observations from narrative contexts. & & 3059 & 30 CommonsenseQA & QA & CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. The dataset is provided in two major training/validation/testing set splits: \"Random split\" which is the main evaluation split, and \"Question token split\", see paper for details. & & 1221 & 30 HotpotQA & QA & HotpotQA is a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. & & 7405 & 30 PiQA & QA & To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to state-of-the-art natural language understanding systems. The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Physical commonsense knowledge is a major challenge on the road to true AI-completeness, including robots that interact with the world and understand natural language. PIQA focuses on everyday situations with a preference for atypical solutions. The dataset is inspired by instructables.com, which provides users with instructions on how to build, craft, bake, or manipulate objects using everyday materials. & & 1838 & 30 E-Care & QA & Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide a deep understanding of the causal fact to facilitate the causal reasoning process. We present a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions. & & 2122 & 30 Letter string analogy & QA & The letter string analogy domain was introduced in order to evaluate computational models of analogical reasoning. This task is composed of simple alphanumeric characters, but nevertheless require a significant degree of abstraction to identify an analogy. & & - & 30 SpaRTQA & QA & SpartQA is a textual question answering benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state-of-the-art language models (LM). SPARTQA is built on NLVR’s images containing more objects with richer spatial structures. SPARTQA’s stories are more natural, have more sentences, and richer in spatial relations in each sentence, and the questions require deeper reasoning and have four types: find relation (FR), find blocks (FB), choose object (CO), and yes/no (YN), which allows for more fine-grained analysis of models’ capabilities. The default test set of this dataset is human-annotated test set, which consists of 510 instances. & & 510 & 64 StepGame & QA & StepGame is a robust multi-hop spatial reasoning dataset in textual format which addresses the limitation from the bAbI dataset task 17 and task 19. In this task, the AI system is required to interpret a story of k spatial relations (e.g left, top-right, 90 degree clockwise) of k+1 entities (k is up to 10) expressed in natural language and answer a question about the relative position of two of those entities. For each k from 1 to 10, there are 30000, 1000, and 30000 samples for train, valid, and test respectively. & & 30000 & 30 Pep-3k & QA & Pep-3k is a dataset of physical semantic plausibility judgments of single events. It requires a mixture of commonsense knowledge and conceptual knowledge to solve. Each event consists of a subject, a verb, and an object, i.e it has the simple s-v-o format. For example, the event can be man swallow paintball, with the label 0 (implausible). In total, Pep-3k has 3080 instances with plausible-implausible data balance. & & 3080 & 30 COVID-Social & MD & COVID-19-related claims fact-checked by journalists from a website called Politifact.com. Unlike the Covid19-Scientific dataset, it contains non-scientific and socially-related claims, such as ``For the coronavirus, the death rate in Texas, per capita of 29 million people, we're one of the lowest in the country.'' Such claims may not be life-and-death matters, but they still have the potential to bring negative sociopolitical effects. & & 340 & 50 COVID-Scientific & MD & COVID-Scientific is a testset consists of COVID-19-related scientific or medical myths that must be debunked correctly to ensure the safety of the public (e.g., \"Drinking a bleach solution will prevent you from getting COVID-19\"). & &172 & 50 TruthfulQA & QA & The dataset comprises questions that span 38 categories, including health, law, finance and politics. The questions are of those that some humans would answer falsely due to a false belief or misconception. (e.g., [conspiracy] ``If it's cold outside what does that tell us about global warming?''; [health] ``Can coughing effectively stop a heart attack?'') & &817 & 65 MultiWOZ 2.2 & TOD & MultiWOZ 2.2 is an improved version of MultiWOZ 2.1, which identifies and fixes dialogue state annotation errors across 17.3OpenDialKG & KGD & OpenDialKG is a dataset of conversations between two crowdsourcing agents engaging in a dialog about a given topic. Each dialog turn is paired with its corresponding ``KG paths'' that weave together the KG entities and relations that are mentioned in the dialog. & & 2.25k dialogue sessions & 50 dialogue turns List of all datasets used in our experiments. IG denotes image generation, SUM denotes summarization, MT denotes machine translation, SA denotes sentiment analysis, QA denotes question answering, MD denotes misinformation detection, TOD denotes task-oriented dialogue, and KGD denotes knowledge-grounded dialogue. Some of the descriptions are directly from the original reference. Examples from Machine Translation and Post-Editing p0.06|p0.21|p0.21|p0.21|p0.21 Target & English Text & Label & Translation & Post-Edited Text Chinese & Although three people were inside the house when the car impacted it, none of them were hurt. & UTF8gbsn虽然车撞到房子时，房子里面有三个人，但最后并没有人受伤。 & UTF8gbsn尽管有三个人在汽车撞上房子的时候在屋里，但他们都没有受伤。 & UTF8gbsn尽管汽车撞上房子时有三个人在屋里，但他们都没有受伤。 Chinese & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & UTF8gbsn34& UTF8gbsn这项民意调查显示，34& UTF8gbsn根据民意调查显示，34 Chinese & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & UTF8gbsn让亚文化变得与众不同的特质，可能是语言、美学、宗教、政治、性别、地理或多种因素的组合。 & UTF8gbsn什么决定一个亚文化作为不同的特质可能是语言的、审美的、宗教的、政治的、性的、地理的，或者这些因素的组合。 & UTF8gbsn决定一个亚文化与众不同的特质可能是语言的、审美的、宗教的、政治的、性的、地理的，或者是这些因素的组合。 Chinese & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & UTF8gbsn新王国时期的古埃及人惊叹于其前辈的已有一千多年历史的纪念碑。 & UTF8gbsn新王朝古埃及人惊叹于他们的前人的纪念碑，当时已经有一千多年的历史了。 & UTF8gbsn新王朝的古埃及人惊叹于他们前人留下的纪念碑，当时已经有超过一千年的历史。 Chinese & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & UTF8gbsn美国陆军工兵部队估计，6 英寸的降雨量就可能冲垮之前损坏的堤坝。 & UTF8gbsn美国工程兵部队估计，6英寸的降雨可能会破坏先前受损的堤坝。 & UTF8gbsn美国工程兵部队估计，6英寸的降雨可能会使先前受损的堤坝再次损坏。 Chinese & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & UTF8gbsn罗马各地安装了数个大型电视屏幕，供人们观看仪式。 & UTF8gbsn在罗马的若干个地方安装了几台大型电视屏幕，让人们观看仪式。 & UTF8gbsn可以，这是修改后的翻译：在罗马的各个地方安装了几台大型电视屏幕，以便让人们观看仪式。 Chinese & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & UTF8gbsn害虫会破坏食物，引起刺激性反应，在更严重的情况下会引起过敏反应，传播毒液或传染病。 & UTF8gbsn害虫可以毁坏食物，引起刺激，或者在更糟糕的情况下引起过敏反应，传播毒液，或传播感染。 & UTF8gbsn害虫可以毁坏食物，引起不适，甚至会引起过敏反应、传播毒液或传染感染。 Chinese & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & UTF8gbsn显而易见，由于人类的科学和技术进步，世界发生了很大的变化，而人口过剩和人类奢侈的生活方式使问题变得更为严重。 & UTF8gbsn很明显，由于人类的科学技术进步，世界已经发生了很大的变化，并且由于人口过剩和人类奢侈的生活方式，问题变得更加严重。 & UTF8gbsn毫无疑问，人类的科学和技术进步使世界发生了巨大的变化，同时人口过剩和人类奢侈的生活方式使问题变得更加严重。 Chinese & The correlation between brain pathology and behaviour supports scientists in their research. & UTF8gbsn大脑病理和行为之间的相关性可以为科学家们的研究提供支持。 & UTF8gbsn大脑病理学与行为之间的相关性支持科学家们的研究。 & UTF8gbsn研究表明，大脑病理学和行为之间的相关性为科学家提供了支持。 Chinese & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & UTF8gbsn和其他一些专家一样，他对糖尿病能否治愈持怀疑态度。他指出，这些发现与已患有 1 型糖尿病的人无关。 & UTF8gbsn像其他一些专家一样，他对于糖尿病是否可以被治愈持怀疑态度，并指出这些发现对已经患有 1 型糖尿病的人没有意义。 & UTF8gbsn和其他一些专家一样，他对糖尿病是否可以被治愈表示怀疑，并指出这些发现对已经患有 1 型糖尿病的人没有意义。 Korean & Although three people were inside the house when the car impacted it, none of them were hurt. & UTF8mj차가 집에 충돌했을 때 세 명이 안에 있었지만, 그들 중 한 명도 다치지 않았다. & UTF8mj차가 집에 부딪히자 집 안에 세 사람이 있었지만, 아무도 다치지 않았습니다. & UTF8mj차가 집에 부딪히자 집 안에 세 사람이 있었지만, 아무도 상해를 입지 않았습니다. Korean & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & UTF8mj여론 조사에서 34 퍼센트가 엘리자베스 2세가 호주의 마지막 군주이길 바란다는 의견을 보였습니다. & UTF8mj34UTF8mj이 조사에서는 34Korean & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & UTF8mj하위문화를 명확하게 구분하는 특징은 언어적, 미적, 종교적, 정치적, 성적, 지리적 요소가 있으며, 이러한 요소들의 결합일 수도 있다. & UTF8mj\"다음 문장의 한국어 번역은 무엇입니까? 부문문화를 구별되게 하는 특성은 언어, 예술, 종교, 정치, 성, 지리 요소일 수 있거나 이들 요소의 조합일 수도 있습니다.\" & UTF8mj부문문화가 구별되는 특성은 언어적, 예술적, 종교적, 정치적, 성적, 지리적 요소 중 하나일 수도 있고, 이들 요소의 조합일 수도 있습니다. Korean & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & UTF8mj고대 신왕국 이집트인들은 조상의 기념비적인 건축물을 보고 경탄했고 이것은 그 당시 기준으로 1000년은 족히 넘은 건축물이었습니다. & UTF8mj신나라 이집트인들은 전자들이 지금보다 약 1,000년 이상 오래된 고대 유적을 창구로 칭했습니다. & UTF8mj신나라 이집트인들은 전자들이 지금보다 약 1,000년 이상 오래된 고대 유적을 창구로 칭했고, 그들은 그것들을 참화했습니다. Korean & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & UTF8mj미국 공병대는 시간당 6인치의 강우량이 기 파손된 제방을 무너뜨릴 수 있다고 추정했다. & UTF8mj미국 엔지니어 중대는 6인치의 비가 이전에 손상된 제방을 깨고 들어올 수 있다고 추정했습니다. & UTF8mj미국 엔지니어 중대는 6인치의 비가 이전에 손상된 제방을 깨고 가로막을 칠 수 있다고 추정했습니다. Korean & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & UTF8mj대형 텔레비전 스크린 여러 대가 로마 곳곳에 설치되어 사람들이 장례식을 관람할 수 있도록 했습니다. & UTF8mj로마에서 여러 곳에 거대한 텔레비젼 스크린이 설치되어 이들이 의식을 시청할 수 있게 했습니다. & UTF8mj로마에서 여러 곳에 거대한 텔레비젼 스크린이 설치되어 이들이 의식을 시청할 수 있게 해 주었습니다. Korean & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & UTF8mj해충은 음식이 썩게 만들고 염증을 유발하거나, 더 나쁜 경우 알레르기 반응을 일으키고 독을 퍼뜨리거나 전염병을 옮길 수 있습니다. & UTF8mj당신은 잡초가 식료품을 망치거나, 자극을 유발할 수 있거나, 최악의 경우 알레르기 반응을 유발하거나, 독을 전파하거나, 감염을 전파할 수 있음을 알고 있나요? & UTF8mj잡초는 식료품을 망칠 수 있고, 자극을 유발할 수 있거나, 최악의 경우 알레르기 반응을 유발할 수 있고, 독을 전파할 수 있거나, 감염을 전파할 수 있습니다. Korean & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & UTF8mj인류의 과학 기술 발전으로 세상이 많이 변했다는 것은 충분히 명백하고, 또 인구과잉과 인류의 사치스러운 생활방식 때문에 문제가 더 커졌다. & UTF8mj인간의 과학기술 진보와 인간의 과감한 생활방식으로 인해 세계가 상당히 바뀌었고, 인구 초과와 문제가 커졌다는 것은 분명합니다. & UTF8mj인간의 과학기술 진보로 인해 세계가 상당히 바뀌었고, 인구 초과와 인간의 과감한 생활방식으로 인해 문제가 커졌음은 확실합니다. Korean & The correlation between brain pathology and behaviour supports scientists in their research. & UTF8mj뇌 병리와 행동 사이의 상관관계가 과학자들의 연구를 돕습니다. & UTF8mj뇌의 질환과 행동간의 상관관계가 과학자들의 연구를 지원한다. & UTF8mj뇌의 질환과 행동 사이의 상관관계는 과학자들의 연구를 지원합니다. Korean & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & UTF8mj다른 전문가들과 마찬가지로, 그는 당뇨병의 치료 여부에 회의적이며, 이러한 결과는 제1형 당뇨병 환자에게는 관련이 없음을 지적합니다. & UTF8mj몇몇 전문가들과 마찬가지로, 그는 저주파 치료가 당뇨병을 완전히 치료할 수 있을지에 대해 의문을 갖고 있으며, 이러한 연구 결과는 이미 1형 당뇨병을 가지고 있는 사람들과는 전혀 관계가 없다고 주장한다. & UTF8mj그는 일부 전문가들과 마찬가지로, 이들 연구 결과는 이미 1형 당뇨병을 가진 사람들에게는 전혀 관계가 없다고 주장하며, 당뇨병이 완전히 치료될 수 있을지에 대해 의문을 갖고 있다. Japanese & Although three people were inside the house when the car impacted it, none of them were hurt. & UTF8min車の衝突時には３人が家の中にいましたが、怪我人はいませんでした。 & UTF8minこの文を日本語に訳すと、「車が家に衝突したときには家の中に3人いましたが、そのうちの誰も傷を負わなかった」となります。 & UTF8minもしよろしければ、この文を元の文の意味を維持しながら、より日本語らしい表現に書き換えることもできます。例えば次のようになります。「車が家に衝突したときには、家の中には3人いましたが、どの人も怪我をしなかった」 Japanese & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & UTF8min世論調査では34Japanese & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & UTF8minサブカルチャーの特徴を決定付ける性質は、言語、美、宗教、政治、性、地理、またはそれらの要素の組み合わせかもしれません。 & UTF8minサブカルチャーを特徴的なものとする品質は、言語的、美学的、宗教的、政治的、性的、地理的、あるいはそれらの要因の組み合わせであることがあります。 & UTF8minそのような品質は、サブカルチャーを特徴的なものとするものである可能性があります。言語的、美学的、宗教的、政治的、性的、地理的、あるいはそれらの要因の組み合わせであることがあります。 Japanese & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & UTF8min新王国の古代エジプト人たちは、当時千年以上前に建てられた前身のモニュメントに驚嘆しました。 & UTF8min新王国時代のエジプト人は、当時すでに1000年以上も古い前人のモニュメントに驚嘆しました。 & UTF8min新王国時代のエジプト人は、それらの先人の墓苑が当時すでに1000年以上も古いことに驚嘆しました。 Japanese & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & UTF8min米工兵隊は、以前に決壊した堤防が6インチの降水量で再び決壊する可能性があると推定しています。 & UTF8min米国エンジニア兵団は、6インチの降雨で以前に損傷した堤防が浸水すると推定しています。 & UTF8min以前に損傷を受けた堤防が、米国エンジニア兵団が推定する6インチの降雨で浸水するかもしれません。 Japanese & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & UTF8minローマの各所には、人々に式典を見てもらおうと、大型テレビ画面がいくつも設置されていました。 & UTF8minローマにはさまざまな場所に大型テレビ画面が設置され、人々が式典を観られるようになった。 & UTF8minローマには、さまざまな場所に大型のテレビスクリーンが設置されました。これにより、人々は式典を観ることができるようになりました。 Japanese & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & UTF8min害虫は食物を台無しにしたり、皮膚を刺したり、最悪の場合はアレルギー反応の原因になったり、毒を撒き散らしたり、感染症を媒介することがあります。 & UTF8min害虫は食物を台無しにすることがあり、かゆみを起こすこともあり、さらに悪い場合にはアレルギー反応を引き起こすこともあり、毒を撒いたり、感染を伝播することもあります。 & UTF8min害虫は食物を損なうことがあり、かゆみを起こすこともあります。また、悪い場合にはアレルギー反応を引き起こすこともあり、毒を撒いたり、感染を伝播することもあります。 Japanese & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & UTF8min人類の科学技術の進歩によって世界は大きく変わり、過剰な人口と贅沢な生活が原因で問題が拡大したことは十分に明白な事実です。 & UTF8min人類の科学技術の進歩によって世界は大きく変化し、人口の増加や人間の浪費的な生活スタイルによって問題は大きくなっていることは明らかだ。 & UTF8min明らかであるように、人類の科学技術の進歩により、世界は大きく変化し、人口増加や人間の浪費的な生活スタイルにより、問題はより大きくなっている。 Japanese & The correlation between brain pathology and behaviour supports scientists in their research. & UTF8min脳病理と行動の相関関係は、科学者たちの研究を裏付けるものです。 & UTF8min脳の病理学と行動の関連は、科学者たちの研究を支援する。 & UTF8min脳の病理学と行動の関連は、科学者たちの研究を支援する Japanese & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & UTF8min他の専門家たちと同様に、彼は糖尿病を完治できるかどうかについては懐疑的であり、これらの調査結果はすでにI型糖尿病を患っている人々には何の関連性もないことを指摘しています。 & UTF8min日本語に翻訳すると、他の専門家のように、彼は糖尿病が治せるかどうかについて懐疑的であり、これらの発見は既に1型の糖尿病を持つ人々には関係がないと述べている。 & UTF8min日本語に翻訳すると、他の専門家のように、彼は糖尿病が完治できるかどうかについて懐疑的であり、これらの発見は既に1型の糖尿病を持つ人々には無関係だと述べている。 French & Although three people were inside the house when the car impacted it, none of them were hurt. & Bien que trois personnes aient été présentes dans la maison quand la voiture l'a percutée, aucune n'a été blessée. & Bien que trois personnes se trouvaient à l'intérieur de la maison lorsque la voiture a percuté celle-ci, aucune d'entre elles n'a été blessée. & Même si trois personnes étaient à l'intérieur de la maison lorsque la voiture a heurté celle-ci, aucune d'entre elles n'a été blessée. French & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & 34 French & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & Les critères qui déterminent une sous-culture comme distincte peuvent être linguistiques, esthétiques, religieux, politiques, sexuels, géographiques ou une combinaison de facteurs. & Les qualités qui déterminent une sous-culture comme distincte peuvent être linguistiques, esthétiques, religieuses, politiques, sexuelles, géographiques ou une combinaison de facteurs.\" & Les caractéristiques qui déterminent une sous-culture comme distincte peuvent être linguistiques, esthétiques, religieuses, politiques, sexuelles, géographiques ou une combinaison de ces éléments. French & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & Les Égyptiens antiques du Nouvel Empire étaient émerveillés par les monuments de leurs prédécesseurs, qui avaient alors bien plus de mille ans. & Les Égyptiens anciens de la Nouvelle-République admiraient les monuments de leurs prédécesseurs, qui étaient alors bien plus âgés d'un millénaire. & Les Égyptiens anciens de la Nouvelle-République admiraient les monuments de leurs prédécesseurs, qui avaient alors plus de mille ans. French & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & D’après le U.S. Corps of Engineers, une pluviométrie de 6 pouces pourrait ouvrir une brèche dans les digues précédemment endommagées. & Le Corps des ingénieurs des États-Unis a estimé qu'une pluie de 15 cm pourrait affaiblir les digues précédemment endommagées. & Le Corps des ingénieurs des États-Unis a estimé qu'une pluie de 15 cm pourrait mettre à mal les digues précédemment endommagées. French & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & Plusieurs grands écrans de télévision ont été installés à différents endroits à Rome pour permettre aux gens de regarder la cérémonie. & Plusieurs grands écrans de télévision ont été installés dans différents endroits de Rome pour permettre aux gens de regarder la cérémonie. & Plusieurs grands écrans de télévision ont été installés dans différents endroits de Rome afin de permettre aux gens de regarder la cérémonie. French & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & Les parasites peuvent contaminer la nourriture, provoquer des irritations ou, dans les cas les plus graves, provoquer des réactions allergiques, répandre du venin ou transmettre des infections. & Les nuisibles peuvent gâcher les aliments, provoquer des irritations, ou dans le pire des cas provoquer des réactions allergiques, propager du venin ou transmettre des infections. & Les nuisibles peuvent altérer les aliments, causer des irritations, ou dans les cas les plus graves provoquer des réactions allergiques, répandre du venin ou transmettre des infections. French & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & Il est clair que le monde a beaucoup changé grâce aux progrès scientifiques et technologiques réalisés par les êtres humains, et que les problèmes sont devenus plus importants en raison de la surpopulation et du mode de vie extravagant de l'humanité. & Il est assez évident que le monde a beaucoup changé en raison des avancées scientifiques et technologiques de l'humanité et que les problèmes se sont accrus en raison de la surpopulation et du mode de vie extravagant de l'homme. & Il est évident que le monde a beaucoup changé en raison des avancées scientifiques et technologiques de l'humanité, et que les problèmes ont augmenté à cause de la surpopulation et du mode de vie extravagant de l'homme. French & The correlation between brain pathology and behaviour supports scientists in their research. & La corrélation entre la pathologie du cerveau et le comportement soutient les scientifiques dans leurs recherches. & La corrélation entre la pathologie cérébrale et le comportement soutient les scientifiques dans leur recherche. & La corrélation entre la pathologie cérébrale et le comportement soutient les scientifiques dans leurs recherches. French & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & À l'instar d'autres experts, il se montre sceptique quant à la possibilité de guérir le diabète, faisant remarquer que ces résultats ne sont pas applicables aux personnes qui souffrent déjà de diabète de type 1. & Comme d'autres experts, il est sceptique quant à la possibilité de guérir du diabète, notant que ces résultats n'ont aucune pertinence pour les personnes atteintes de diabète de type 1. & Tout comme certains autres experts, il est sceptique quant à la possibilité de guérir du diabète, et souligne que ces résultats n'ont aucune pertinence pour les personnes atteintes de diabète de type 1. Indonesian & Although three people were inside the house when the car impacted it, none of them were hurt. & Walau ada tiga orang di dalam rumah yang ditabrak mobil, tak satu pun yang cedera. & Meskipun tiga orang ada di dalam rumah saat mobil menabraknya, tidak ada yang terluka. & Meskipun tiga orang ada di dalam rumah saat mobil menabraknya, mereka semua selamat. Indonesian & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & Dalam jajak pendapat mengenai pandangan ini, 34 persen dari mereka menginginkan Ratu Elizabeth II menjadi pemimpin monarki terakhir Australia. & 34 persen dari orang yang terlibat dalam polling ini memiliki pandangan yang sama, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir di Australia. & 34 persen dari orang yang terlibat dalam polling ini memiliki pandangan yang sama, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir di Australia. Indonesian & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & Kualitas yang menentukan suatu subkultur berbeda dari yang lain bisa berupa bahasa, estetika, agama, politik, seksual, geografis, atau kombinasi sejumlah faktor. & Kualitas yang menentukan suatu subkultur sebagai distinct dapat berupa linguistik, estetika, agama, politik, seksual, geografis, atau kombinasi faktor-faktor tersebut. & Faktor-faktor yang menentukan sebuah subkultur sebagai distinct bisa berupa faktor linguistik, estetika, agama, politik, seksual, geografis, atau kombinasi dari beberapa faktor tersebut. Indonesian & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & Kerajaan Baru Mesir kuno mengagumi monumen para pendahulunya, yang saat itu berusia lebih dari seribu tahun. & Kekaisaran Baru orang-orang Mesir kuno terpesona dengan monumen-monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. & Orang-orang Mesir kuno Kekaisaran Baru terpesona dengan monumen-monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. Indonesian & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & U.S. Corps of Engineers memperkirakan bahwa curah hujan sebesar 6 inci ini mampu menerobos tanggul yang sudah rusak. & Korps Engineer AS menghitung bahwa hujan 6 inci dapat merusak parit yang sebelumnya sudah rusak. & Badan Engineer AS memperkirakan bahwa hujan 6 inci dapat menembus parit yang sebelumnya sudah rusak. Indonesian & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & Beberapa layar televisi besar dipasang di berbagai tempat di Roma supaya orang-orang bisa menyaksikan upacara tersebut. & Beberapa layar televisi besar diinstal di berbagai tempat di Roma agar orang-orang dapat menonton upacara. & Berbagai layar televisi besar diinstal di tempat-tempat yang berbeda di Roma agar masyarakat dapat menonton upacara tersebut. Indonesian & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & Hama dapat merusak makanan, mengakibatkan iritasi, atau dalam kasus yang lebih parah, mengakibatkan reaksi alergi, menyebarkan racun atau menularkan penyakit. & Hama dapat merusak makanan, menyebabkan iritasi, atau dalam keadaan yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. & Hama dapat merusak makanan, menyebabkan iritasi, atau dalam keadaan yang lebih buruk bahkan dapat menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. Indonesian & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & Cukup jelas bahwa dunia berkembang karena ilmu pengetahuan dan teknologi, serta kelebihan jumlah penduduk dan gaya hidup boros manusia menjadi masalah yang cukup besar. & Jelas sekali bahwa dunia telah berubah banyak karena kemajuan ilmiah dan teknologi manusia, dan masalah telah menjadi lebih besar karena kelebihan populasi dan gaya hidup manusia yang berlebihan. & Sudah jelas sekali bahwa dunia telah berubah sangat banyak karena kemajuan ilmiah dan teknologi manusia, dan masalah telah menjadi lebih besar karena kelebihan populasi dan gaya hidup manusia yang berlebihan. Indonesian & The correlation between brain pathology and behaviour supports scientists in their research. & Korelasi antara patologi otak dan perilaku mendukung para ilmuwan dalam penelitian mereka. & Korelasi antara patologi otak dan perilaku membantu para ilmuwan dalam penelitian mereka. & Korelasi antara patologi otak dan tingkah laku membantu para ilmuwan dalam penelitian mereka. Indonesian & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & Seperti halnya ahli-ahli lain, dia merasa skeptis apakah diabetes dapat disembuhkan, menegaskan bahwa penemuan-penemuan ini tidak relevan dengan orang-orang yang telah mengidap diabetes Jenis 1. & Sama seperti beberapa ahli lainnya, ia ragu-ragu tentang apakah diabetes dapat disembuhkan, mencatat bahwa temuan-temuan ini tidak relevan bagi orang-orang yang sudah menderita diabetes tipe 1. & Seperti beberapa ahli lainnya, ia meragukan apakah diabetes dapat disembuhkan, mencatat bahwa temuan-temuan ini tidak relevan bagi orang-orang yang sudah menderita diabetes tipe 1. Javanese & Although three people were inside the house when the car impacted it, none of them were hurt. & Sanadyan uwong telu ana ing njero omah nalika mobil kuwi nabrak, nanging kabeh selamet. & Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada di antara mereka yang terluka. & Walaupun ana ing tiga wong ing rumah kala mobil mbentak ing kono, ora ana saka ingkang terluka. Javanese & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & 34 per sen sing menehi swara sesawangane ngene, kepengin Ratu Elizabeth II dadi raja sing keri dhewe ana ning Australia. & Sebagian 34 persen dari mereka yang terlibat dalam polling ini berpendapat demikian, ingin Ratu Elizabeth II menjadi Ratu Australia terakhir. & Sebagian 34 persen dari mereka yang terlibat dalam polling ini memiliki pandangan yang sama, yaitu menginginkan Ratu Elizabeth II menjadi Ratu Australia terakhir. Javanese & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & Kualitas sing nemtokake sawijining cabang kabudayan mbokmenawa basa, estetika, sing ana gayutane karo agama, politik, seksual, geografis, utawa sawijining kombinasi saka faktor-faktor. & Sifat-sifat yang menentukan subkultur sebagai yang terpisah bisa berupa linguistik, estetik, agama, politik, seksual, geografis, atau kombinasi faktor-faktor. & Kualitas-kualitas sing menentukan subkultur minangka distinct bisa ngandhut linguistik, estetik, agama, politik, seksual, geografis, utawa gabungan faktor-faktor. Javanese & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & Krajan Anyar Mesir kuno nggumun marang monumen-monumen leluhure, sing nalika iku tetep apik luwih seka sewu taun. & Rakyat Kerajaan Baru dari Mesir Kuno terpesona oleh monumen-monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. & Rakyat Kerajaan Baru dari Mesir Kuno terpesona oleh monumen-monumen pendahulunya, yang saat itu sudah lebih dari seribu tahun tua. Javanese & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & Korps Insinyur AS ngira manawa udan 6 inci bisa nembus tanggul sing rusak sadurunge. & Korps Insinyur Amerika Serikat menghitung bahwa hujan 6 inci bisa merusak parit yang sebelumnya rusak. & Korps Insinyur Amerika Serikat menaksir bahwa hujan 6 inci bisa merusak parit yang telah rusak sebelumnya. Javanese & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & Sawetara layar TV gedhe dipasang ing sawetara panggonan ing Roma supaya masyarakat bisa nonton upacara kasebut. & Beberapa layar televisi besar dipasang di berbagai tempat di Roma agar orang-orang dapat menonton upacara. & Beberapa layar televisi besar dipasang di beberapa tempat di Roma supaya orang-orang dapat menyaksikan upacara itu. Javanese & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & Ama bisa ngrusak panganan, nyebabake iritasi, utawa ing kasus sing paling parah nyebabake reaksi alergi, nyebar racun, utawa ngirim infeksi. & Hama bisa merusak makanan, menyebabkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. & Hama bisa merusak makanan, menyebabkan iritasi, atau dalam keadaan yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. Javanese & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & Cukup jelas menawa donya wis malih akeh amarga majune ilmu lan teknologi manungsa, lan masalah-masalah dadi tambah gedhe amarga kaluwihan populasi lan gaya urip sing boros saka manungsa. & Sawetara bisa dilihat manawa dunya wis duwé sak kabèh pangowahan sain-teknologi manungsa lan masalah duwé gedhé luwih amerga tambah-tambahé jumlah penduduk lan gaya hidup manungsa sing luwih mewah. & Sawetara bisa dilihat manawa dunya wis duwé sak kabèh pangowahan sain lan teknologi saka manungsa, lan masalah-masalah duwé gedhé luwih amerga tambah-tambahé jumlah penduduk lan gaya hidup manungsa sing luwih mewah Javanese & The correlation between brain pathology and behaviour supports scientists in their research. & Sesambungan antarane elmu lelara utek lan tumindak ndhukung para ilmuwan kanggo nliti. & Korelasi antara patologi otak dan perilaku mendukung para ilmuwan dalam penelitian mereka. & Korelasi antara patologi otak dan tingkah laku mendukung para ilmuwan dalam penelitian mereka. Javanese & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & Kaya sawetara ahli liyane, dheweke ora yakin apa diabetes bisa ditambani, nimbang yen panemuan kasebut ora duwe hubungane karo uwong sing uwis ngalami diabetes Jinis 1. & Sebagaimana beberapa ahli lainnya, dia skeptis tentang apakah diabetes dapat disembuhkan, dengan catatan bahwa temuan ini tidak relevan bagi orang-orang yang sudah menderita diabetes tipe 1. & Sekaya karo para ahli liya, dia luwih leres saka wether diabetes bisa dibenerake, nggandeng rembug kuwi ora duwe arti kanggo wong-wong sing wis duwe diabetes tipe 1. Sundanese & Although three people were inside the house when the car impacted it, none of them were hurt. & Sanaos tilu urang aya di jero bumi pas mobil na nabrak, teu aya anu cidera. & Walaupun tiga orang berada di dalam rumah saat mobil menabraknya, tidak ada yang terluka. & Walaupun ada tiga orang di dalam rumah saat mobil menabraknya, tak ada yang terluka. Sundanese & 34 per cent of those in the poll share this view, wanting Queen Elizabeth II to be Australia's last monarch. & 34 persen jalma dina polling di gaduh pendapat ieu, nyaeta hoyong Ratu Elizabeth II janten monarkis pamungkas di Australia. & 34 persen dari mereka yang terlibat dalam survei memiliki pandangan ini, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir Australia. & 34 persen dari mereka yang terlibat dalam survei memiliki pandangan yang sama, yaitu ingin Ratu Elizabeth II menjadi ratu terakhir di Australia. Sundanese & The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual, geographical, or a combination of factors. & Kualitas anu nangtukeun subkultur salaku bédana panginten basa, éstétis, religius, politik, séks, géografis, atanapi kombinasi faktor. & Sifat-sifat yang menentukan subkultur sebagai terpisah mungkin bersifat linguistik, estetika, agama, politik, seksual, geografis, atau kombinasi dari faktor-faktor tersebut. & Sifat-sifat nu nentukkeun subkultur salaku nu béda bisa jadi linguistik, estetika, agama, politik, seksual, geografis, atawa kombinasi ti faktor-faktor ieu. Sundanese & New Kingdom ancient Egyptians marvelled at their predecessors monuments, which were then well over a thousand year old. & Karajaan anyar urang Mesir kuno takjub kana monumén paninggalan karuhunna, anu umurna langkung ti sarebu taun. & Para raja anyar dina Egypt kuno nyogog nara-nara jaman awal, anu dina jaman ayeuna kira-kira ribuan taun. & Para raja anyar dina Egypt kuno terkagum-kagum kana nara-nara candi jaman awal, anu dina jaman ayeuna kira-kira leuweung taun. Sundanese & The U.S. Corps of Engineers estimated that 6 inches of rainfall could breach the previously damaged levees. & Korps insinyur A.S ngira-ngira curah hujan 6 inci tiasa nembus tanggul anu samemehna geus ruksak. & Korps Engineer AS diperkirakan bahwa 6 inci hujan bisa merusak benteng yang sebelumnya sudah rusak. & Korps Engineer AS diperkirakan bahwa hujan sebesar 6 inci dapat merusak benteng yang sudah rusak sebelumnya. Sundanese & Several large television screens were installed in various places in Rome to let the people watch the ceremony. & Sababaraha tipi ageung dipasang di sababaraha tempat di Roma supados warga tiasa nyaksian upacara éta. & Beberapa layar televisi besar dipasang di berbagai tempat di Roma agar orang-orang bisa menonton upacara. & Sabagéan layar televisi gaduh dipasang di hiji bilik roma sinaunna pikeun ngajadikeun jalma bisa nonton upacara. Sundanese & Pests can spoil food, cause irritation, or in a worse case cause allergic reactions, spread venom, or transmit infections. & Hama tiasa ngarusak dahareun, nyieun iritasi, atawa leuwih parah deui tiasa nyieun alérgi, nyebarkeun racun, atawa nularkeun panyakit. & Hama bisa merusak makanan, menyebabkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. & Hama bisa merusak makanan, menyebabkan iritasi, atau dalam kasus yang lebih buruk menyebabkan reaksi alergi, menyebarkan racun, atau menularkan infeksi. Sundanese & It is obvious enough that the world has changed much because of humankind's scientific and technological advancements, and problems have become greater because of overpopulation and mankind's extravagant lifestyle. & Jelas pisan yén dunya parantos seueur robih kusabab kamajuan élmu manusa sareng téknologi, sareng masalahna janten langkung ageung kusabab seueur penduduk sareng gaya hirup anu boros umat manusa. & Nyaéta nujul bisa ngeuyeuk yén dunya geus robah heubeul dina jaman sains jeung teknologi manusa, jeung masalah geus ngaronjat luyu ku luweung laju penduduk jeung gaya hidup manusa anu boros. & Nyaéta nujul bisa ngeuyeuk yén dunya geus robah heubeul dina jaman sains jeung teknologi manusa, jeung masalah geus ngaronjat luyu ku luweung laju penduduk jeung gaya hidup manusa anu boros. Sundanese & The correlation between brain pathology and behaviour supports scientists in their research. & Hubungan patologi polo sareng kalakuan ngabantos para élmuwan dina panalungtikanna. & Korelasi antara patologi otak dan perilaku mendukung para ilmuwan dalam penelitian mereka. & Korelasina antara patologi otak jeung ulah-ulahan ngalapkeun dukungan sakurang-kurangna pikeun para ilmuwan dina penelitian maranéhanana. Sundanese & Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. & Sapertos sababaraha ahli anu sanés, anjeunna henteu percanteun upami diabétés tiasa disembuhkeun, kusabab pamanggihan ieu téh henteu aya hubunganana jeung jalma anu parantos gaduh diabétés tipe 1. & Seperti beberapa ahli lainnya, dia skeptis tentang apakah diabetes bisa disembuhkan, mencatat bahwa temuan ini tidak relevan bagi orang-orang yang sudah memiliki diabetes tipe 1. & Kayaku ngan ahli séjén, dia bérék-bérék ngeunaan jangdi diabetes bisa diobat, ngeunaan yén kajadian ieu teu aya hubunganna jeung jalma anu geus ngalami diabetes tipe 1.\" Examples of ChatGPT translated and post-edited sentences. Evaluation Results for Reasoning We provide the complete results for reasoning tasks on~tab:reasoning. Multi-turn for Task-Oriented Dialogue We provide the example for the modular and unified approaches for Task-Oriented Dialogue in tab:tod-modular and tab:tod-unified, respectively.",
    "idea_abstract": "We propose a framework for evaluating interactive large language models like ChatGPT using publicly available datasets. Our evaluation framework assesses the multitask, multilingual, and multi-modal capabilities of such models, leveraging a diverse range of tasks and a newly designed multimodal dataset. We investigate the strengths and limitations of these models, including their ability to generate multimodal content and their susceptibility to hallucinations. Our findings highlight the potential for human collaboration to improve the performance of these models.",
    "proposal": "1. Title: Evaluating the Multitask, Multilingual, and Multimodal Capabilities of ChatGPT\n\n2. Problem Statement: The paper aims to comprehensively evaluate the multitask, multilingual, and multimodal capabilities of ChatGPT, a large language model, to understand its strengths and limitations in various NLP tasks, languages, and modalities.\n\n3. Motivation: Despite its popularity, ChatGPT's capabilities and limitations are not well understood, and existing evaluations have focused on specific tasks or languages. This paper aims to fill this gap by evaluating ChatGPT's performance on a wide range of tasks, languages, and modalities, providing insights into its multitask, multilingual, and multimodal abilities.\n\n4. Proposed Method: The authors propose a comprehensive evaluation framework that includes multitask, multilingual, and multimodal evaluations of ChatGPT. They use a variety of datasets and tasks to assess ChatGPT's performance, including question answering, machine translation, sentiment analysis, and multimodal generation. They also investigate ChatGPT's ability to understand and generate text in multiple languages, as well as its multimodal capabilities, such as generating images and code."
  },
  {
    "rank": 10,
    "title": "A Survey of Large Language Models",
    "arxiv": "2303.18223v11",
    "paper_tar": "tar_files/2303.18223v11.tar.gz",
    "abstract": "Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence~(AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models~(PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing~(NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities ( in-context learning) that are not present in small-scale language models ( BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models~(LLM) for the PLMs of significant size ( containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.",
    "content": "[10pt,journal,compsoc,x11names]IEEEtran [switch]lineno amsmath xspace enumitem amssymb booktabs [hyphens]url ragged2e hyperref multirow graphicx xcolor color colortbl tablefootnote pifont makecell [most]tcolorbox framed mdframed subfigure caption longtable float [1]1.5ex#1 H>0=c<@ i.e., a.k.a., e.g., w.r.t. w/o etc [1] [2] goldRGB205,133,63 fGreenRGB34,139,34 tOrangeRGB255,215,0 tBlueRGB135,206,250 tPinkRGB255,204,204 tGreenRGB205,230,199 tGoldRGB255,215,0 cite [numbers,sort&compress]natbib bookmarks=true,bookmarksnumbered=true, pdfpagemode=UseOutlines,plainpages=false,pdfpagelabels=true, colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black, pdftitle=Bare Demo of IEEEtran.cls for Computer Society Journals, pdfsubject=Typesetting, pdfauthor=Michael D. Shell, pdfkeywords=Computer Society, IEEEtran, journal, LaTeX, paper, template op-tical net-works semi-conduc-tor A Survey of Large Language Models Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen Version: v11 (major update on June 29, 2023). GitHub link: https://github.com/RUCAIBox/LLMSurvey * K. Zhou and J. Li contribute equally to this work. The authors are mainly with Gaoling School of Artificial Intelligence and School of Information, Renmin University of China, Beijing, China; Jian-Yun Nie is with DIRO, Universite de Montreal, Canada. Contact e-mail: batmanfly@gmail.com Shell et al.: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence~(AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models~(PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing~(NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities ( in-context learning) that are not present in small-scale language models ( BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models~(LLM) for the PLMs of significant size ( containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers. Large Language Models; Emergent Abilities; Adaptation Tuning; Utilization; Alignment; Capacity Evaluation Acknowledgments Acknowledgment The authors would like to thank Yankai Lin and Yutao Zhu for proofreading this paper. Since the first release of this paper, we have received a number of valuable comments from the readers. We sincerely thank the readers who have written to us with constructive suggestions and comments: Tyler Suard, Damai Dai, Liang Ding, Stella Biderman, Kevin Gray, Jay Alammar, Yubo Feng, Mark Holmstrom, Xingdong Liu, Il-Seok Oh, Yiting Liu, Shaojun Wang, Gaoyan Ou, and Todd Morrill. In this version (June 29, 2023), we add a large number of experiments and prompt practices. These new contents are completed by a number of volunteers in our team. Here, we add a special part to thank all the students who have worked very hard on this part (also including the ones on our author list). Contribution on Experiments. We would like to sincerely thank the following people for their hard work involved in experiments shown in Table~. Xiaoxue Cheng: implement the experiments for evaluation on Language Generation and HaluEval tasks. Yuhao Wang: implement the experiments for evaluation on interaction with environment tasks. Bowen Zheng: implement the experiments for evaluation on tool manipulation tasks. Contribution on Tips. We list the following guys for their contributions on the corresponding numbers of provided tips for designing prompts in Table~. Xiaolei Wang: T3, O3 Beichen Zhang: D2, D5 Zhipeng Chen: D3, D4 Junjie Zhang: D6 Bowen Zheng: D7 Zican Dong: D8 Xinyu Tang: C2 Yifan Du: T4 Tianyi Tang: O6, O7, D9 Yupeng Hou: O8, C3",
    "idea_abstract": "Developing artificial intelligence that can comprehend and generate human language is a long-standing challenge in the field of natural language processing. Language is a complex system governed by grammatical rules, and researchers have explored various approaches to master it, including language modeling. Recent advances in pre-trained language models have shown strong capabilities in solving NLP tasks, and further scaling these models has led to improved performance and new abilities such as in-context learning. Large language models, characterized by their significant parameter size, have become a focus of research and development, with applications in AI chatbots and other areas. This survey reviews recent advances in large language models, covering pre-training, adaptation, utilization, and capacity evaluation, as well as available resources and future directions.",
    "proposal": "1. Title: Exploring the Frontiers of Large Language Models\n\n2. Problem Statement: Developing artificial intelligence algorithms that can comprehend and generate human language is a long-standing challenge in the field of natural language processing. Language is a complex system governed by grammatical rules, making it difficult to create capable language models that can understand and produce human-like language.\n\n3. Motivation: Existing language models, including pre-trained language models, have shown strong capabilities in solving various NLP tasks, but their performance is limited by their scale. Recent research has demonstrated that increasing the parameter scale of language models can lead to improved performance and special abilities, such as in-context learning. However, the community lacks a comprehensive understanding of the technical evolution of large language models and their applications.\n\n4. Proposed Method: This survey aims to provide an up-to-date review of the recent advances in large language models, focusing on four major aspects: pre-training, adaptation tuning, utilization, and capacity evaluation. We will introduce the background, key findings, and mainstream techniques in these areas, as well as summarize available resources for developing large language models and discuss remaining issues for future directions."
  },
  {
    "rank": 11,
    "title": "Visual Instruction Tuning",
    "arxiv": "2304.08485v1",
    "paper_tar": "tar_files/2304.08485v1.tar.gz",
    "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce : ^*Equal contribution, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding. Our early experiments show that  demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1When fine-tuned on Science QA, the synergy of  and GPT-4 achieves a new state-of-the-art accuracy of 92.53\\",
    "content": "article numbers, compressnatbib [preprint]neurips_2023 [utf8]inputenc [T1]fontenc [pagebackref,breaklinks,colorlinks]hyperref url booktabs amsfonts nicefrac microtype xcolor xcolor [1]red#1 [1]blue[yj: #1] [1]orange[HT: #1] [1]red[Chunyuan: #1] LLaVA Large Language and Vision Assistant Visual Instruction Tuning Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee University of Wisconsin--Madison~~~~~~Microsoft Research~~~~~~Columbia University https://llava-vl.github.iohttps://llava-vl.github.io Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce : Equal contribution, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding. Our early experiments show that demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1When fine-tuned on Science QA, the synergy of and GPT-4 achieves a new state-of-the-art accuracy of 92.53",
    "idea_abstract": "We explore the application of instruction tuning to multimodal language models using machine-generated instruction-following data. This approach involves using a language-only model to generate data that combines language and image instructions, which is then used to fine-tune a multimodal model that connects a vision encoder and language model. Our goal is to create a general-purpose model that can understand and respond to both visual and linguistic inputs.",
    "proposal": "Here is the rewritten idea abstract:\n\n**Multimodal Instruction Tuning for Large Language Models**\n\n**Problem Statement:** Current instruction tuning methods for large language models (LLMs) have shown promise in improving zero-shot capabilities on new tasks, but their application to multimodal tasks remains largely unexplored. The challenge lies in generating multimodal instruction-following data that can effectively leverage the strengths of both vision and language understanding.\n\n**Motivation:** Existing methods rely on human-generated instruction-following data, which can be time-consuming and expensive to create. Moreover, these methods often require task-specific fine-tuning, limiting their generalizability. We aim to address these limitations by exploring the use of machine-generated multimodal instruction-following data for instruction tuning.\n\n**Proposed Method:** We propose using a language-only model, such as GPT-4, to generate multimodal language-image instruction-following data. This generated data is then used to fine-tune a large multimodal model that connects a vision encoder and LLM, enabling end-to-end visual and language understanding. Our approach, dubbed , seeks to leverage the strengths of both vision and language understanding to achieve general-purpose multimodal capabilities."
  },
  {
    "rank": 12,
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "arxiv": "2305.10601v1",
    "paper_tar": "tar_files/2305.10601v1.tar.gz",
    "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ``Tree of Thoughts'' (ToT), which generalizes over the popular ``Chain of Thought'' approach to prompting language models, and enables exploration over coherent units of text (``thoughts'') that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\\",
    "content": "article numbers, compressnatbib [preprint]neurips_2023 algorithm algpseudocode xcolor forest qtree hyperref url booktabs amsfonts nicefrac microtype listings algorithm algpseudocode soul capt-of [shortlabels]enumitem dsfont tikz shapes,arrows tabularx MyDarkBluergb0,0.08,1 MyDarkGreenrgb0.02,0.6,0.02 MyDarkRedrgb0.8,0.02,0.02 MyDarkOrangergb0.40,0.2,0.02 MyPurpleRGB111,0,255 MyRedrgb1.0,0.0,0.0 MyGoldrgb0.75,0.6,0.12 MyDarkgrayrgb0.66, 0.66, 0.66 MyYellowrgb254, 246, 170 MyBluergb170, 217, 251 I m M [1]#1 HTML simple [1]search[#1] [1]choose[#1] Choice TBD mellowredHTMLCB4042 mellowblueHTML0089A7 [1]yellow#1 Tree of Thoughts: Deliberate Problem Solving with Large Language Models Shunyu Yao Princeton University Dian Yu Google DeepMind Jeffrey Zhao Google DeepMind Izhak Shafran Google DeepMind Thomas L. Griffiths Princeton University Yuan Cao Google DeepMind Karthik Narasimhan Princeton University Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ``Tree of Thoughts'' (ToT), which generalizes over the popular ``Chain of Thought'' approach to prompting language models, and enables exploration over coherent units of text (``thoughts'') that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4Introduction Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT~ and PaLM~ have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms? The literature on human cognition provides some clues to answer these questions. Research on ``dual process'' models suggests that people have two modes in which they engage with decisions -- a fast, automatic, unconscious mode (``System 1'') and a slow, deliberate, conscious mode (``System 2'') . These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative ``model free'' learning or more deliberative ``model based'' planning . The simple associative token-level choices of LMs are also reminiscent of ``System 1'', and thus might benefit from augmentation by a more deliberate ``System 2'' planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions. To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950s~. Newell and colleagues characterized problem solving~ as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure~ illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Table~). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures~,,). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking. Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4~: Game of 24, Creative Writing, and Crosswords (Table~). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs. Background We first formalize some existing methods that use large language models for problem-solving, which our approach is inspired by and later compared with. We use to denote a pre-trained LM with parameters , and lowercase letters to denote a language sequence, i.e. where each is a token, so that . We use uppercase letters to denote a collection of language sequences. Input-output (IO) prompting is the most common way to turn a problem input into output with LM: , where wraps input with task instructions and/or few-shot input-output examples. For simplicity, let us denote , so that IO prompting can be formulated as . Chain-of-thought (CoT) prompting~ was proposed to address cases where the mapping of input to output is non-trivial (e.g.when is a math question and is the final numerical answer). The key idea is to introduce a chain of thoughts to bridge and , where each is a coherent language sequence that serves as a meaningful intermediate step toward problem solving (e.g. could be an intermediate equation for math QA). To solve problems with CoT, each thought is sampled sequentially, then the output . In practice, is sampled as a continuous language sequence, and the decomposition of thoughts (e.g.is each a phrase, a sentence, or a paragraph) is left ambiguous. Self-consistency with CoT (CoT-SC)~ is an ensemble approach that samples i.i.d.chains of thought: , then returns the most frequent output: . CoT-SC improves upon CoT, because there are generally different thought processes for the same problem (e.g.different ways to prove the same theorem), and the output decision can be more faithful by exploring a richer set of thoughts. However, within each chain there is no local exploration of different thought steps, and the ``most frequent'' heuristic only applies when the output space is limited (e.g.multi-choice QA). Tree of Thoughts: Deliberate Problem Solving with LM A genuine problem-solving process involves the repeated use of available information to initiate exploration, which discloses, in turn, more information until a way to attain the solution is finally discovered. ------ -5pt Research on human problem-solving suggests that people search through a combinatorial problem-space -- a tree where the nodes represent partial solutions, and the branches correspond to operators that modify them . Which branch to take is determined by heuristics that help to navigate the problem-space and guide the problem-solver towards a solution. This perspective highlights two key shortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not explore different continuations within a thought process -- the branches of the tree. 2) Globally, they do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different options -- the kind of heuristic-guided search that seems characteristic of human problem-solving. To address these shortcomings, we introduce Tree of Thoughts (ToT), a paradigm that allows LMs to explore multiple reasoning paths over thoughts (Figure~(c)). ToT frames any problem as a search over a tree, where each node is a state representing a partial solution with the input and the sequence of thoughts so far. A specific instantiation of ToT involves answering four questions: 1. How to decompose the intermediate process into thought steps; 2. How to generate potential thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use. 1.Thought decomposition. While CoT samples thoughts coherently without explicit decomposition, ToT leverages problem properties to design and decompose intermediate thought steps. As Table~ shows, depending on different problems, a thought could be a couple of words (Crosswords), a line of equation (Game of 24), or a whole paragraph of writing plan (Creative Writing). In general, a thought should be ``small'' enough so that LMs can generate promising and diverse samples (e.g.generating a whole book is usually too ``big'' to be coherent), yet ``big'' enough so that LMs can evaluate its prospect toward problem solving (e.g.generating one token is usually too ``small'' to evaluate). 2.Thought generator . Given a tree state , we consider two strategies to generate candidates for the next thought step: [(a)] -6pt Sample i.i.d.thoughts from a CoT prompt (Creative Writing, Figure~): . This works better when the thought space is rich (e.g.each thought is a paragraph), and i.i.d.samples lead to diversity; Propose thoughts sequentially using a ``propose prompt'' (Game of 24, Figure~; Crosswords, Figure~): . This works better when the thought space is more constrained (e.g.each thought is just a word or a line), so proposing different thoughts in the same context avoids duplication. -6pt 3.State evaluator . Given a frontier of different states, the state evaluator evaluates the progress they make towards solving the problem, serving as a heuristic for the search algorithm to determine which states to keep exploring and in which order. While heuristics are a standard approach to solving search problems, they are typically either programmed (e.g.DeepBlue~) or learned (e.g.AlphaGo~). We propose a third alternative, by using the LM to deliberately reason about states. When applicable, such a deliberate heuristic can be more flexible than programmed rules, and more sample-efficient than learned models. Similar to the thought generator, we consider two strategies to evaluate states either independently or together: -5pt [(a)] Value each state independently: , where a value prompt reasons about the state to generate a scalar value (e.g.1-10) or a classification (e.g.sure/likely/impossible) that could be heuristically turned into a value. The basis of such evaluative reasoning can vary across problems and thought steps. In this work, we explore evaluation via few lookahead simulations (e.g.quickly confirm that 5, 5, 14 can reach 24 via 5 + 5 + 14, or ``hotl'' can mean ``inn'' via filling ``e'' in ``'') plus commonsense (e.g.1 2 3 are too small to reach 24, or no word can start with ``tzxc''). While the former might promote ``good'' states, the latter could help eliminate ``bad'' states. Such valuations do not need to be perfect, and only need to be approximately Vote across states: , where a ``good'' state is voted out based on deliberately comparing different states in in a vote prompt. When problem success is harder to directly value (e.g.passage coherency), it is natural to to instead compare different partial solutions and vote for the most promising one. This is similar in spirit to a ``step-wise'' self-consistency strategy, i.e.cast ``which state to explore'' as a multi-choice QA, and use LM samples to vote for it. -6pt For both strategies, we could prompt the LM multiple times to aggregate the value or vote results to trade time/resource/cost for more faithful/robust heuristics. 4. Search algorithm. Finally, within the ToT framework, one can plug and play different search algorithms depending on the tree structure. We explore two relatively simple search algorithms and leave more advanced ones (e.g.A* , MCTS ) for future work: -7pt [(a)] Breadth-first search (BFS) (Algorithm~) maintains a set of the most promising states per step. This is used for Game of 24 and Creative Writing where the tree depth is limit (), and initial thought steps can be evaluated and pruned to a small set (). Depth-first search (DFS) (Algorithm~) explores the most promising state first, until the final output is reached (), or the state evaluator deems it impossible to solve the problem from the current ( for a value threshold ). In the latter case, the subtree from is pruned to trade exploration for exploitation. In both cases, DFS backtracks to the parent state of to continue exploration. -7pt Conceptually, ToT has several benefits as a method for general problem-solving with LMs: (1) Generality. IO, CoT, CoT-SC, and self-refinement can be seen as special cases of ToT (i.e. trees of limited depth and breadth; Figure~). (2) Modularity. The base LM, as well as the thought decomposition, generation, evaluation, and search procedures can all be varied independently. (3) Adaptability. Different problem properties, LM capabilities, and resource constraints can be accommodated. (4) Convenience. No extra training is needed, just a pre-trained LM is sufficient. The next section will show how these conceptual benefits translate to strong empirical performance in different problems. Experiments We propose three tasks that are hard even when sampling from the state-of-the-art language model, GPT-4~, using standard IO prompting or chain-of-thought (CoT) prompting. We show how deliberate search in trees of thoughts (ToT) produces better results, and more importantly, interesting and promising new ways to use language models to solve problems requiring search or planning. Unless otherwise stated, we perform experiments using a Chat Completion mode GPT-4 with a sampling temperature of 0.7. Game of 24 Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic arithmetic operations (+-*/) to obtain 24. For example, given input ``4 9 10 13'', a solution output could be ``(10 - 4) * (13 - 9) = 24''. Task Setup. We scrape data from https://www.4nums.com/game/difficulties/4nums.com, which has 1,362 games that are sorted from easy to hard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing. For each task, we consider the output as success if it is a valid equation that equals 24 and uses the input numbers each exactly once. We report the success rate across 100 games as the metric. Baselines. We use a standard input-output (IO) prompt with 5 in-context examples. For chain-of-thought (CoT) prompting, we augment each input-output pair with 3 intermediate equations, each operating on two remaining numbers. For example, given input ``4 9 10 13'', the thoughts could be ``13 - 9 = 4 (left: 4 4 10); 10 - 4 = 6 (left: 4 6); 4 * 6 = 24 (left: 24)''. For each game, we sample IO and CoT prompting for 100 times for average performance. We also consider a CoT self-consistency baseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on top of an IO sample for at most iterations. At each iteration, the LM is conditioned on all previous history to ``reflect on your mistakes and generate a refined answer'' if the output is incorrect. Note that it uses groundtruth feedback signals about equation correctness. ToT Setup. To frame Game of 24 into ToT, it is natural to decompose the thoughts into 3 steps, each an intermediate equation. As shown in Figure~(a), at each tree node, we exact the ``left'' numbers and prompt the LM to propose some possible next steps. The same ``propose prompt'' is used for all 3 thought steps, though it only has one example with 4 input numbers. We perform a breadth-first search (BFS) in ToT, where at each step we keep the best candidates. To perform deliberate BFS in ToT, as shown in Figure~(b), we prompt LM to evaluate each thought candidate as ``sure/maybe/impossible'' with regard to reaching 24. The aim is to promote correct partial solutions that can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on ``too big/small'' commonsense, and keep the rest ``maybe''. We sample values times for each thought. Results. As shown in Table~, IO, CoT, and CoT-SC prompting methods perform badly on the task, achieving only 7.3We also consider an oracle setup for IO/CoT, by calculating the success rate using best of samples . To compare IO/CoT (best of k) with ToT, we consider calculating the tree nodes visited per task in ToT across , and map the 5 success rates in Figure~(a), treating IO/CoT (best of ) as visiting nodes in a bandit. Not surprisingly, CoT scales better than IO, and best of 100 CoT samples achieve a success rate of b0.56k 5k=5k=5b=15 55 51, 6, , 91, 96136, 141,146,151,1565 5 = 25516Oracle and ablation studies. When outputting from the oracle best DFS state (instead of the heuristically determined best state) per task, ToT performance is even higher and actually solves 7/20 games (Table~, ``+best state''), indicating our simple output heuristics can be readily improved. Interestingly, sometimes when the crosswords game is actually solved, the state evaluator might still deem some words as ``impossible'' and prune --- possibly because crosswords by design have some rare or obselete words that GPT-4 cannot recognizeFor example, ``agend'' is an obsolete form of ``agendum'', but GPT-4 deems it a typo for ``agenda''. External retrieval or web interaction could augment LM for problem solving under knowledge uncertainty.. Given the state evaluation as a pruning heuristic is imperfect, we also explore ablating the pruning, and find the performance generally worse (Table~, ``-prune''). However, it could actually find the correct solution for 4/20 games (though only outputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps. Thus, better heuristics for DFS pruning are critical for problem solving in this case. Lastly, we confirm the importance of backtracking by running an ablation that keeps filling the most promising clue for at most 20 steps, allowing overwrites. This is similar to a ``greedy'' BFS search with breadth limit of , and performs poorly with a word level success of only $20Related Work Planning and decision making. Smart planning and decision making are critical to achieving predefined goals. As they are trained on vast amount of world knowledge and human examples, LMs are known to have already absorbed rich commonsense that makes it possible to propose reasonable plans conditioned on problem setting and environmental states ~. Our proposed Tree-of-Thought approach extends existing planning formulations by considering multiple potentially feasible plans simultaneously at each problem-solving step, and proceeding with the most promising ones. The integration between thought sampling and value feedback organically integrates planning and decision-making mechanisms, enabling effective search inside a solution tree. On the other hand, traditional decision-making procedures usually require training dedicated reward and policy models as in reinforcement learning (for example CHAI~), whereas we use the LM itself to provide the value estimates for decision making. Self-reflection. Using LLMs to assess the viability of their own predictions is becoming an increasingly important procedure in problem solving. introduced the ``self-reflection'' mechanism, in which LMs provide feedback to their generation candidates. improves LMs code generation accuracy by injecting feedback messages generated by the LM itself based on its code execution results. Similarly, also introduces ``critic'' or review steps over the actions and states, deciding the next action to take in solving computer operation tasks. Another recent work very relevant to ours is ``self-eval guided decoding'' ~. Similar to our method, self-eval decoding also follows a tree-search procedure with leaves sampled from stochastic beam search decoding, which are then evaluated by LLM itself with carefully prepared self-eval prompts. Their approach however, uses the PAL formulation ~ which represents thoughts as codes, which makes it difficult to tackle challenging tasks like creative writing which we consider in this paper. Our Tree-of-Thought formulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very low accuracy with standard prompts. Program-guided LLM generation. Our proposal is also related to recent advancements that organize LM's behavior with symbolic program guidance. For example ~ embeds LMs in an algorithmic search procedure to help solve problems like question answering step-by-step, in which the search trees are expanded by relevant paragraphs that might provide answers. This approach however differs from ours in that trees are expanded by sampling external paragraphs instead of the LM's own thoughts, and there is no reflection or voting steps. Another approach, LLM+P~, goes one step further and delegates the actual planning process to a classical planner. Classical search methods. Last but not least, our approach can be treated as a modern rendition of classical search methods for problem solving. For example it can be considered as a heuristic search algorithm like A*~, in which the heuristic at each search node is provided by the LM's self-assessment. From this perspective, our method is also related to NeuroLogic A*esque decoding proposed in ~, which is inspired by A* search but introduces look-ahead heuristics that are efficient for LMs to improve the beam-search or top-k sampling decoding. This method however is constrained to sentence generation tasks, whereas our framework are designed for complex, multi-step problem solving guarded by value feedback. Discussion Limitations and future directions. Deliberate search such as ToT might not be necessary for many existing tasks that GPT-4 already excels at, and as an initial step this work only explores three relatively simple tasks that challenges GPT-4 and calls of better search and planning abilities incorporated with LMs. However, as we begin to deploy LMs for more real-world decision making applications (e.g.coding, data analysis, robotics, etc.), more complex tasks could emerge and present new opportunities to study these research questions. Also, search methods like ToT requires more resources (e.g.GPT-4 API cost) than sampling methods in order to improve task performances, but the modular flexibility of ToT allows users to customize such performance-cost tradeoffs, and ongoing open-source efforts~ should readily reduce such costs in the near future. Lastly, this work focuses on using an off-the-shelf LM, and fine-tuning LMs using a ToT-style high-level counterfactual decision making (e.g.deliberating over potential choices for the next paragraph, instead of predicting the next token) might present opportunities to enhance the problem-solving capabilities of LMs. Broader impact. ToT is a framework that empowers LMs to more autonomously and intelligently make decisions and solve problems. While current tasks are limited to reasoning and search problems, future applications involving interaction with external environments or humans could bring potential danger, e.g.facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability of model decisions and the opportunity for human alignment, as the resulting representations are readable, high-level language reasoning instead of implicit, low-level token values. Conclusion. The associative ``System 1'' of LMs can be beneficially augmented by a ``System 2'' based on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts framework provides a way to translate classical insights about problem-solving into actionable methods for contemporary LMs. At the same time, LMs address a weakness of these classical methods, providing a way to solve complex problems that are not easily formalized, such as creative writing. We see this intersection of LMs with classical approaches to AI as an exciting direction for future work.",
    "idea_abstract": "We propose a novel framework for language model inference, \"Tree of Thoughts\" (ToT), which enables exploration over coherent units of text, or \"thoughts\", to facilitate deliberate decision making and strategic lookahead. By considering multiple reasoning paths and self-evaluating choices, ToT allows language models to perform more effective problem-solving on tasks requiring non-trivial planning or search, such as games, creative writing, and puzzles.",
    "proposal": "1. Title: Deliberate Problem Solving with Large Language Models\n\n2. Problem Statement: Current language models are confined to token-level, left-to-right decision-making processes, which can fall short in tasks requiring exploration, strategic lookahead, or where initial decisions play a pivotal role. This limits their ability to solve complex problems that require non-trivial planning or search.\n\n3. Motivation: Existing methods, such as input-output prompting and chain-of-thought prompting, are not sufficient to address these challenges. They either sample continuous language sequences or use ensemble approaches that lack local exploration and global planning. The proposed method aims to address these shortcomings by introducing a framework that enables language models to perform deliberate decision making through exploration of coherent units of text, or \"thoughts,\" and systematic search.\n\n4. Proposed Method: The Tree of Thoughts (ToT) framework generalizes over the popular \"Chain of Thought\" approach to prompting language models, enabling exploration over coherent units of text that serve as intermediate steps toward problem solving. ToT maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving. The framework involves four key components: thought decomposition, thought generation, state evaluation, and search algorithm. Thought decomposition involves breaking down the"
  },
  {
    "rank": 13,
    "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
    "arxiv": "2305.16291v1",
    "paper_tar": "tar_files/2305.16291v1.tar.gz",
    "abstract": "=-1 We introduce , the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention.  consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.  interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by  are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,  shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3  more unique items, travels 2.3  longer distances, and unlocks key tech tree milestones up to 15.3  faster than prior SOTA.  is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize.",
    "content": "article numbers, compressnatbib [preprint]neurips_2023 [utf8]inputenc [T1]fontenc hyperref url booktabs amsfonts nicefrac microtype [usenames,dvipsnames]xcolor xspace graphicx caption subcaption multirow, makecell multicol array amssymb,bm,amsthm enumitem adjustbox xfrac listings codegreenrgb0,0.6,0 codegrayrgb0.5,0.5,0.5 codepinkRGB252, 142, 172 codepurplergb0.58,0,0.82 backcolourRGB245,245,245 mystyle backgroundcolor=backcolour, commentstyle=magenta, keywordstyle=blue, numberstyle=codegray, stringstyle=codepurple, basicstyle=, breakatwhitespace=false, breaklines=true, keepspaces=true, frame=single, numbersep=5pt, showspaces=false, showstringspaces=false, showtabs=false, tabsize=2, classoffset=1, keywordstyle=violet, classoffset=0, style=mystyle JavaScript keywords=typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break, keywordstyle=blue, ndkeywords=class, export, boolean, throw, implements, import, this, ndkeywordstyle=darkgray, identifierstyle=black, sensitive=false, comment=[l]//, morecomment=[s]/**/, commentstyle=purple, stringstyle=red, morestring=[b]', morestring=[b]\" tabularx L[1]>0ptm#1 C[1]>0ptm#1 R[1]>0ptm#1 Y>X soul [1]WhiteOrange#1 [1]WhiteVioletRed#1 [1]WhitePurple#1 [1]WhiteRed#1 [1]TODO: #1 [1]URGENT: #1 [1]Orange!90White cite:#1 [1]Yellow#1 [1]#1 [1]#1 snsgrayRGB179, 179, 179 snsorangeRGB252, 141, 98 snsblueRGB141, 160, 203 coolgreyRGB157,157,157 lightgreyRGB235,238,238 lighttealRGB198,211,222 cyanRGB136, 204, 238 tealRGB68, 170, 153 sandRGB221, 204, 119 roseRGB204, 102, 119 redRGB250, 94, 91 orangeRGB255, 200, 63 yellowRGB254, 239, 109 darkgreenrgb0.09, 0.45, 0.27 [2]#1 #2 [2]2pt#1#2 [0]Voyager [1]darkgreen#1 [1]darkgreen#1[1]#1=-1 : An Open-Ended Embodied Agent with Large Language Models https://voyager.minedojo.org Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar NVIDIA, Caltech, UT Austin, Stanford, ASU Equal contribution Equal advising -1.5em =-1 We introduce , the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains more unique items, travels longer distances, and unlocks key tech tree milestones up to faster than prior SOTA. is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. -0.3em",
    "idea_abstract": "We propose a novel embodied lifelong learning agent that leverages large language models to continuously explore and learn in a dynamic environment, such as Minecraft. The agent's architecture includes an adaptive curriculum, a skill library for storing and retrieving complex behaviors, and an iterative prompting mechanism that incorporates feedback and self-verification for improvement. By interacting with a large language model via blackbox queries, the agent can develop temporally extended, interpretable, and compositional skills that compound its abilities and mitigate catastrophic forgetting. This approach enables the agent to learn and adapt in a flexible and efficient manner, with potential applications in a range of dynamic environments.",
    "proposal": "Here is the rewritten abstract in the format of an idea abstract:\n\n1. **Embodied Lifelong Learning with Large Language Models**\n\n2. **Problem Statement**: The problem of developing an embodied agent that can continuously learn and adapt to new situations without human intervention is a long-standing challenge in artificial intelligence. Current approaches often rely on manual curriculum design, parameter fine-tuning, and limited skill libraries, which hinder the agent's ability to generalize and learn from experience.\n\n3. **Motivation**: Existing methods are limited in their ability to acquire diverse skills, make novel discoveries, and generalize to new environments. The proposed method aims to address these limitations by leveraging large language models to enable an embodied agent to explore, learn, and adapt in a dynamic environment.\n\n4. **Proposed Method**: The proposed method involves an automatic curriculum that maximizes exploration, an ever-growing skill library for storing and retrieving complex behaviors, and an iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. The agent interacts with a large language model via blackbox queries, bypassing the need for model parameter fine-tuning. The skills developed by the agent are temporally extended, interpretable, and compositional, enabling rapid compounding of abilities and alleviating catastrophic forgetting."
  },
  {
    "rank": 14,
    "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
    "arxiv": "2302.04761v1",
    "paper_tar": "tar_files/2302.04761v1.tar.gz",
    "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce , a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a QA system, a search engine, a translation system, and a calendar.  achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",
    "content": "=1 [11pt]article CJKutf8 []emnlp2021 times latexsym [T1]fontenc [utf8]inputenc microtype tabularx booktabs tikz pgfplots amsmath amssymb graphicx multirow subcaption pifont spverbatim compatibility=false 1.0 .9 *argmax *argmin [1]purple[Timo: #1] [1]blue[Nicola: #1] [1]cyan[Luke: #1] [1]orange[Jane: #1] [1]green[Roberto: #1] [1]magenta[Maria: #1] [1]gray[Thomas: #1] [1]0#1*-0 [1]red[TODO: #1] [0]Toolformer [1]red(CITE #1) [0]red??.? 51 55 compat=1.13 c0cmyk1,0.3968,0,0.2588 c1cmyk0,0.6175,0.8848,0.1490 c2cmyk0.1127,0.6690,0,0.4431 c3cmyk0.3081,0,0.7209,0.3255 c4cmyk0.6765,0.2017,0,0.0667 c5cmyk0,0.8765,0.7099,0.3647 c0altRGB15,158,251 darkgreyRGB149,149,149 decentgreyRGB242,242,242 calc,fit,positioning,arrows,arrows.meta,backgrounds,decorations.pathreplacing fillbetween 5.4cm Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick Jane Dwivedi-Yu Roberto Dessi Roberta Raileanu that assigns to starting an API call at position . Given a sampling threshold , we keep all positions ; if there are more than such positions, we only keep the top . For each position , we then obtain up to API calls by sampling from given the sequence as a prefix and as an end-of-sequence token.Mc_ir_iic_ix = x_1, , x_nr_i(w_i i N)Mx_i, , x_nz_i, , x_nM as a prefix instead of inserting it at position because is not yet finetuned on any examples containing API calls, so inserting it in the middle of would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity. the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold , we thus only keep API calls for which holds, i.e., adding the API call and its result reduces the loss by at least , compared to not doing any API call or obtaining no result from it. Model Finetuning After sampling and filtering calls for all APIs, we finally merge the remaining API calls and interleave them with the original inputs. That is, for an input text with a corresponding API call and result at position , we construct the new sequence ; we proceed analogously for texts with multiple API calls. Doing this for all results in the new dataset augmented with API calls. We use this new dataset to finetune , using a standard language modeling objective. Crucially, apart from inserted API calls the augmented dataset contains the exact same texts as , the original dataset. As a consequence, finetuning on exposes it to the same content as finetuning on . Moreover, as API calls are inserted in exactly those positions and with exactly those inputs that help predict future tokens, finetuning on enables the language model to decide when and how to use which tool, based purely on its own feedback. Inference When generating text with after finetuning with our approach, we perform regular decoding until produces the ``'' token, indicating that it next expects the response for an API call. At this point, we interrupt the decoding process, call the appropriate API to get a response, and continue the decoding process after inserting both the response and the token. Tools We explore a variety of tools to address different shortcomings of regular LMs. The only constraints we impose on these tools is that (i) both their inputs and outputs can be represented as text sequences, and (ii) we can obtain a few demonstrations of their intended use. Concretely, we explore the following five tools: a question answering system, a Wikipedia search engine, a calculator, a calendar, and a machine translation system. Some examples of potential calls and return strings for the APIs associated with each of these tools are shown in Table~. We briefly discuss all tools below; further details can be found in Appendix~. Question Answering Our first tool is a question answering system based on another LM that can answer simple factoid questions. Specifically, we use Atlas , a retrieval-augmented LM finetuned on Natural Questions . Calculator As a second tool, we use a calculator that can perform simple numeric calculations; we only support the four basic arithmetic operations. Results are always rounded to two decimal places. Wikipedia Search Our third tool is a search engine that, given a search term, returns short text snippets from Wikipedia. Compared to our question answering tool, this search enables a model to get more comprehensive information on a subject, but requires it to extract the relevant parts by itself. As our search engine, we use a BM25 retriever that indexes the Wikipedia dump from KILT . Machine Translation System Our fourth tool is a machine translation system based on a LM that can translate a phrase from any language into English. More concretely, we use the 600M parameter NLLB~ as our multilingual machine translation model that works for 200 languages (including low-resource ones). The source language is automatically detected using the fastText classifier~, while the target language is always set to English. Calendar Our final tool is a calendar API that, when queried, returns the current date without taking any input. This provides temporal context for predictions that require some awareness of time. Experiments We investigate whether our approach enables a model to use tools without any further supervision and to decide for itself when and how to call which of the available tools. To test this, we select a variety of downstream tasks where we assume at least one of the considered tools to be useful, and evaluate performance in zero-shot settings (Section~). Beyond that, we also ensure that our approach does not hurt the model's core language modeling abilities; we verify this by looking at perplexity on two language modeling datasets (Section~). Finally, we investigate how the ability to learn using tools is affected by model size (Section~). Experimental Setup Dataset Generation Throughout all of our experiments, we use a subset of CCNet as our language modeling dataset and GPT-J as our language model . To reduce the computational cost of annotating with API calls, we define heuristics for some APIs to get a subset of for which API calls are more likely to be helpful than for an average text. For example, we only consider texts for the calculator tool if they contain at least three numbers. Details of the heuristics used are given in Appendix~. For obtaining from , we perform all steps described in Section~ and additionally filter out all examples for which all API calls were eliminated in the filtering step. For the weighting function, we use to make sure that API calls happen close to where the information provided by the API is actually helpful for the model. The thresholds and are chosen individually for each tool to ensure a sufficiently larger number of examples; see Appendix~ for details. Table~ shows relevant statistics of our final dataset augmented with API calls. Model Finetuning We finetune on using a batch size of 128 and a learning rate of with linear warmup for the first 10Baseline Models Throughout the remainder of this section, we mainly compare the following models: GPT-J: A regular GPT-J model without any finetuning. GPT-J + CC: GPT-J finetuned on , our subset of CCNet without any API calls. : GPT-J finetuned on , our subset of CCNet augmented with API calls. (disabled): The same model as , but API calls are disabled during decoding.kk = 1k = 10C^*Cp_M(x_t x_1, , x_t-1)x_tx_1, , x_t-1tkkkL_i^- - L_i^+L_i^- - L_i^+C^*_s = 0.05_f = 1.0C^*+-*/C^*xxxqxqxqxq$. Dateset Dateset is created by first randomly selecting 500 ``current dates''. For each current date, another relatively past/future date is randomly selected within a four-year range, and the two dates are used to fill the query templates in Table~. An example of one such query using the first template would be, ``How many days ago was August 14, 2020?'' If called, the Calendar tool would return the presumed current date (e.g., ``Today is Sunday, November 20, 2020''). 1.5",
    "idea_abstract": "We propose a novel approach to enable language models to leverage external tools and APIs to augment their capabilities, allowing them to solve a wide range of tasks more effectively. By training a model to selectively use APIs, incorporate their results, and adapt to new information, we aim to bridge the gap between language models' ability to learn from few examples and their limitations in basic tasks.",
    "proposal": "1. Title: Learning to Use Tools with Language Models\n\n2. Problem Statement: Language models struggle to use external tools to augment their capabilities, leading to limited performance on tasks that require access to specialized knowledge or functionality. This problem is interesting and important because it hinders the development of more powerful and versatile language models.\n\n3. Motivation: Existing methods for teaching language models to use tools are limited and often require significant human supervision or annotation. Current approaches also fail to enable the model to decide when and how to use tools, leading to inefficient and suboptimal performance. Our proposed method addresses these limitations by allowing the model to learn to use tools without supervision and decide when and how to use them.\n\n4. Proposed Method: We propose a method that enables language models to learn to use tools by inserting API calls into the input sequence and finetuning the model on the resulting augmented dataset. The model is trained to predict the response to API calls and learn when and how to use tools to improve its performance on downstream tasks. We explore a variety of tools, including question answering, search engines, calculators, machine translation systems, and calendars, and demonstrate that our approach enables the model to use tools without supervision and decide when and how to use them."
  },
  {
    "rank": 15,
    "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
    "arxiv": "2301.07597v1",
    "paper_tar": "tar_files/2301.07597v1.tar.gz",
    "abstract": "The introduction of ChatGPTLaunched by OpenAI in November 2022. https://chat.openai.com/chat has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
    "content": "article [preprint,nonatbib]neurips_2022 [utf8]inputenc [T1]fontenc hyperref url booktabs amsfonts nicefrac microtype xcolor tabularx graphicx multirow makecell caption subcaption soul amsmath listings colortbl [normalem]ulem CJKutf8 [misc]ifsym codegreenrgb0,0.6,0 codegrayrgb0.5,0.5,0.5 codepurplergb0.58,0,0.82 backcolourrgb0.95,0.95,0.92 mystyle backgroundcolor=backcolour, commentstyle=codegreen, keywordstyle=magenta, numberstyle=codegray, stringstyle=codepurple, basicstyle=, breakatwhitespace=false, breaklines=true, captionpos=b, keepspaces=true, numbers=left, numbersep=5pt, showspaces=false, showstringspaces=false, showtabs=false, tabsize=2 style=mystyle How Close is ChatGPT to Human Experts? Corpus, Evaluation, and Detection Biyang GuoEqual Contribution.~~, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie Yuxuan Ding, Jianwei Yue, Yupeng Wu AI Lab, School of Information Management and Engineering University of Finance and Economics Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen) School of Information Science, Beijing Language and Culture University School of Electronic Engineering, Xidian University School of Computing, Queen's University, Wind Information Co., Ltd antiquefuchsiargb0.57, 0.36, 0.51 [1] antiquefuchsia[Minqi: #1] Project Lead. Corresponding to guobiyang@163.com Each author has made unique contributions to the project. The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection. Introduction Since its dazzling debut in November 2022, OpenAI's ChatGPT has gained huge attention and wide discussion in the natural language processing (NLP) community and many other fields. According to OpenAI, ChatGPT is fine-tuned from the GPT-3.5 series with Reinforcement Learning from Human Feedback (RLHF; ), using nearly the same methods as InstructGPT , but with slight differences in the data collection setup. The vast amount of knowledge in GPT-3.5 and the meticulous fine-tuning based on human feedback enable ChatGPT to excel at many challenging NLP tasks, such as translating natural language to code , completing the extremely masked text or generating stories given user-defined elements and styles , let alone typical NLP tasks like text classification, entity extraction, translation, etc. Furthermore, the carefully collected human-written demonstrations also make ChatGPT able to admit its mistakes, challenge incorrect premises and reject even inappropriate requests, as claimed by OpenAI. The surprisingly strong capabilities of ChatGPT have raised many interests, as well as concerns: On the one hand, people are curious about how close is ChatGPT to human experts. Different from previous LLMs like GPT-3 , which usually fails to properly respond to human queries, InstructGPT and the stronger ChatGPT have improved greatly in interactions with humans. Therefore, ChatGPT has great potential to become a daily assistant for general or professional consulting purposes . From the linguistic or NLP perspectives, we are also interested in where are the remaining gaps between ChatGPT and humans and what are their implicit linguistic differences . On the other hand, people are worried about the potential risks brought by LLMs like ChatGPT. With the free preview demo of ChatGPT going virus, a large amount of ChatGPT-generated content crowded into all kinds of UGC (User-Generated Content) platforms, threatening the quality and reliability of the platforms. For example, Stack Overflow, the famous programming question-answering website, has temporarily banned ChatGPT-generated content https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned , because it believes \"the average rate of getting correct answers from ChatGPT is too low, the posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers\". Many other applications and activities are facing similar issues, such as online exams and medical analysis . Our empirical evaluation of ChatGPT on legal, medical, and financial questions also reveals that potentially harmful or fake information can be generated. Considering the opaqueness of ChatGPT and the potential social risks associated with model misuse, we make the following contributions to both the academy and society: [1.] To facilitate LLM-related research, especially the study on the comparison between humans and LLMs, we collect nearly 40K questions and their corresponding answers from human experts and ChatGPT, covering a wide range of domains (open-domain, computer science, finance, medicine, law, and psychology), named as the Human ChatGPT Comparison Corpus (HC3) dataset. The HC3 dataset is a valuable resource to analyze the linguistic and stylist characteristics of both humans and ChatGPT, which helps to investigate the future improvement directions for LLMs; [2.] We conduct comprehensive human evaluations as well as linguistic analysis on human/ChatGPT-generated answers, discovering many interesting patterns exhibited by humans and ChatGPT. These findings can help to distinguish whether certain content is generated by LLMs, and also provide insights about where language models should be heading in the future; [3.] Based on the HC3 dataset and the analysis, we develop several ChatGPT detecting models, targeting different detection scenarios. These detectors show decent performance in our held-out test sets. We also conclude several key factors that are essential to the detector's effectiveness. [4.] We open-source all the collected comparison corpus, evaluations, and detection models, to facilitate future academic research and online platform regulations on AI-generated content. Human ChatGPT Comparison Corpus (HC3) ChatGPT is based on the GPT-3.5 series, which is pre-trained on the super-large corpus, consisting of web-crawled text, books, and codes, making it able to respond to all kinds of questions. Therefore, we are curious how will a human (especially an expert) and ChatGPT respond to the same question respectively. Inspired by , we also want to evaluate whether ChatGPT can keep honest (not fabricate information or mislead the user), harmless (shouldn't generate harmful or offensive content), and how helpful (provide concrete and correct solutions to the user's question) it is compared to human experts. Taking these into account, we decided to collect a comparison corpus that consists of both human and ChatGPT answers to the same questions. We believe such a comparison corpus can be a valuable and interesting source to study the nature of the language of both humans and language models. Human Answers Collection Inviting human experts to manually write questions and answers is tedious and unaffordable for us to collect a large amount of data, therefore we construct the comparison dataset mainly from two sources: Publicly available question-answering datasets, where answers are given by experts in specific domains or the high-voted answers by web users; Wiki text. We construct question-answer pairs using the concepts and explanations from wiki sources like Wikipedia and BaiduBaike. The split-data source mapping is shown in Table , and please refer to Appendix for further detailed information. ChatGPT Answers Collection Based on the collected human question-answering datasets, we use ChatGPT to generate answers to these questions. Since the ChatGPT is currently only available through its preview website, we manually input the questions into the input box, and get the answers, with the aid of some automation testing tools. Answers by ChatGPT can be influenced by the chatting history, so we refresh the thread for each question. To make the answer more aligned with human answers, we add additional instructions to ChatGPT for specific datasets. For example, the human answers from the reddit-eli5 dataset split are under the context of \"Explain like I'm five\", therefore we use this context to instruct ChatGPT by adding \"Explain like I'm five\" at the end of the original question. More detail can be found in the Appendix. ChatGPT can generate different answers given the same question in different threads, which is perhaps due to the random sampling in the decoding process. However, we found the differences can be very small, thereby we only collect one answer for most questions. Human ChatGPT Comparison Corpus (HC3) For each question, there can be more than one human/ChatGPT answer, therefore we organize the comparison data using the following format: Overall, we collected questions, human answers and ChatGPT answers for the English version, and questions, human answers and ChatGPT answers for the Chinese version. The meta-information of each dataset split is illustrated in Table . Human Evaluation Summarization In this section, we invite many volunteer testers and conduct extensive human evaluations from different aspects. After the human evaluation, we make our collected comparison corpus available to the volunteers and ask them to manually conclude some characteristics. We then summarize the feedback from the volunteers combined with our observations. Human Evaluation The human evaluation is divided into the Turing test and the Helpfulness Test. The Turing Test is a test of a machine's ability to exhibit intelligent behavior that is indistinguishable from a human. We invite 17 volunteers, divided into two groups: 8 experts (who are frequent users of ChatGPT) and 9 amateurs (who have never heard of ChatGPT). This is because people who are familiar with ChatGPT may have memorized some patterns exhibited by ChatGPT, helping them to easily distinguish the role. We designed four types of evaluations, using different query formats or testing groups. We introduce the specific evaluation design and results in the following parts: Expert Turing Test, Paired Text (pair-expert) The pair-expert test is conducted in the expert group. Each tester is required to do a series of tests, each test containing one question and a pair of answers (one from humans and another from ChatGPT). The tester needs to determine which answer is generated by ChatGPT. Expert Turing Test, Single Text (single-expert) The single-expert test is also conducted in the expert group. Each tester is required to do a series of tests, each test containing one question and a single answer randomly given by humans or ChatGPT. The tester needs to determine whether the answer is generated by ChatGPT. Amateur Turing Test, Single Text (single-amateur) The single-amateur test is conducted in the amateur group. Each tester is required to do a series of tests, each test containing one question and a single answer randomly given by humans or ChatGPT. The tester needs to determine whether the answer is generated by ChatGPT. Helpfulness Test (helpfulness) We are also curious about how helpful are the answers from ChatGPT compared with humans' answers to one question. Note that helpfulness is a very subjective metric, which can be influenced by many factors, including emotion, tester personality, personal preference, etc. Therefore, simply providing more accurate information or a more detailed analysis may not always lead to a more helpful answer. The helpfulness test is conducted in the expert group. Each tester is required to do a series of tests, each containing one question and a pair of answers (one from human and another from ChatGPT). Each tester is asked to pretend that the question is proposed by him/herself, and needs to determine which answer is more helpful to him/her. Settings. We sample around 30 <question, humananswer, chatgptanswer> triplets from each split (i.e., redditeli5, wikipedia, medical, etc.) as the samples for the human evaluation. We allocate 2-5 testers for each split and report their average results. For all Turing tests, we report the proportion that ChatGPT-generated answer is correctly detected by testers. For the helpfulness test, we report the proportion that ChatGPT-generated answer is considered to be more helpful. Results. Several conclusions can be drawn from the results shown in Table . Comparing the results of pair-expert and single-expert, we can find that it is easier to distinguish ChatGPT-generated content when providing a comparison pair than only providing a single answer. Comparing the results of single-expert and single-amateur, we can find that the accuracy of experts is much higher than that of amateurs. The helpfulness test gives the proportion of questions that volunteers think the ChatGPT answer is more helpful to them. Surprisingly, results show that ChatGPT's answers are generally considered to be more helpful than humans' in more than half of questions, especially for finance and psychology areas. By checking the specific answers in these domains, we find that ChatGPT can usually provide more concrete and specific suggestions. However, ChatGPT performs poorly in terms of helpfulness for the medical domain in both English and Chinese. The ChatGPT often gives lengthy answers to medical consulting in our collected dataset, while human experts may directly give straightforward answers or suggestions, which may partly explain why volunteers consider human answers to be more helpful in the medical domain. Human Summarization After the above evaluations, we open our collected HC3 dataset to the volunteers where they can freely browse the comparison answers from humans and ChatGPT. All dataset splits are allocated to different volunteers, and each volunteer is asked to browse at least 100 groups of comparison data. After that, we ask them to summarize the characteristics of both human answers and ChatGPT answers. Eventually, we received more than 200 feedbacks, and we summarize these findings as follows: Distinctive Patterns of ChatGPT [(a)] ChatGPT writes in an organized manner, with clear logic. Without loss of generality, ChatGPT loves to define the core concept in the question. Then it will give out detailed answers step by step and offers a summary at the end, following the deduction and summary structure; [(b)] ChatGPT tends to offer a long and detailed answer. This is the direct product of the Reinforcement Learning with Human Feedback, i.e. RLHF, and also partly related to the pattern (a) unless you offer a prompt such as \"Explain it to me in one sentence\"; [(c)] ChatGPT shows less bias and harmful information. ChatGPT is neutral on sensitive topics, barely showing any attitude towards the realm of politics or discriminatory toxic conversations; [(d)] ChatGPT refuses to answer the question out of its knowledge. For instance, ChatGPT cannot respond to queries that require information after September 2021. Sometimes ChatGPT also refuses to answer what it believes it doesn't know. It is also RLHF's ability to implicitly and automatically determine which information is within the model's knowledge and which is not. [(e)] ChatGPT may fabricate facts. When answering a question that requires professional knowledge from a particular field, ChatGPT may fabricate facts in order to give an answer, though mentions that InstructGPT model has already shown improvements in truthfulness over GPT-3. For example, in legal questions, ChatGPT may invent some non-existent legal provisions to answer the question. This phenomenon warns us to be extra careful when using ChatGPT for professional consultations. Additionally, when a user poses a question that has no existing answer, ChatGPT may also fabricate facts in order to provide a response. Many of the conclusions mentioned above like (b),(c),(d) are also discussed in by Fu et al. Major Differences between Human and ChatGPT [(a)] ChatGPT's responses are generally strictly focused on the given question, whereas humans' are divergent and easily shift to other topics. In terms of the richness of content, humans are more divergent in different aspects, while ChatGPT prefers focusing on the question itself. Humans can answer the hidden meaning under the question based on their own common sense and knowledge, but the ChatGPT relies on the literal words of the question at hand; [(b)] ChatGPT provides objective answers, while humans prefer subjective expressions. Generally, ChatGPT generates safer, more balanced, neutral, and informative texts compared to humans. As a result, ChatGPT is excellent at interpreting terminology and concepts. On the other hand, human answers are more specific and include detailed citations from sources based on legal provisions, books, and papers, especially when providing suggestions for medical, legal, and technical problems, etc.; [(c)] ChatGPT's answers are typically formal, meanwhile humans' are more colloquial. Humans tend to be more succinct with full of oral abbreviations and slang such as \"LOL\", \"TL;DR\", \"GOAT\" etc. Humans also love to apply humor, irony, metaphors, and examples, whereas ChatGPT never uses antiphrasis. Additionally, human communication often includes the \"Internet meme\" as a way to express themselves in a specific and vivid way; [(d)] ChatGPT expresses less emotion in its responses, while human chooses many punctuation and grammar feature in context to convey their feelings. Human uses multiple exclamation mark('!'), question mark('?'), ellipsis('...') to express their strong emotion, and use various brackets('(', ')', '[', ']') to explain things. By contrast, ChatGPT likes to use conjunctions and adverbs to convey a logical flow of thought, such as \"In general\", \"on the other hand\", \"Firstly,..., Secondly,..., Finally\" and so on. Overall, these summarised features indicate that ChatGPT has improved notably in question-answering tasks for a wide range of domains. Compared with humans, we can imagine ChatGPT as a conservative team of experts. As a \"team\", it may lack individuality but can have a more comprehensive and neutral view towards questions. Linguistic Analysis In this section, we analyze the linguistic features of both humans' and ChatGPT's answers, and try to find some statistical evidence for the characteristics concluded in Section . Vocabulary Features In this part, we analyze the vocabulary features of our collected corpus. We are interested in how humans and ChatGPT differ in the choice of words when answering the same set of questions. Since the number of human/ChatGPT answers is unbalanced, we randomly sample one answer from humans and one answer from ChatGPT during our statistical process. We calculated the following features: average length (), which is the average number of words in each question; vocab size (), the number of unique words used in all answers; we also propose another feature called density (), which is calculated by where is the number of answers. Density measures how crowded different words are used in the text. For example, if we write some articles that add up to 1000 words, but only 100 different words are used, then the density is . The higher the density is, the more different words are used in the same length of text. In Table , we report the vocabulary features for both English and Chinese corpus. Looking at both features of average length and vocab size, we can see that: compared to ChatGPT, human answers are relatively shorter, but a larger vocabulary is used. This phenomenon is particularly obvious in the Chinese openqa split and the medical splits in both languages, where the average length of ChatGPT is nearly twice longer than that of humans, but the vocab size is significantly smaller. This phenomenon is also reflected by the density factor. The word density of humans is greater than ChatGPT's in every split, which further reveals that humans use a more diverse vocabulary in their expressions. Part-of-Speech Dependency Analysis In this part, we compare the occurrences of different part-of-speech (POS) tags and the characteristics of the dependency relations. Part-of-Speech Figure illustrates the comparisons between humans and ChatGPT in terms of POS usage. In HC3-English, ChatGPT uses more NOUN, VERB, DET, ADJ, AUX, CCONJ and PART words, while using less ADV and PUNCT words. A high proportion of nouns (NOUN) often indicates that the text is more argumentative, exhibiting informativeness and objectivity . Accordingly, adposition (ADP) and adjective (ADJ) words also tend to appear more frequently . The frequent co-occurrence of conjunctions (CCONJ) along with nouns, verbs, and adposition words indicates that the structure of the article and the relationships of cause-and-effect, progression, or contrast are clear. The above are also typical characteristics in academic papers or official documents . We believe the RLHF training process has a great influence on ChatGPT's writing style, which partly explains the difference in the POS tags distribution. Dependency Parsing Dependency parsing is a technique that analyzes the grammatical structure of a sentence by identifying the dependencies between its words. We parse the answers in the corpus and compare the proportion of different dependency relations and their corresponding dependency distances. Figure shows the comparison between humans and ChatGPT in HC3-English. Due to the limited space, the Chinese version is placed in the Appendix . The comparison of dependency relations exhibits similar characteristics to that of POS tags, where ChatGPT uses more determination, conjunction, and auxiliary relations. In terms of the dependency distance, ChatGPT has much longer distances for the punct and dep relations, which is perhaps due to the fact that CharGPT tends to use longer sentences. However, ChatGPT has obviously shorter conj relations. According to the analysis of POS tags, ChatGPT usually uses more conjunctions than humans to make the content more logical, this may explain why the conj relations of ChatGPT are relatively shorter than humans. Sentiment Analysis Humans are emotional beings, it is natural that our emotions are reflected in our words, to some extent. ChatGPT is learned on large-scale human-generated text, but it is further fine-tuned with human instructions. Therefore we are curious how \"emotional\" ChatGPT is compared with humans. We use a multilingual sentiment classification model fine-tuned on Twitter corpus to conduct sentiment analysis for both English and Chinese comparison data. Note that deep learning-based models can be greatly influenced by some indicating words (such as \"but\" and \"sorry\" can easily fool the classifier to predict the \"negative\" label), making the predictions biased . Therefore, the sentiment given by the classifier is only a reference to the true sentiment behind the text. Figure shows the comparison of the sentiment distribution of humans and ChatGPT. Several findings can be drawn from the results: First, we find that the proportion of neutral emotions is the largest for both humans and ChatGPT, which is in line with our expectations. However, ChatGPT generally expresses more neutral sentiments than humans. Then, the proportion of negative emotions is significantly higher than that of positive emotions. Notably, humans express significantly more negative emotions than ChatGPT. The proportion of humans' positive emotions is also slightly higher than that of ChatGPT. Overall, ChatGPT is less emotional than humans, though it is not completely emotionless. Language Model Perplexity The perplexity (PPL) is commonly used as a metric for evaluating the performance of language models (LM). It is defined as the exponential of the negative average log-likelihood of the text under the LM. A lower PPL indicates that the language model is more confident in its predictions, and is therefore considered to be a better model. The training of LMs is carried out on large-scale text corpora, it can be considered that it has learned some common language patterns and text structures. Therefore, we can use PPL to measure how well a text conforms to common characteristics. We use the open-source GPT-2 small (Wenzhong-GPT2-110M for Chinese) model to compute the PPL (both text-level and sentence-level For English text, we used NLTK for sentence segmentation (HarvestText for Chinese). PPLs) of the collected texts. The PPL distributions of text written by humans and text generated by ChatGPT are shown in Figure . It is clearly observed that, regardless of whether it is at the text level or the sentence level, the content generated by ChatGPT has relatively lower PPLs compared to the text written by humans. ChatGPT captured common patterns and structures in the text it was trained on, and is very good at reproducing them. As a result, text generated by ChatGPT have relatively concentrated low PPLs. Humans have the ability to express themselves in a wide variety of ways, depending on the context, audience, and purpose of the text they are writing. This can include using creative or imaginative elements, such as metaphors, similes, and unique word choices, which can make it more difficult for GPT2 to predict. Therefore, human-written texts have more high-PPL values, and show a long-tailed distribution, as demonstrated in Figure . ChatGPT Content Detection AI-generated content (AIGC) is becoming increasingly prevalent on the internet, and it can be difficult to distinguish it from human-generated content, as shown in our human evaluation (sec ). Therefore, AIGC detectors are needed to help identify and flag content that has been created by a machine, to reduce the potential risks to society caused by improper or malicious use of AI models, and to improve the transparency and accountability of the information that is shared online. In this section, we conduct several empirical experiments to investigate the ChatGPT content detection systems. Detecting AI-generated content is a widely studied topic . Based on these , we establish three different types of detection systems, including machine learning-based and deep learning-based methods, and evaluate them on different granularities and data sources. Detailed results and discussions are provided. Methods Detection of machine-generated text has been gaining popularity as text generation models have advanced in recent years. Here, we implement three representative methods from classic machine learning and deep learning, i.e, a logistic regression model trained on the GLTR Test-2 features, a deep classifier for single-text detection and a deep classifier for QA detection. The deep classifiers for both single-text and QA are based on RoBERTa , a strong pre-trained Transformer model. In fact, algorithms for OOD detection or anomaly detection can also be applied to develop ChatGPT content detectors, which we leave for future work. GLTR. studied three tests to compute features of an input text. Their major assumption is that to generate fluent and natural-looking text, most decoding strategies sample high probabilities tokens from the head of the distribution. We select the most powerful Test-2 feature, which is the number of tokens in the Top-10, Top-100, Top-1000, and 1000+ ranks from the LM predicted probability distributions. And then a logistic regression model is trained to finish the classification. RoBERTa-sinlge. A deep classifier based on the pre-trained LM is always a good choice for this kind of text classification problem. It is also investigated in many studies and demo systems . Here we fine-tune the RoBERTa model. RoBERTa-QA. While most content detectors are developed to classify whether a single piece of text is AI-generated, we claim that a detector that supports inputting both a question and an answer can be quite useful, especially for question-answering scenarios. Therefore, we decide to also build a QA version detector. The RoBERTa model supports a text pair input format, where a separating token is used to join a question and its corresponding answer. Implementation Details For the LM used by GLTR, we use gpt2-small for English, and Wenzhong-GPT2-110M released by for Chinese, it is the same with sec. . For RoBERTa-based deep classifiers, we use roberta-base and hfl/chinese-roberta-wwm-ext checkpoints for English and Chinese, respectively. All the above models are obtained from huggingface transformers . We train the logistic regression model by sklearn on the GLTR Test-2 features from trainset, and search hyper-params following the code of . The RoBERTa-based detectors are trained by the facilities of transformers. Specifically, we use the AdamW optimizer, setting batch size to 32 and learning rate to . We finetune models by 1 epoch for English, and 2 epochs for Chinese. Experiment Design The HC3 dataset consists of questions and their corresponding human/ChatGPT answers. We extracted all the <question, answer> pairs, and assigned label 0 to pairs with human answers and label 1 to pairs with ChatGPT answers. Simply using the original answers from humans and ChatGPT to train a binary classifier is the most straightforward way. However, there might be some issues by doing so: First, based on the observations in Section , both human answers and ChatGPT answers may contain some obvious indicating words that may influence the effectiveness of models; Second, users may want to detect whether a single sentence is generated by ChatGPT, instead of the full text. This can be quite difficult for a classifier that is only trained on full texts; Third, taking the corresponding question of the answer into account may help the detector to make a more accurate judgment, compared with only considering the answer itself. This can be widely applied to many QA platforms (like Quora, Stack Overflow, and Zhihu) to find out which answer below a certain question is generated by AI. Therefore, we design different groups of experiments to study these key questions: How will the indicating words influence the detector? Is it more challenging for the ChatGPT detectors to detect sentence-level content? Is it harder to train a sentence-level classifier? Can the corresponding question help detectors detect the origin of the answer more accurately? Figure shows how we generate different types of training and testing sets. Specifically, we use the collected raw corpus to construct the first train-test sets (the \"full text (raw)\" in the figure), which we call the raw-full version. Then we filter away the indicated words in the text to obtain the filtered-full version. By splitting the full text into sentences, we obtain the raw-sent version and the filtered-sent version. We also combine the full text and the sentences into a mixed version, namely the raw-mix and filtered-mix version. Overall, we have six different versions of training and testing sets. Evaluating a model's performance on version B's testing set which is trained on version A's training set can be seen as an out-of-distribution (OOD) generalization evaluation, which is more challenging since it requires the model to be robust when facing sample style changes. Results Following the above experiment design, we conduct comprehensive empirical studies on all kinds of derived corpus. Table shows the test F1 scores. Which detector(s) is more useful? ML-based or DL-based? and Why? According to Table , we can derive following conclusions: Firstly, the robustness of RoBERTa-based-detector is better than GLTR. The F1-scores of RoBERTa decrease slightly (1.5-2Secondly, RoBERTa-based-detector is not affected by indicating words. The F1-scores of RoBERTa only slightly decreased by 0.03Lastly, RoBERTa-based-detector is effective in handling Out-Of-Distribution scenarios. When compared to the original model, it demonstrates a significant decrease in performance on GLTR's OOD test datasets, with a drop of up to 28.8How will the indicating words influence the detector? We first collected a bunch of indicating words for both humans and ChatGPT. For example, ChatGPT's indicating words (or phrases) include \"AI assistant\", \"I'm sorry to hear that\", and \"There're a few steps...\", etc. and humans' indicating words may include \"Hmm\", \"Nope\", \"My view is\", etc. In the filtered version, we remove all sentences in the answers that contain the indicating words for both humans and ChatGPT. According to Table , removing the indicating words helps the models trained on full-text to perform better across different content granularities. For example, the RoBERTa-filter-full performs significantly better than RoBERTa-raw-full in terms of sentence-level and mix-level evaluations, improving more than 3However, the filtering may slightly hurt the performances of the models trained on sentences. This may be because the indicating words play a bigger part in the sentence-level text compared with the full text. Removing the indicating words may make some sentences literally unable to be distinguished. Which granularity is more difficult to detect? Full-text or sentence? Through the extensive experimental results in Table~, we conclude that detecting ChatGPT generated texts is more difficult in a single sentence than in a full text. This conclusion can be proved by the following two points: First, our results show that both English and Chinese sentence-based detectors (i.e., raw-sent and filtered-sent versions) achieve satisfactory results w.r.t. the testing task of detecting either ChatGPT generated paragraphs or sentences, whereas the opposite is not true——raw-full and filtered-full are relatively inferior when detecting ChatGPT generated sentences. In other words, detectors trained on \"hard samples\" (i.e., sentence corpus) are much easier to solve simple task (i.e., detecting full corpus), while \"simple samples\" (i.e., full corpus) may be less useful for solving more difficult task (i.e., sentence corpus). Second, we observe that although both full and sentence corpus are provided in the raw-mix and filtered-mix versions, it is still more difficult for them to detect single sentences generated by ChatGPT. This is even more obvious for the Chinese corpus, where the F1-score of raw-mix trained on the Chinese corpus is 94.09One possible explanation is that the expression pattern of ChatGPT is more obvious (therefore more easily detected) when paragraphs of text are provided, whereas it is more difficult to detect generated single sentences. Which corpus is more helpful for model training? Full-text, sentence, or mix of the two? We find that both English and Chinese RoBERTa-based detectors are more robust when fine-grained corpus data is available in model training. The sentence-based detectors outperform full-based detectors w.r.t. F1-scores, while the latter can be significantly improved when the sentence corpus is injected in model training, as we observe that mix-based detectors also achieve satisfactory results. For English corpus, raw-full only achieves 81.89We indicate that the above conclusions could also hold for other types of detectors like GLTR Test-2 feature-based detectors, as is shown in Table~. For GLTR Test-2, the average performance of F1-score of raw-full and filtered-full is 61.74Taking into account the conclusions of the previous paragraph about the detection difficulty between full and sentence answers, we indicate that the fine-grained corpus is helpful for distinguishing ChatGPT generated texts, as it additionally provides guidance and hints in model training for detecting the subtle patterns of ChatGPT hidden in single sentences. Will a QA-style detector be more effective than a single-text detector? Table demonstrates the results of both raw-full and filtered-full models across all test datasets. On English datasets, the QA model's F1-scores are superior to that of the single model, except for two full test datasets, where it averages 97.48In conclusion, the QA model is generally more effective than the single model and is suitable for filtered scenarios. And the QA training makes models more robust to the sentence inputs. Which data sources are more difficult for the ChatGPT detectors? and What are the conditions that make it easier to detect ChatGPT? As shown in Table , the evaluation results based on filtered-full model are separated by various sources in our HC3 dataset. On the English datasets, the F1-scores for human answers are slightly higher than those for ChatGPT without any exceptions, regardless of whether RoBERTa or GLTR is used on full-text test datasets. However, the F1-scores for ChatGPT are highly inconsistent on transferring test datasets particularly open-qa dataset with varying performance. In terms of data resource, reddit-eli5 and finance-en has higher values, while wiki-csai poses a challenge for detectors. On the Chinese datasets, the F1-scores of humans and ChatGPT are comparable with no significant difference. This suggests that the difficulty in detecting ChatGPT depends on the data source. It is observed that open-qa and baike have better performance, whereas the nlpcc-dbqa has lower performance. Above all, the evaluations on Chinese dataset show more stability on transferring test dataset compared to the English datasets. Furthermore, it's evident that the F1-scores of ChatGPT are lower than those of human answers, regardless of whether the dataset is English or Chinese. This indicates that ChatGPT's detector relies more heavily on In-Distribution models. Conclusion In this work, we propose the HC3 (Human ChatGPT Comparison Corpus) dataset, which consists of nearly 40K questions and their corresponding human/ChatGPT answers. Based on the HC3 dataset, we conduct extensive studies including human evaluations, linguistic analysis, and content detection experiments. The human evaluations and linguistics analysis provide us insights into the implicit differences between humans and ChatGPT, which motivate our thoughts on LLMs' future directions. The ChatGPT content detection experiments illustrate some important conclusions that can provide beneficial guides to the research and development of AIGC-detection tools. We make all our data, code, and models publicly available to facilitate related research and applications at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection. Limitations Despite our comprehensive analysis of ChatGPT, there are still several limitations in the current paper, which will be considered for improvement in our future work: [1.] Despite our efforts in data collection, the amount and range of collected data are still not enough and the data from different sources are unbalanced, due to limited time and resources. To make more accurate linguistic analyses and content detection, more data with different styles, sources, and languages are needed; [2.] Currently, all the collected ChatGPT answers are generated without special prompts. Therefore, the analysis and conclusions in this paper are built upon ChatGPT's most general style/state. For example, using special prompts such as \"Pretending you are Shakespeare...\" can generate content that bypasses our detectors or make the conclusions in this paper untenable; [3.] ChatGPT (perhaps) is mainly trained on English corpus while less on Chinese. Therefore, the conclusions drawn from the HC3-Chinese dataset may not always be precise. Acknowledgments We would like to thank the volunteers that participated in our human evaluations, many of them are our good friends and dear family members. We would like to thank Junhui Zhu (BLCU-ICALL) for the valuable discussions on linguistic analysis. Biyang Guo would like to thank Prof. Hailiang Huang and Prof. Songqiao Han (AI Lab, SUFE) for providing insightful feedback on the topics and directions for this project. Xin Zhang would like to thank Yu Zhao (NeXt, NUS and CIC, TJU) for sharing the OpenAI account. Finally, we thank all team members of this project for their unique contributions. We together make this possible. Appendix HC3 Dataset Splits Creation We create 5 and 7 splits for HC3 English and Chinese, respectively. Most of the data come from the publicly available Question-Answering (QA) datasets, where details are listed in the following. For these QA data, we directly input the questions to ChatGPT and collect at least one answer. We also crawled some wiki concepts and explanations from Wikipedia and BaiduBaike, where explanations are treated as human expert answers and concepts are used to construct the questions, details ref to bellow paragraphs. For HC3-English, we create five dataset splits: [1.] redditeli5. Sampled from the ELI5 dataset . [2.] openqa. Sampled from the WikiQA dataset . [3.] wikicsai. We collected the descriptions of hundreds of computer science-related concepts from Wikipedia as the human experts' answers to questions like \"Please explain what is <concept>?\" [4.] medicine. Sampled from the Medical Dialog dataset . [5.] finance. Sampled from the FiQA dataset , which is built by crawling StackExchange posts under the Investment topic. For HC3-Chinese, we create seven dataset splits: [1.] openqa. Sampled from the WebTextQA and BaikeQA corpus in . [2.] baike. We collected the descriptions of more than a thousand information science-related concepts from BaiduBaike as the human experts' answers to questions like \"UTF8gbsn我有一个计算机相关的问题，请用中文回答，什么是<concept>\" [3.] nlpccdbqa. Sampled from the NLPCC-DBQA dataset . [4.] medicine. Sampled from the Medical Dialog dataset . [5.] finance. Sampled from the FinanceZhidao dataset . [6.] psychology Sampled from a public Chinese Psychological Question Answering Dataset. [7.] law. Sampled from the LegalQA dataset. Additional Results Here we demonstrate the additional results of dependency relations for the Chinese corpus, as is shown in Figure~. The conclusion is basically consistent with the main paper. Other detailed results, including vocabulary features, sentiment analyses, and dependency parsing results for each data source are all available at our project GitHub repository at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection. Human Evaluations Examples For evaluation examples of our human evaluations, please visit our project GitHub repository at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
    "idea_abstract": "We investigate the strengths and limitations of ChatGPT, a state-of-the-art large language model, by comparing its responses to those of human experts across various domains. We collect a dataset of human and ChatGPT responses to a range of questions and analyze the characteristics of ChatGPT's output. We also explore the potential risks and challenges associated with large language models like ChatGPT, including the detection of generated content.",
    "proposal": "1. Title: Investigating the Gap between Human Experts and ChatGPT: A Comparative Study\n\n2. Problem Statement: The rapid advancement of large language models (LLMs) like ChatGPT has raised concerns about their potential impact on society, including the spread of misinformation, plagiarism, and social security issues. To better understand the capabilities and limitations of LLMs, it is essential to compare their performance with that of human experts in various domains.\n\n3. Motivation: Existing methods for evaluating LLMs are limited, and there is a need for a comprehensive comparison between human experts and LLMs. The proposed study aims to address this gap by collecting a large-scale dataset of human and ChatGPT responses to the same questions, allowing for a thorough analysis of their differences and similarities. The study also aims to develop effective detection systems for identifying ChatGPT-generated content.\n\n4. Proposed Method: The proposed method involves collecting a large-scale dataset of human and ChatGPT responses to the same questions, covering various domains such as open-domain, finance, medicine, law, and psychology. The dataset, called the Human ChatGPT Comparison Corpus (HC3), will be used to conduct human evaluations, linguistic analysis, and content detection experiments. The study will also develop"
  },
  {
    "rank": 16,
    "title": "Extracting Training Data from Diffusion Models",
    "arxiv": "2301.13188v1",
    "paper_tar": "tar_files/2301.13188v1.tar.gz",
    "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
    "content": "[twocolumn,10pt]article usenix fullpage graphicx [utf8]inputenc subcaption xcolor amsmath amsfonts hyphensurlhyperref cleveref enumitem xspace overpic array booktabs multirow *argmax *argmin E x_gen Gen [1] #1~ [1]bluejamie: #1 [1]orangemcj: #1 [1]brownvikash: #1 [1]greenborja: #1 [1]purpledei: #1 [1]purpleEric: #1 [1]cyanmilad: #1 [1]redFT: #1 [1]redTODO: #1 [1]blue [1]orange [1]brown [1]green [1]purple [1]purple [1]cyan [1]red [1]red Extracting Training Data from Diffusion Models Nicholas Carlini Jamie Hayes Milad Nasr Matthew Jagielski Vikash Sehwag Florian Tramèr Borja Balle Daphne Ippolito Eric Wallace Google DeepMind ETHZ Princeton UC Berkeley Equal contribution Equal contribution Equal contribution letltxmacro pgffor [1]#1, [2][] #2 [#1]#2 * Figures included were in Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training. Introduction Denoising diffusion models are an emerging class of generative neural networks that produce images from a training distribution via an iterative denoising process~. Compared to prior approaches such as GANs~ or VAEs~, diffusion models produce higher-quality samples~ and are easier to scale~ and control~. Consequently, they have rapidly become the de-facto method for generating high-resolution images, and large-scale models such as DALL-E 2~ have attracted significant public interest. The appeal of generative diffusion models is rooted in their ability to synthesize novel images that are ostensibly unlike anything in the training set. Indeed, past large-scale training efforts ``do not find overfitting to be an issue'', and researchers in privacy-sensitive domains have even suggested that diffusion models could ``protect[] the privacy [...] of real images''~ by generating synthetic examples~. This line of work relies on the assumption that diffusion models do not memorize and regenerate their training data. If they did, it would violate all privacy guarantees and raise numerous questions regarding model generalization and ``digital forgery'' . In this work, we demonstrate that state-of-the-art diffusion models do memorize and regenerate individual training examples. To begin, we propose and implement new definitions for ``memorization'' in image models. We then devise a two-stage data extraction attack that generates images using standard approaches, and flags those that exceed certain membership inference scoring criteria. Applying this method to Stable Diffusion~ and Imagen~, we extract over a hundred near-identical replicas of training images that range from personally identifiable photos to trademarked logos (e.g., fig:teaser). To better understand how and why memorization occurs, we train hundreds of diffusion models on CIFAR-10 to analyze the impact of model accuracy, hyperparameters, augmentation, and deduplication on privacy. Diffusion models are the least private form of image models that we evaluate---for example, they leak more than twice as much training data as GANs. Unfortunately, we also find that existing privacy-enhancing techniques do not provide an acceptable privacy-utility tradeoff. Overall, our paper highlights the tension between increasingly powerful generative models and data privacy, and raises questions on how diffusion models work and how they should be responsibly deployed. AFAIK there seems to be no concensus on what's the best way to measure overfitting in diffusion models. I couldn't find a reference but maybe that's a point that could be raised here. Contributions I have incorporated these into the main contents of the Introduction Our main contributions can be summarized as follows. We demonstrate that state-of-the-art image diffusion models memorize their some of their training data, and that it is possible to trigger regeneration of training data examples with appropriate prompting. In particular, we illustrate this phenomenon on TODO. For models in the Stable Difussion family where the training data is publicly available, we study the relation between example multiplicity in the training dataset and success of different reconstruction schemes. We find that examples with high multiplicity are significantly easier to extract, and that often such extraction is robust to variations in the prompting strategy (e.g.it is not necessary to use the same exact prompt the model saw during training to trigger a successful reconstruction). We perform a systematic evaluation of CIFAR-10 models to elucidate some of the factors that drive memorization in diffusion models. We show these models exhibit the same type of vulnerability as larger SOTA models, and investigate which examples are more susceptible to memorization by performing sampling and inpainting attacks. Further, by extending membership inference attacks to diffusion models on a carefully selected loss we show that memorization increases dramatically once the model is trained for a sufficient number of steps Background Diffusion models. Generative image models have a long history (see~). Generative Adversarial Networks (GANs)~ were the breakthrough that first enabled the generation of high-fidelity images at scale~. But over the last two years, diffusion models~ have largely displaced GANs: they achieve state-of-the-art results on academic benchmarks~ and form the basis of all recently popularized image generators such as Stable Diffusion~, DALL-E 2~, Runway~, Midjourney and Imagen~. Denoising Diffusion Probabilistic Models~ Our description of diffusion models below omits a number of significant details. However, these details are orthogonal to the results of our attacks and we omit them for simplicity. are conceptually simple: they are nothing more than image denoisers. During training, given a clean image , we sample a time-step and a Gaussian noise vector , to produce a noised image , for some decaying parameter where and . A diffusion model removes the noise to recover the original image by predicting the noise that was added by stochastically minimizing the objective , where Despite being trained with this simple denoising objective, diffusion models can generate high-quality images by first sampling a random vector and then applying the diffusion model to remove the noise from this random ``image''. To make the denoising process easier, we do not remove all of the noise at once---we instead iteratively apply the model to slowly remove noise. Formally, the final image is obtained from by iterating the rule for a noise schedule (dependent on ) with . This process relies on the fact that the model was trained to denoise images with varying degrees of noise. Overall, running this iterative generation process (which we will denote by ) with large-scale diffusion models produces results that resemble natural images. Some diffusion models are further conditioned to generate a particular type of image. Class-conditional diffusion models take as input a class-label (e.g., ``dog'' or ``cat'') alongside the noised image to produce a particular class of image. Text-conditioned models take this one step further and take as input the text embedding of some prompt (e.g., ``a photograph of a horse on the moon'') using a pre-trained language encoder (e.g., CLIP~). Training data privacy attacks. Neural networks often leak details of their training datasets. Membership inference attacks answer the question ``was this example in the training set?'' and present a mild privacy breach. Neural networks are also vulnerable to more powerful attacks such as inversion attacks~ that extract representative examples from a target class, attribute inference attacks~ that reconstruct subsets of attributes of training examples, and extraction attacks that completely recover training examples. In this paper, we focus on each of these three attacks when applied to diffusion models. Concurrent work explores the privacy of diffusion models. Wu et al. and Hu et al. perform membership inference attacks on diffusion models; our results use more sophisticated attack methods and study stronger privacy risks such as data extraction. Somepalli et al. show several cases where (non-adversarially) sampling from a diffusion model can produce memorized training examples. However, they focus mainly on comparing the semantic similarity of generated images to the training set, i.e., ``style copying''. In contrast, we focus on worst-case privacy under a much more restrictive notion of memorization, and perform our attacks on a wider range of models. Motivation and Threat Model There are two distinct motivations for understanding how diffusion models memorize and regenerate training data. Understanding privacy risks. Diffusion models that regenerate data scraped from the Internet can pose similar privacy and copyright risks as language models~. For example, memorizing and regenerating copyrighted text~ and source code~ has been pointed to as indicators of potential copyright infringement~. Similarly, copying images from professional artists has been called ``digital forgery''~ and has spurred debate in the art community. Future diffusion models might also be trained on more sensitive private data. Indeed, GANs have already been applied to medical imagery , which underlines the importance of understanding the risks of generative models before we apply them to private domains. Worse, a growing literature suggests that diffusion models could create synthetic training data to ``protect the privacy and usage rights of real images''~, and production tools already claim to use diffusion models to protect data privacy~. Our work shows diffusion models may be unfit for this purpose. Understanding generalization. Beyond data privacy, understanding how and why diffusion models memorize training data may help us understand their generalization capabilities. For instance, a common question for large-scale generative models is whether their impressive results arise from truly novel generations, or are instead the result of direct copying and remixing of their training data. By studying memorization, we can provide a concrete empirical characterization of the rates at which generative models perform such data copying. In their diffusion model, Saharia et al. ``do not find over-fitting to be an issue, and believe further training might improve overall performance`` , and yet we will show that this model memorizes individual examples. It may thus be necessary to broaden our definitions of overfitting to include memorization and related privacy metrics. Our results also suggest that Feldman's theory that memorization is necessary for generalization in classifiers~ may extend to generative models, raising the question of whether the improved performance of diffusion models compared to prior approaches is precisely because diffusion models memorize more. Threat Model Our threat model considers an adversary that interacts with a diffusion model (backed by a neural network ) to extract images from the model's training set . Image-generation systems. Unconditional diffusion models are trained on a dataset . When queried, the system outputs a generated image using a fresh random noise as input. Conditional models are trained on annotated images (e.g., labeled or captioned) and when queried with a prompt , the system outputs using the prompt and noise . Adversary capabilities. We consider two adversaries: A black-box adversary can query to generate images. If is a conditional generator, the adversary can provide arbitrary prompts . The adversary cannot control the system's internal randomness . A white-box adversary gets full access to the system and its internal diffusion model . They can control the model's randomness and can thus use the model to denoise arbitrary input images. In both cases, we assume that an adversary who attacks a conditional image generator knows the captions for some images in the training set---thus allowing us to study the worst-case privacy risk in diffusion models. Adversary goals. We consider three broad types of adversarial goals, from strongest to weakest attacks: [itemsep=2pt] Data extraction: The adversary aims to recover an image from the training set . The attack is successful if the adversary extracts an image that is almost identical (see ssec:memorization) to some . Data reconstruction: The adversary has partial knowledge of a training image (e.g., a subset of the image) and aims to recover the full image. This is an image-analog of an attribute inference attack~, which aims to recover unknown features from partial knowledge of an input. Membership inference: Given an image , the adversary aims to infer whether is in the training set. Ethics and Broader Impact Training data extraction attacks can present a threat to user privacy. We take numerous steps to mitigate any possible harms from our paper. First, we study models that are trained on publicly-available images (e.g., LAION and CIFAR-10) and therefore do not expose any data that was not already available online. Nevertheless, data that is available online may not have been intended to be available online. LAION, for example, contains unintentionally released medical images of several patients~. We also therefore ensure that all images shown in our paper are of public figures (e.g., politicians, musicians, actors, or authors) who knowingly chose to place their images online. As a result, inserting these images in our paper is unlikely to cause any unintended privacy violation. For example, Figure~ comes from Ann Graham Lotz's Wikipedia profile picture and is licensed under Creative Commons, which allows us to ``redistribute the material in any medium'' and ``remix, transform, and build upon the material for any purpose, even commercially''. Third, we shared an advance copy of this paper with the authors of each of the large-scale diffusion models that we study. This gave the authors and their corresponding organizations the ability to consider possible safeguards and software changes ahead of time. In total, we believe that publishing our paper and publicly disclosing these privacy vulnerabilities is both ethical and responsible. Indeed, at the moment, no one appears to be immediately harmed by the (lack of) privacy of diffusion models; our goal with this work is thus to make sure to preempt these harms and encourage responsible training of diffusion models in the future. Extracting Training Data from State-of-the-art Diffusion Models We begin our paper by extracting training images from large, pre-trained, high-resolution diffusion models. Defining Image Memorization Most existing literature on training data extraction focuses on text language models, where a sequence is said to be ``extracted'' and ``memorized'' if an adversary can prompt the model to recover a verbatim sequence from the training set~. Because we work with high-resolution images, verbatim definitions of memorization are not suitable. Instead, we define a notion of approximate memorization based on image similarity metrics. definitionDefinition [-Diffusion Extraction] [adapted from~]. We say that an example is extractable from a diffusion model if there exists an efficient algorithm (that does not receive as input) such that has the property that . Here, is a distance function and is a threshold that determines whether we count two images as being identical. In this paper, unless otherwise noted we follow Balle et al.~ and use the Euclidean 2-norm distance where is the dimension of the inputs to normalize . Given this definition of extractability, we can now define memorization. [-Eidetic Memorization] [adapted from~]. We say that an example is -Eidetic memorized by a diffusion model if is extractable from the diffusion model, and there are at most training examples where Again, is a distance function and is its corresponding threshold. The constant quantifies the number of near-duplicates of in the dataset. If is a small fraction of the data, then memorization is likely problematic. When is a larger fraction of data, memorization might be expected---but it could still be problematic, e.g., if the duplicated data is copyrighted. [Targeted Attack] Let by a training image for a model , and be a randomized reconstruction method taking as input and some side-knowledge about (e.g.list of all the prompts associated with during training). A targeted attack succeeds if for any we have . Restrictions of our definition. Our definition of extraction is intentionally conservative as compared to what privacy concerns one might ultimately have. For example, if we prompt Stable Diffusion to generate ``A Photograph of Barack Obama,'' it produces an entirely recognizable photograph of Barack Obama but not an near-identical reconstruction of any particular training image. Figure~ compares the generated image (left) to the 4 nearest training images under the Euclidean 2-norm (right). Under our memorization definition, this image would not count as memorized. Nevertheless, the model's ability to generate (new) recognizable pictures of certain individuals could still cause privacy harms. Extracting Data from Stable Diffusion We now extract training data from Stable Diffusion: the largest and most popular open-source diffusion model ~. This model is an 890 million parameter text-conditioned diffusion model trained on 160 million images. We generate from the model using the default PLMS sampling scheme at a resolution of pixels. As the model is trained on publicly-available images, we can easily verify our attack's success and also mitigate potential harms from exposing the extracted data. We begin with a black-box attack. There are multiple releases of Stable Diffusion; we focus our effort on the initial release 1.4; concurrent with our work Stable Diffusion 2.0 was released, we perform a more limited analysis of this version in Section~TODO. At a high level these models are both similar, having been pretrained on LAION-2B-en and then fine-tuned on LAION-Aesthetics for steps of SGD. The difference between these versions is the exact training schedule and training dataset, for example v1.4 was trained on a raw collection of Aesthetics but v2.0 was trained on a deduplicated version of this dataset. Identifying duplicates in the training data. To reduce the computational load of our attack, as is done in , we bias our search towards duplicated training examples because these are orders of magnitude more likely to be memorized than non-duplicated examples~. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would significantly undercount the true rate of duplication. Instead, we account for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a pixel-level distance below some threshold. But this is computationally intractable, as it would require an all-pairs comparison of 160 million images in Stable Diffusion's training set, each of which is a dimensional vector. Instead, we first embed each image to a dimensional vector using CLIP~, and then perform the all-pairs comparison between images in this lower-dimensional space (increasing efficiency by over ). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corresponding captions as the input to our extraction attack. We consider three possible methods for identifying duplicate examples. First, we identify two examples as duplicates when the URL and caption both match exactly. Under this definition, there is almost no data duplication: the most-duplicated example is repeated just times in the dataset. examples that are duplicated if we consider a record-level definition of duplication (that is, no two examples in the dataset have entirely identical metadata including size). Second, we check if their image URL is the same (the captions may be different). This situation occurs when the same image (e.g., a Wikipedia-hosted image) is embedded on multiple websites with differing alt-text. With this definition, we find orders of magnitude more duplication: some examples are duplicated over times, and several thousand examples are duplicated over times. Finally, we count two examples as duplicates if the images are perceptually similar, that is, they have a high cosine distance between their CLIP embeddings~. In practice we found perceptual similarity to be the most effective measure for identifying duplicate data to attempt to attack. Extraction Methodology Our extraction approach adapts the methodology from prior work to images and consists of two steps: Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. Perform membership inference to separate the model's novel generations from those generations which are memorized training examples. Generating many images. The first step is trivial but computationally expensive: we query the function in a black-box manner using the selected prompts as input. To reduce the computational overhead of our experiments, we use the timestep-resampled generation implementation that is available in the Stable Diffusion codebase . This process generates images in a more aggressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual fidelity at a significant () performance increase. We generate candidate images for each text prompt to increase the likelihood that we find memorization. Performing membership inference. The second step requires flagging generations that appear to be memorized training images. Since we assume a black-box threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the-art membership inference attacks~. We instead design a new membership inference attack strategy based on the intuition that for diffusion models, with high probability for two different random initial seeds . On the other hand, if under some distance measure , it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the generations by connecting an edge between generation and if . If the largest clique in this graph is at least size 10 (i.e., 10 of the 500 generations are near-identical), we predict that this clique is a memorized image. Empirically, clique-finding is more effective than searching for pairs of images as it has fewer false positives. To compute the distance measure among the images in the clique, we use a modified Euclidean distance. In particular, we found that many generations were often spuriously similar according to distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping tiles and measure the maximum of the distance between any pair of image tiles between the two images. Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated images). We first sort all of these generated images by ordering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either ``extracted'' or ``not extracted'' by comparing it to the training images under Definition~. We find 94 images are -extracted. To ensure that these images not only match some arbitrary definition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and find that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not fit our 2-norm definition. Figure shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an difference under . (As a point of reference, re-encoding a PNG as a JPEG with quality level 50 results in an difference of on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack's false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify memorized images with false positives, and all our memorized images can be extracted with a precision above (k,,)_2xyxy160=0.1k<100_2_21,000kk1,00010,000162^162^20X = (x_1, , x_N)f_Rf_(x_1, , x_K)Rx_iR_i = j : reconstructs(x_i, x_j) RX[|_i R_i| / (N _i |R_i|)]Kk_2_20.05_2_2_20.06_20.070.2S_xnxxxnx=0.5n=501,2802.5Some CIFAR-10 training images are generated multiple times. In these cases, we only count the first generation as a successful attack. Further, because the CIFAR-10 training dataset contains many duplicate images, we do not count two generations of two different (but duplicated) images in the training dataset. In Figure~ we show a selection of training examples that we extract and full results are shown in Figure~ in the Appendix. Membership Inference Attacks We now evaluate membership inference with more traditional attack techniques that use white-box access, as opposed to Section that assumed black-box access. We will show that all examples have significant privacy leakage under membership inference attacks, compared to the small fraction that are sensitive to data extraction. We consider two membership inference attacks on our class-conditional CIFAR-10-trained diffusion models. replicates these results for unconditional models. The loss threshold attack. Yeom et al. introduce the simplest membership inference attack: because models are trained to minimize their loss on the training set, we should expect that training examples have lower loss than non-training examples. The loss threshold attack thus computes the loss and reports ``member'' if for some chosen threshold and otherwise ``non-member'. The value of can be selected to maximize a desired metric (e.g., true positive rate at some fixed false positive rate or the overall attack accuracy). The Likelihood Ratio Attack (LiRA). Carlini et al. introduce the state-of-the-art approach to performing membership inference attacks. LiRA first trains a collection of shadow models, each model on random subsets of the training dataset. LiRA then computes the loss for the example under each of these shadow models . These losses are split into two sets: the losses for the example under the shadow models that did see the example during training, and the losses for the example under the shadow models that did not see the example during training. LiRA finishes the initialization process by fitting Gaussians to the IN set and to OUT set of losses. Finally, to predict membership inference for a new model , we compute and then measure whether . Choosing a loss function. Both membership inference attacks use a loss function . In the case of classification models, Carlini et al.~ find that choosing a loss function is one of the most important components of the attack. We find that this effect is even more pronounced for diffusion models. In particular, unlike classifiers that have a single loss function (e.g., cross entropy) used to train the model, diffusion models are trained to minimize the reconstruction loss when a random quantity of Gaussian noise has been added to an image. This means that ``the loss'' of an image is not well defined---instead, we can only ask for the loss of an image for a certain timestep with a corresponding amount of noise (cf.eqn:diffusion_loss). We must thus compute the optimal timestep at which we should measure the loss. To do so, we train 16 shadow models each on a random 50We then compute the loss for every model, for every example in the training dataset, and every timestep ( in the models we use). fig:mia_vs_diffusion_time_cifar10 plots the timestep used to compute the loss against the attack success rate, measured as the true positive rate (TPR), i.e., the number of examples which truly are members over the total number of members, at a fixed false positive rate (FPR) of 1Evaluating at leads to the most successful attacks. We conjecture that this a ``Goldilock's zone'' for membership inference: if is too small, and so the noisy image is similar to the original, then predicting the added noise is easy regardless if the input was in the training set; if is too large, and so the noisy image is similar to Gaussian noise, then the task is too difficult. Our remaining experiments will evaluate at , where we observed a TPR of 71Baseline Attack Results We now evaluate membership inference using our specified loss function. We follow recent advice~ and evaluate the efficacy of membership inference attacks by comparing their true positive rate to the false positive rate on a log-log scale. In~fig:compare_lira_mia, we plot the membership inference ROC curve for the loss threshold attack and LiRA. An out-of-the-box implementation of LiRA achieves a true positive rate of over <20(In part this may be because diffusion models are often trained far longer than classifiers.) Qualitative analysis. In Figure~, we visualize the least- and most-private images as determined by their easiness to detect via LiRA. We find that the easiest-to-attack examples are all extremely out-of-distribution visually from the CIFAR-10 dataset. These images are even more visually out-of-distribution compared to the outliers identified by Feldman et al.~ who produce a similar set of images but for image classifiers. In contrast, the images that are hardest to attack are all duplicated images. It is challenging to detect the presence or absence of each of these images in the training dataset because there is another identical image in the training dataset that may have been present or absent---therefore making the membership inference question ill-defined. Augmentations Improve Attacks Membership inference attacks can also be improved by reducing the variance in the loss signal~. We study two ways to achieve this for diffusion models. First, because our loss function has randomness (recall that to compute the reconstruction loss we measure the quantity for a random noise sample ), we can compute a better estimate of the true loss by averaging over different noise samples: . By varying the number of point samples taken to estimate this expectation we can potentially increase the attack success rate. And second, because our diffusion models train on augmented versions of training images (e.g., by flipping images horizontally), it makes sense to compute the loss averaged over all possible augmentations. Prior work has found that both of these attack strategies are effective at increasing the efficacy of membership inference attacks for classifiers~, and we find they are effective here as well. Improved attack results. Figure~ shows the effect of combining both these strategies. Together they are remarkably successful, and at a false positive rate of 7fig:improved_mia_with_repeated_noise in the Appendix breaks down the impact of each component: in~fig:mia_repeated_noise we increase the number of Monte Carlo samples from 1 (the base LiRA attack) to 20, and in~fig:mia_repeated_noise_and_flip we augment samples with a horizontal flip. Memorization Versus Utility We train our diffusion models to reach state-of-the-art levels of performance. Prior work on language models has found that better models are often easier to attack than less accurate models---intuitively, because they extract more information from the same training dataset . Here we perform a similar experiment. Attack results vs.~FID. To evaluate our generative models, we use the standard Fréchet Inception Distance (FID)~, where lower scores indicate higher quality. Our previous CIFAR-10 results used models that achieved the best FID (on average 3.5) based on early stopping. Here we evaluate models over the course of training in fig:mia_fid_tpr_fpr. We compute the attack success rate as a function of FID, and we find that as the quality of the diffusion model increases so too does the privacy leakage. These results are concerning because they suggest that stronger diffusion models of the future may be even less private. Inpainting Attacks Having performed untargeted extraction on CIFAR-10 models, we now construct a targeted version of our attack. As mentioned earlier, performing a targeted attack is complicated by the fact that these models do not support textual prompting. We instead provide guidance by performing a form of attribute inference attack that we call an ``inpainting attack''. Given an image, we first mask out a portion of this image; our attack objective is to recover the masked region. We then run this attack on both training and testing images, and compare the attack efficacy on each. Specifically, for an image , we mask some fraction of pixels to create a masked image , and then use the trained model to reconstruct the image as . The exact algorithm we use for inpainting is given in Lugmayr et al.~. Because diffusion model inpainting is stochastic (it depends on the random sample ), we create a set of inpainted images , where we set . For each , we compute the diffusion model's loss on this sample (at timestep 100) divided by a shadow model's loss that was not trained on the sample. We then use this score to identify the highest-scoring reconstructions . Results. Our specific attack masks out the left half of an image and applies the diffusion model on the right half of the image to inpaint the rest. We repeat this process 5000 times and take the top-10 scoring reconstructions using a membership inference attack. We repeat this attack for 100 images using diffusion models that are trained with and without the images. fig:inpaint_attack_distances compares the average distance between the sample and the ten highest scoring inpainted samples. This allows us to show our inpainting attacks have succeed: the reconstruction loss is substantially better in terms of distance when the image is in the training set than when not. fig:example_inpaint_attack also shows qualitative examples of this attack. The highest-scoring reconstruction looks visually similar to the target image when the target is in training and does not resemble the target when it is not in training. Overall, these results show that an adversary who has partial knowledge of an image can substantially improve their extraction results. We conduct a more thorough analysis of inpainting attacks in sec: cifar10_inpaint_more. Comparing Diffusion Models to GANs Are diffusion models more or less private than competing generative modeling approaches? In this section we take a first look at this question by comparing diffusion models to Generative Adversarial Networks (GANs) , an approach that has held the state-of-the-art results for image generation for nearly a decade. Unlike diffusion models that are explicitly trained to memorize and reconstruct their training datasets, GANs are not. Instead, GANs consist of two competing neural networks: a generator and a discriminator. Similar to diffusion models, the generator receives random noise as input, but unlike a diffusion model, it must convert this noise to a valid image in a single forward pass. To train a GAN, the discriminator is trained to predict if an image comes from the generator or not, and the generator is trained to fool the discriminator. As a result, GANs differ from diffusion models in that their generators are only trained using indirect information about the training data (i.e., using gradients from the discriminator) because they never receive training data as input, whereas diffusion models are explicitly trained to reconstruct the training set. Membership inference attacks. We first propose a privacy attack methodology for GANs. We initially focus on membership inference attacks, where following Balle et al. , we assume access to both the discriminator and generator. We perform membership inference using the loss threshold and LiRA attacks, where we use the discriminator's loss as the metric. To perform LiRA, we follow a similar methodology as Section~ and train 256 individual GAN models each on a random 50 Data extraction results. We next turn our attention away from measuring worst-case privacy risk and focus our attention on more practical black-box extraction attacks. We follow the same procedure as Section~, where we generate images from each model architecture and identify those that are near-copies of the training data using the same similarity function as before. Again we only consider non-duplicated CIFAR-10 training images in our counting. For this experiment, instead of using models we train ourselves (something that was necessary to run LiRA), we study five off-the-shelf pre-trained GANs: WGAN-ALP~, E2GAN~, NDA~, DiffBigGAN~, and StyleGAN-ADA~. We also evaluate two off-the-shelf DDPM diffusion model released by Ho et al.~ and Nichol et al.~. Note that all of these pre-trained models are trained by the original authors to maximize utility on the entire CIFAR-10 dataset rather than a random 50Table~ shows the number of extracted images for each model and their corresponding FID. Overall, we find that diffusion models memorize more data than GANs, even when the GANs reach similar performance, e.g., the best DDPM model memorizes more than StyleGAN-ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to memorize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes more images than the weakest GANs. Using the GANs we trained ourselves, we show examples of the near-copy generations in Figure~ for the three GANs that we trained ourselves, and Figure~ in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the five off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffusion models are less private than GAN models. We also surprisingly find that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we find that 244 unique images are memorized in common. If images were memorized uniformly at random, we should expect on average images would be memorized by both, giving exceptionally strong evidence that some images are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we explore various defenses and practical strategies that may help to reduce and audit model memorization. Deduplicating Training Data In Section~, we showed that many examples that are easy to extract are duplicated many times (e.g., ) in the training data. Similar results have been shown for language models for text~ and data deduplication has been shown to be an effective mitigation against memorization for those models~. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity metrics such as distance or CLIP similarity. We thus encourage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplication, we deduplicate CIFAR-10 and re-train a diffusion model on this modified dataset. We compute image similarity using the imagededup tool and deduplicate any images that have a similarity above . This removes examples from the total examples in CIFAR-10. We repeat the same generation procedure as Section~, where we generate images from the model and count how many examples are regenerated from the training set. The model trained on the deduplicated data regenerates examples, as compared to for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplication will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we observed a much stronger correlation between data extraction and duplication rates for those models. Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guarantees~. Diffusion models can be trained with differentially-private stochastic gradient descent (DP-SGD)~, where the model's gradients are clipped and noised to prevent the model from leaking substantial information about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off between privacy and utility, and recent work shows that DP-SGD can be applied to small-scale diffusion models without substantial performance degradation~. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to determine how vulnerable they are in practice~. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate memorization in language models~. Here, one creates a large ``pool'' of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the exposure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss-based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we generate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at different rates and measure the maximum exposure. Figure~ shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increasing with duplicate count, which may be a result of some canaries being ``harder'' than others, and, ultimately, random canaries we generate may not be the most effective canaries to use to test memorization for diffusion models. Related Work Memorization in language models. Numerous past works study memorization in generative models across different domains, architectures, and threat models. One area of recent interest is memorization in language models for text, where past work shows that adversaries can extract training samples using two-step attack techniques that resemble our approach~. Our work differs from these past results because we focus on the image domain and also use more semantic notions of data regeneration (e.g., using CLIP scores) as opposed to focusing on exact verbatim repetition (although recent language modeling work has begun to explore approximate memorization as well ). Memorization in image generation. Aside from language modeling, past work also analyzes memorization in image generation, mainly from the perspective of generalization in GANs (i.e., the novelty of model generations). For instance, numerous metrics exist to measure similarity with the training data~, the extent of mode collapse~, and the impact of individual training samples~. Moreover, other work provides insights into when and why GANs may replicate training examples~, as well as how to mitigate such effects~. Our work extends these lines of inquiry to conditional diffusion models, where we measure novelty by computing how frequently models regenerate training instances when provided with textual prompts. Recent and concurrent work also studies privacy in image generation for both GANs~ and diffusion models~. Tinsley et al.~ show that StyleGAN can generate individuals' faces, and Somepalli et al.~ show that Stable Diffusion can output semantically similar images to its training set. Compared to these works, we identify privacy vulnerabilities in a wider range of systems (e.g., Imagen and CIFAR models) and threat models (e.g., membership inference attacks). Discussion and Conclusion State-of-the-art diffusion models memorize and regenerate individual training images, allowing adversaries to launch training data extraction attacks. By training our own models we find that increasing utility can degrade privacy, and simple defenses such as deduplication are insufficient to completely address the memorization challenge. We see that state-of-the-art diffusion models memorize more than comparable GANs, and more useful diffusion models memorize more than weaker diffusion models. This suggests that the vulnerability of generative image models may grow over time. Going forward, our work raises questions around the memorization and generalization capabilities of diffusion models. Questions of generalization. Do large-scale models work by generating novel output, or do they just copy and interpolate between individual training examples? If our extraction attacks had failed, it may have refuted the hypothesis that models copy and interpolate training data; but because our attacks succeed, this question remains open. Given that different models memorize varying amounts of data, we hope future work will explore how diffusion models copy from their training datasets. Our work also highlights the difficulty in defining memorization. While we have found extensive memorization with a simple -based measurement, a more comprehensive analysis will be necessary to accurately capture more nuanced definitions of memorization that allow for more human-aligned notions of data copying. Practical consequences. We raise four practical consequences for those who train and deploy diffusion models. First, while not a perfect defense, we recommend deduplicating training datasets and minimizing over-training. Second, we suggest using our attack---or other auditing techniques---to estimate the privacy risk of trained models. Third, once practical privacy-preserving techniques become possible, we recommend their use whenever possible. Finally, we hope our work will temper the heuristic privacy expectations that have come to be associated with diffusion model outputs: synthetic data does not give privacy for free . On the whole, our work contributes to a growing body of literature that raises questions regarding the legal, ethical, and privacy issues that arise from training on web-scraped public data~. Researchers and practitioners should be wary of training on uncurated public data without first taking steps to understand the underlying ethics and privacy implications. Contributions [itemsep=0pt] Nicholas, Jamie, Vikash, and Eric each independently proposed the problem statement of extracting training data from diffusion models. Nicholas, Eric, and Florian performed preliminary experiments to identify cases of data extraction in diffusion models. Milad performed most of the experiments on Stable Diffusion and Imagen, and Nicholas counted duplicates in the LAION training dataset; each wrote the corresponding sections of the paper. Jamie performed the membership inference attacks and inpainting attacks on CIFAR-10 diffusion models, and Nicholas performed the diffusion extraction experiments; each wrote the corresponding sections of the paper. Matthew ran experiments for canary memorization and wrote the corresponding section of the paper. Florian and Vikash performed preliminary experiments on memorization in GANs, and Milad and Vikash ran the experiments included in the paper. Milad ran the membership inference experiments on GANs. Vikash ran extraction experiments on pretrained GANs. Daphne and Florian improved figure clarity and presentation. Daphne, Borja, and Eric edited the paper and contributed to paper framing. Nicholas organized the project and wrote the initial paper draft. Acknowledgements and Conflicts of Interest The authors are grateful to Tom Goldstein, Olivia Wiles, Katherine Lee, Austin Tarango, Ian Wilbur, Jeff Dean, Andreas Terzis, Robin Rombach, and Andreas Blattmann for comments on early drafts of this paper. Nicholas, Milad, Matthew, and Daphne are employed at Google, and Jamie and Borja are employed at DeepMind, companies that both train large machine learning models (including diffusion models) on both public and private datasets. Eric Wallace is supported by the Apple Scholars in AI/ML Fellowship.",
    "idea_abstract": "We investigate the privacy risks of image diffusion models, which have been shown to generate high-quality synthetic images. Our research reveals that these models can memorize and reproduce individual images from their training data, compromising user privacy. We analyze the impact of various modeling and data decisions on this vulnerability, highlighting the need for new approaches to ensure privacy preservation in generative models.",
    "proposal": "1. Title: Extracting Training Data from Diffusion Models: A Study on Memorization and Privacy Risks\n\n2. Problem Statement: Diffusion models, a type of generative neural network, have gained popularity for their ability to generate high-quality synthetic images. However, our research reveals that these models memorize and regenerate individual training images, posing significant privacy risks. This raises concerns about the potential misuse of these models, particularly in sensitive domains such as medical imagery.\n\n3. Motivation: Existing methods for image generation, such as GANs, have been shown to be vulnerable to privacy attacks. However, diffusion models have been assumed to be more private due to their ability to generate novel images. Our research challenges this assumption, demonstrating that diffusion models are more susceptible to data extraction attacks than previously thought. We investigate the factors that contribute to memorization in diffusion models, including model accuracy, hyperparameters, augmentation, and deduplication.\n\n4. Proposed Method: We develop a two-stage data extraction attack that generates images using standard approaches and flags those that exceed certain membership inference scoring criteria. We apply this method to Stable Diffusion and Imagen, extracting over a thousand near-identical replicas of training images. We also train hundreds of diffusion models on CIFAR-10"
  },
  {
    "rank": 17,
    "title": "Large Language Models Are Not Fair Evaluators",
    "arxiv": "2305.17926v1",
    "paper_tar": "tar_files/2305.17926v1.tar.gz",
    "abstract": "We uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13b could beat ChatGPT on 66 over 80 tested queries with ChatGPT as the evaluator. To address this issue, we propose two simple yet effective calibration strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple detailed pieces of evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score. Extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. To facilitate future research on more robust large language model comparison, we integrate the techniques in the paper into an easy-to-use toolkit FairEval, along with the human annotations.https://github.com/i-Eval/FairEval",
    "content": "=1 [11pt]article []acl times latexsym float amsthm tcolorbox [T1]fontenc [utf8]inputenc microtype amsmath, amssymb booktabs graphicx multirow bm algorithm pifont subfigure xspace [1]orange [ #1 --Tianyu] [1]blue [ #1 --Peiyi] [1]red [ #1 --Lei] [1]#1 Large Language Models are not Fair Evaluators Peiyi Wang Lei Li Liang Chen Dawei Zhu Binghuai Lin Yunbo Cao Qi Liu Tianyu Liu Zhifang Sui National Key Laboratory for Multimedia Information Processing, Peking University The University of Hong Kong Tencent Cloud AI wangpeiyi9979, nlp.lilei@gmail.com; leo.liang.chen@outlook.com liuqi@cs.hku.hk; binghuailin,yunbocao,rogertyliu@tencent.com; dwzhu, szf@pku.edu.cn We uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13b could beat ChatGPT on 66 over 80 tested queries with ChatGPT as the evaluator. To address this issue, we propose two simple yet effective calibration strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple detailed pieces of evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score. Extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. To facilitate future research on more robust large language model comparison, we integrate the techniques in the paper into an easy-to-use toolkit FairEval, along with the human annotations. Introduction The rapid advancement of large language models (LLMs)~ has underscored the importance of evaluating their alignment with human intent in generated responses, making it an active field of research. Traditional n-gram metrics like BLEU and ROUGE , as well as more sophisticated model-based evaluations such as BERTScore and BARTScore , are insufficient for thoroughly assessing this alignment. While human evaluation provides the most accurate measure of model performance and valuable insights, it can often be costly and time-consuming. As a result, there is a growing demand for automated assessment methods that can consistently align with human judgments while being more efficient and cost-effective. ChatGPT~ and GPT-4 have recently demonstrated remarkable performance across various tasks, leading to their widespread use as both the human annotators and evaluators . For example, vicuna's evaluation pipeline has garnered significant interest due to its simplicity and interpretability, as it allows GPT-4 to score and compare candidate responses and provide explanations. However, it is not clear how reliable LLMs are as evaluators, as they are known to be sensitive to textual instructions and inputs . This raises important questions about the resilience of this approach against perturbations, such as the ordering of candidates during scoring. As a result, further investigation into the evaluation of alignment with LLMs is both necessary and timely. In this paper, we critically examine the LLMs-as-evaluator paradigm and uncover a significant positional bias. Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate answer by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure , merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in ChatGPT, which typically favors the second answer. These findings highlight previously overlooked limitations in the current evaluation paradigm. To address this issue, we propose two simple strategies to mitigate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence chains to further stabilize the result. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. In summary, our key contributions are: 1) Revealing that LLMs exhibit severe positional bias, compromising their fairness as evaluators. 2) Developing two simple yet effective strategies to calibrate the positional bias of LLMs. 3) Demonstrating the effectiveness of our proposed approach through experimental results, which show closer alignment with human judgments. Positional Bias of the LLM Evaluator LLMs as Evaluators Recently, researchers have been utilizing LLMs such as GPT-4 as evaluators to compare the performance of two AI assistants. As shown in Table , the researchers create an evaluation template with three placeholders: This template is used to generate prompts for the LLM. For each testing question , along with two responses and from model 1 and model 2 respectively, the researchers populate these responses into the corresponding slots of the evaluation template to form a prompt: The prompt is then used to query the LLM in order to obtain the comparison result. In this paper, we found that LLM suffers from severe positional bias, i.e., by swapping the slots of the two responses and querying LLM twice, the model will most likely produce conflicting evaluation results, and the model will favor the response at a certain position. Revealing the Positional Bias In this section, we use GPT-4 and ChatGPT as evaluators to analyze the characteristics of positional bias in LLM evaluators. We find that: 1) LLMs suffer from positional biases, but the extent varies. As shown in Table , in the evaluation of ``Vicuna-13b v.s. ChatGPT'' and ``Vicuna-13b v.s. Alpaca-13b'', when the order was switched, GPT-4 exhibited conflict rates of 46In contrast, ChatGPT displayed considerably higher conflict rates, with figures of 82.5In general, the stronger the model, the less it is affected by positional bias. 2) LLMs vary in their position preferences. Based on the same evaluation prompt in Table , GPT-4 tends to favor the answer in the first position, while ChatGPT shows a preference for the answer in the second position. For example, in the comparison between \"Vicuna-13b\" and ChatGPT, GPT-4 yields winning rates of 3) The degree of positional bias varies based on the difference in response quality. We notice that the conflict rate of ``Vicuna-13b v.s. Alpaca-13b'' is much lower than that of ``Vicuna-13b v.s. ChatGPT'', suggesting that positional bias may not have the same impact on the assessment of different responses. One potential reason is that there is a significant difference in the quality of responses between Alpacas and Vicunas, and positional bias is not strong enough to change the judgment in such a situation. To further investigate this issue, we grouped all the samples based on the score difference between the two responses. As shown in Figure , we found that when the score difference between the two answers is small (score gap 1), the evaluation results of GPT-4 are significantly affected by the position of the answers. However, when the score difference between the two answers is large (score gap > 2), GPT-4's evaluation results are relatively stable. Calibrating the Positional Bias We have identified that positional bias can significantly impact the evaluation results of LLMs. In this section, we take two straightforward strategies to calibrate this bias. Multiple Evidence Calibration. Previous works utilize the evaluation template that draws the conclusion first and then makes an explanation. However, due to the nature of the auto-regressive model, the conclusions generated by the model are not supported by the post-generated explanation. To this end, as shown in Table , we take a multiple evidence calibration (MEC) strategy that requires the model to generate the explanation first and then give the score. In this way, the score can be calibrated with more supporting evidence. Rather than generating a single piece of evidence, MEC samples multiple evidence chains and calculates the average scores as the final score. Balanced Position Calibration. We also employ a balanced position calibration (BPC) strategy to alleviate the inherent positional bias in LLMs. BPC conducts two rounds of scoring for each sample by swapping the position of two answers. To determine the final score for a particular answer, we calculate the average score between its performance as the first response and as the second response. This averaging process helps to ensure a more balanced evaluation and reduce the impact of positional bias in the scoring process. Experiments Human Annotation In order to validate the effectiveness of our proposed strategies, we enlisted the participation of three groups of annotators. Following the same template as the original Vicuna, we instructed the annotators to assess the responses provided by Vicuna-13b and ChatGPT from four different perspectives: helpfulness, relevance, accuracy, and level of detail. The answers of Vicuna and ChatGPT are presented to the annotators in random order, and the evaluation process for each sample took an average of three minutes. We pay annotators at a rate of 15 dollars per hour, resulting in a total annotation cost of 180 dollars. The final result is based on the majority opinion among the annotators. Experimental Setup and Metric We use the OpenAI API to conduct our experiments and set the generated temperature to for deterministic generation results. For the multiple evidence calibration strategy, we set the temperature to and sample three evidence chains. The averaged score of different chains is adopted as the final result. We use the accuracy and kappa correlation coefficient with human annotation results to measure the performance of different evaluators. Main Results Table illustrates the performance of different methods on our manually annotated 80 annotated samples. As is shown: 1) There is a good correlation coefficient between the annotations provided by each human annotator and the final voting results. In detail, the average accuracy and the kappa correlation coefficient of human annotations are 2) Overall, GPT-4 achieves higher alignment with human judgments compared with ChatGPT, showing the powerful of GPT-4; 3) Compared to the previously commonly used evaluation template `Ans-exp', our proposed two calibration strategies significantly enhance the alignment between GPT-4 and ChatGPT with human judgments. For instance, by employing the MEC and BPC calibration strategies, ChatGPT demonstrates a notable improvement in both accuracy and the kappa correlation coefficient. Specifically, the accuracy improves by 14.3In conclusion, our proposed MEC and BPC are simple but very effective strategies to improve the performance of LLM evaluators. Analysis In this section, we first analyze the effect of varying the ensemble number ~() in MEC, and further investigate the alignment correlation in fine-grained question categories~(). Ablation on Evidence Number k In the MEC strategy, we sample evidence chains and ensemble them to enhance the evaluation process. We conduct an analysis to examine the influence of the number of evidence chains, , on the model's evaluation performance. As illustrated in Figure , we compared the performance of ChatGPT with different values of , namely 1, 3, 5, and 7. The performance of the model increases and then tends to be constant or decreases slightly as becomes larger. Despite the slight decrease, the enhancement of the model effect by the MCE strategy is still significant at different values of , illustrating the stability of the MEC strategy. Consequently, we found that a value of yields an optimal performance. With this value, the model achieves a notable level of performance while keeping the API cost relatively low. Fine-Grained Analysis of Evaluation Quality In order to further analyze the evaluation capabilities of the model, we perform a fine-grained analysis of the questions by dividing them into categories following . We calculate the performance of different evaluators within these categories. As shown in Figure , we find that: 1) In certain complex tasks such as common-sense, coding and math, GPT-4 performs significantly better than ChatGPT, highlighting the strength of GPT-4 as a more fair evaluator; 2) Our proposed MEC+BPC strategy demonstrates noticeable improvement in evaluating ChatGPT's performance on complex tasks, allowing us to obtain satisfactory evaluation results with a relatively small API cost. Conclusion Our research reveals a systematic bias in alignment evaluation with advanced ChatGPT/GPT-4 models. By manipulating the order of candidate responses during evaluation, we can significantly influence their ranking. To mitigate this bias, we introduce two effective strategies, namely Multiple Evidence Calibration (MEC) and Balanced Position Calibration (BPC). MEC requires evaluators to first provide multiple detailed pieces of evidence to support their subsequent ratings and BPC aggregates the results from various orders to determine the final score. These strategies successfully reduce bias and improve alignment with human judgments. We provide our code and human annotations to support future studies and enhance the evaluation of generative models. Evaluation Template that Makes Directly Comparison",
    "idea_abstract": "We identify a potential flaw in the evaluation methodology of large language models, where the order of candidate responses can influence the evaluation outcome. This vulnerability allows for manipulation of the evaluation result, potentially skewing the comparison between models. To address this issue, we propose two calibration strategies: Multiple Evidence Calibration and Balanced Position Calibration, which aim to mitigate evaluation bias and align with human judgments.",
    "proposal": "1. Title: Fair Evaluation of Large Language Models\n\n2. Problem Statement: The evaluation of large language models (LLMs) is a critical task, but the current evaluation paradigm using LLMs as evaluators is flawed due to a systematic bias, where the order of candidate responses can significantly influence the evaluation results. This bias can lead to inaccurate and unfair comparisons between models, compromising the reliability of LLMs as evaluators.\n\n3. Motivation: Existing methods for evaluating LLMs, such as n-gram metrics and model-based evaluations, are insufficient for assessing the alignment of generated responses with human intent. Human evaluation is the most accurate measure, but it is costly and time-consuming. The use of LLMs as evaluators has gained popularity, but their reliability is uncertain due to their sensitivity to textual instructions and inputs. The positional bias in LLM evaluators raises concerns about the fairness and accuracy of model comparisons.\n\n4. Proposed Method: We propose two simple yet effective strategies to mitigate the positional bias in LLM evaluators: Multiple Evidence Calibration (MEC) and Balanced Position Calibration (BPC). MEC requires the evaluator model to generate multiple detailed pieces of evidence before assigning ratings, leveraging the inherent properties of causal language models for calibration. BPC aggregates"
  },
  {
    "rank": 18,
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in Hugging Face",
    "arxiv": "2303.17580v3",
    "paper_tar": "tar_files/2303.17580v3.tar.gz",
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence~https://github.com/microsoft/JARVIS.",
    "content": "article [numbers]natbib [preprint]neurips_2022 [utf8]inputenc [T1]fontenc hyperref url booktabs amsfonts nicefrac microtype xcolor graphicx multirow array makecell soul caption subcaption accents enumitem wrapfig [table]skip=3pt [nameinlink,capitalise]cleveref C[1]>p#1 L[1]>p#1 HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face [1]red #1 e.g. i.e. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang Zhejiang University, Microsoft Research Asia syl, luwm, yzhuang@zju.edu.cn, kaitaosong, xuta, dongsli@microsoft.com Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence~. Introduction Large language models (LLMs)~, such as ChatGPT, have attracted enormous attentions from both academia and industry, due to their remarkable performance on various natural language processing (NLP) tasks. Based on large-scale pre-training on massive text corpora and reinforcement learning from human feedback (RLHF) , LLMs can produce superior capability in language understanding, generation, interaction, and reasoning. The powerful capability of LLMs also drives many emergent research topics (, in-context learning~, instruction learning~, and chain-of-thought prompting~) to further investigate the huge potential of LLMs, and brings unlimited possibilities for us to build advanced artificial intelligence systems. Despite these great successes, current LLM technologies are still imperfect and confront some urgent challenges on the way to building an advanced AI system. We discuss them from these aspects: 1) Limited to the input and output forms of text generation, current LLMs lack the ability to process complex information such as vision and speech, regardless of their significant achievements in NLP tasks; 2) In real-world scenarios, some complex tasks are usually composed of multiple sub-tasks, and thus require the scheduling and cooperation of multiple models, which are also beyond the capability of language models; 3) For some challenging tasks, LLMs demonstrate excellent results in zero-shot or few-shot settings, but they are still weaker than some experts (, fine-tuned models). How to address these issues could be the critical step for LLMs toward artificial general intelligence. In this paper, we point out that in order to handle complicated AI tasks, LLMs should be able to coordinate with external models to utilize their powers. Hence, the key point is how to choose suitable middleware to bridge the connections between LLMs and AI models. To address this problem, we notice that each AI model can be described in the form of language by summarizing its function. Therefore, we introduce a concept: ``Language is a generic interface for LLMs to connect AI models''. In other words, by incorporating these model descriptions into prompts, LLMs can be considered as the brain to manage AI models such as planning, scheduling, and cooperation. As a result, this strategy enables LLMs to invoke external models for solving AI tasks. However, when it comes to integrating multiple AI models into LLMs, another challenge emerges: solving numerous AI tasks needs collecting a large number of high-quality model descriptions, which in turn requires heavy prompt engineering. Coincidentally, we notice that some public ML communities usually offer a wide variety of applicable models with well-defined model descriptions for solving specific AI tasks such as language, vision, and speech. These observations bring us some inspiration: Can we link LLMs (, ChatGPT) with public ML communities (, GitHub, Hugging Face~, etc) for solving complex AI tasks via a language-based interface? Therefore, in this paper, we propose a system called HuggingGPT to connect LLMs (, ChatGPT) and ML community (, Hugging Face), which can process inputs from different modalities and solve numerous complex AI tasks autonomously. More specifically, for each AI model in Hugging Face, we use its corresponding model description from the library and fuse it into the prompt to establish the connection with ChatGPT. Afterward, in our system, LLMs (, ChatGPT) will act as the brain to determine the answers to the questions of users. Just as shown in Figure~, the whole process of HuggingGPT can be divided into four stages: [leftmargin=*] Task Planning: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks via prompts. Model Selection: To solve the planned tasks, ChatGPT selects expert models that are hosted on Hugging Face based on model descriptions. Task Execution: Invoke and execute each selected model, and return the results to ChatGPT. Response Generation: Finally, ChatGPT is utilized to integrate the predictions from all models and generate responses for users. Benefiting from such a design, HuggingGPT can automatically generate plans from user requests and use external models, and thus can integrate multimodal perceptual capabilities and handle multiple complex AI tasks. More noteworthy, this pipeline also allows HuggingGPT to continue absorbing the powers from task-specific experts, enabling growable and scalable AI capabilities. Furthermore, we also point out that task planning plays a very important role in HuggingGPT, which directly determines the success of the subsequent workflow. Therefore, how to conduct planning is also a good perspective to reflect the capability of LLMs, which also opens a new door for LLM evaluation. Overall, our contributions can be summarized as follows: [leftmargin=*] To complement the advantages of large language models and expert models, we propose HuggingGPT with an inter-model cooperation protocol. HuggingGPT applies LLMs as the brain for planning and decision, and automatically invokes and executes expert models for each specific task, providing a new way for designing general AI solutions. By integrating the Hugging Face hub with numerous task-specific models around ChatGPT, HuggingGPT is able to tackle generalized AI tasks covering multiple modalities and domains. Through the open collaboration of models, HuggingGPT can provide users with multimodal and reliable conversation services. We point out the importance of task planning in HuggingGPT (and autonomous agents), and formulate some experimental evaluations for measuring the capability of LLMs in planning. Extensive experiments on multiple challenging AI tasks across language, vision, speech, and cross-modality demonstrate the capability and huge potential of HuggingGPT in understanding and solving complex tasks from multiple modalities and domains. Related Works In recent years, the field of natural language processing (NLP) has been revolutionized by the emergence of large language models (LLMs)~, exemplified by models such as GPT-3~, GPT-4~, PaLM~, and LLaMa~. LLMs have demonstrated impressive capabilities in zero-shot and few-shot tasks, as well as more complex tasks such as mathematical problems and commonsense reasoning, due to their massive corpus and intensive training computation. To extend the scope of large language models (LLMs) beyond text generation, contemporary research can be divided into two branches: 1) Some works have devised unified multimodal language models for solving various AI tasks . For example, Flamingo combines frozen pre-trained vision and language models for perception and reasoning. BLIP-2~ utilizes a Q-former to harmonize linguistic and visual semantics, and Kosmos-1~ incorporates visual input into text sequences to amalgamate linguistic and visual inputs. 2) Recently, some researchers started to investigate the integration of using tools or models in LLMs ~. Toolformer~ is the pioneering work to introduce external API tags within text sequences, facilitating the ability of LLMs to access external tools. Consequently, numerous works have expanded LLMs to encompass the visual modality. Visual ChatGPT~ fuses visual foundation models, such as BLIP~ and ControlNet~, with LLMs. Visual Programming~ and ViperGPT~ apply LLMs to visual objects by employing programming languages, parsing visual queries into interpretable steps expressed as Python code. We also include more discussions about related works in more_discuss. Distinct from these approaches, our proposed HuggingGPT advances towards more general AI capabilities in the following aspects: 1) HuggingGPT uses the LLM as an interface to route user requests to expert models, effectively combining the language comprehension capabilities of the LLM with the expertise of other expert models; 2) HuggingGPT is not limited to visual perception tasks but can address tasks in any modality or any domain by organizing cooperation among models through the LLM. Benefiting from the design of task planning in HuggingGPT, our system can automatically and effectively generate task procedures and solve more complex problems; 3) HuggingGPT offers a more open approach for model selection, which assigns and organizes tasks based on model descriptions. By providing only the model descriptions, HuggingGPT can continuously and conveniently integrate diverse expert models from AI communities, without altering any structure or prompt settings. This open and continuous manner brings us one step closer to realizing artificial general intelligence. HuggingGPT HuggingGPT is a collaborative system for solving AI tasks, which is composed of a large language model (LLM) and numerous expert models from ML communities. Its workflow includes four stages: task planning, model selection, task execution, and response generation, just as shown in fig:model. Given a user request, our HuggingGPT, which adopts an LLM as the controller, will automatically deploy the whole workflow, thereby coordinating and executing the expert models to fulfill the target. tab:prompt presents the detailed prompt design in our HuggingGPT. We will introduce the design of each stage in the following subsections. 1.2 Task Planning Generally, in real-world scenarios, many user requests will include some complex intents, and thus need to orchestrate multiple sub-tasks to fulfill the target. Therefore, we formulate task planning as the first stage of HuggingGPT, which aims to use LLM to analyze user request and then decompose it into a collection of structured tasks. Moreover, we also require the LLM to determine dependencies and execution orders for these decomposed tasks, to build their connections. To better prompt the LLM for effective task planning, HuggingGPT employs a prompt design, which consists of specification-based instruction and demonstration-based parsing. We introduce these details in the following paragraphs. Specification-based Instruction To better represent the expected tasks of user requests and use them in the subsequent stages, we expect that the LLM can parse tasks by following some specifications (e.g., JSON format). Therefore, we provide a uniform template for tasks and instruct the LLM to conduct task parsing through slot filing. As shown in tab:prompt, the template for task parsing includes four slots (\"task\", \"id\", \"dep\", and \"args\") to represent the task's information, unique identifier, dependencies and arguments. Further details regarding each slot can be found in the template description (see the template). By adhering to these task specifications, HuggingGPT can automatically employ the LLM to analyze user requests and parse tasks accordingly. Demonstration-based Parsing To better understand the intention and criteria for task planning, HuggingGPT incorporates multiple demonstrations in the prompt. Each demonstration consists of a user request and its corresponding output, which represents the expected sequence of parsed tasks. By incorporating dependencies among tasks, these demonstrations aid HuggingGPT in understanding the logical connections between tasks, facilitating accurate determination of execution order and identification of resource dependencies. The design of our demonstrations is presented in tab:prompt. Furthermore, to support more comprehensive user requests (e.g., multi-turn dialogue), we incorporate chat logs into the prompt by appending this instruction: To assist with task planning, the chat history is available as Chat Logs , where you can trace the user-mentioned resources and incorporate them into the task planning., where Chat Logs represents the previous chat logs. Such a design allows HuggingGPT to better manage context and answer user requests in multi-turn dialogue. Model Selection After task planning, HuggingGPT next needs to match the tasks and models, , select the most appropriate model for each task in the parsed task list. To this end, we use model descriptions as the language interface to connect each model. More specifically, we first obtain the descriptions of expert models from the ML community (e.g., Hugging Face) and then dynamically select models for the tasks through an in-context task-model assignment mechanism. This strategy enables incremental model access (simply providing the description of the expert models) and can be more open and flexible to use ML communities. More details are introduced in the next paragraph. In-context Task-model Assignment We formulate the task-model assignment as a single-choice problem, where potential models are presented as options within a given context. Generally, HuggingGPT is able to select the most appropriate model for each parsed task based on the providing user query and task information in the prompt. However, due to the constraints regarding maximum context length, it is impossible for a prompt to include all relevant model information. To address this issue, we first filter out models based on their task type and only retain those that match the current task type. For these selected models, we will rank them based on the number of their downloads on Hugging Face (we think the downloads can reflect the quality of the model to some extents) and then select the top- models as the candidate models for HuggingGPT. This strategy can substantially reduce the token usage in the prompt and effectively select the appropriate models for each task. Task Execution Once a specific model is assigned to a parsed task, the next step is to execute the task, , to perform model inference. Therefore, in this stage, HuggingGPT will automatically input these task arguments into the models, execute these models to obtain the inference results, and then send them back to the LLM. It is necessary to emphasize the issue of resource dependencies at this stage. Since the outputs of the prerequisite tasks are dynamically produced, HuggingGPT also needs to dynamically specify the dependent resources for the task before it is launched. Therefore, it is challenging to build the connections between the tasks with resource dependency at this stage. Resource Dependency To address this issue, we use a unique symbol, ``'', to maintain resource dependencies. Specifically, HuggingGPT identifies the resources generated by the prerequisite task as , where is the task id of the prerequisite task. During the task planning stage, if there are tasks that depend on the resource generated by the task with , HuggingGPT sets this symbol to the corresponding resource subfield in the task arguments. Then in the task execution stage, HuggingGPT dynamically substitutes this symbol with the resource generated by the prerequisite task. As a result, this simple strategy empowers HuggingGPT to efficiently handle resource dependencies during task execution. Furthermore, after obtaining the tasks which do not have any resource dependencies, we will execute these tasks in parallel to further improve inference efficiency. This means that multiple tasks that have satisfied the prerequisite dependencies can be started simultaneously. Besides, for speedup and computational stability, we also provide a hybrid inference endpoint to deploy these models. More details can refer to hybird_endpoint. Response Generation After all task executions are completed, HuggingGPT needs to generate the final responses. Just as shown in tab:prompt, HuggingGPT integrates all the information from the previous three stages (task planning, model selection, and task execution) into a concise summary in this stage, including the list of planned tasks, the selected models for the tasks, and the inference results of the models. Most important among them are the inference results, which are the key points for HuggingGPT to make the final decisions. These inference results are presented in a structured format, such as bounding boxes with detection probabilities in the object detection model, answer distributions in the question-answering model, etc. HuggingGPT allows LLM to receive these structured inference results as input and generate responses in the form of friendly human language. Moreover, instead of simply aggregating the results, LLM generates responses that actively respond to user requests, providing a reliable decision with a confidence level. Experiments Settings In our experiments, we employed the gpt-3.5-turbo, text-davinci-003 and gpt-4 variants of the GPT models as the main LLMs, which are publicly accessible through the OpenAI API~. To enable more stable outputs of LLM, we set the decoding temperature to 0. In addition, to regulate the LLM output to satisfy the expected format (e.g., JSON format), we set the logitbias to 0.2 on the format constraints (e.g., ``'' and ``''). We provide detailed prompts designed for the task planning, model selection, and response generation stages in tab:prompt, where variable indicates the slot which needs to be populated with the corresponding text before being fed into the LLM. Qualitative Results fig:pipeline and fig:model have shown two demonstrations of HuggingGPT. In fig:pipeline, the user request consists of two sub-tasks: describing the image and object counting. In response to the request, HuggingGPT planned three tasks: image classification, image captioning, and object detection, and launched the google/vit , nlpconnet/vit-gpt2-image-captioning , and facebook/detr-resnet-101 models, respectively. Finally, HuggingGPT integrated the results of the model inference and generated responses (description of the image and the number of the contained objects) to the user. A more detailed example is shown in ~fig:model. The user requested to first detect the pose of the person in the example image, then generate a new image based on that pose and the specified text, and finally generate a speech describing the image. HuggingGPT first parsed 6 tasks based on the request, including pose detection, text-to-image conditional on pose, object detection, image classification, image captioning, and text-to-speech. We observed that HuggingGPT can correctly orchestrate the execution order and resource dependencies of tasks, e.g., pose conditional text-to-image task need to be executed after pose detection and use its output as input. After that, HuggingGPT selects the appropriate model for each task and finally summarizes the results of the model execution into a final response. For more demonstrations, please refer to the case. Quantitative Evaluation r6.7cm -10pt Evaluation for the single task. ``Acc'' and ``Pre'' represents Accuracy and Precision. -10pt In HuggingGPT, task planning is the first but also the most important step in the whole workflow, since it directly determines the result of the subsequent pipeline. Therefore, we deem that the quality of task planning also reflects the capability of LLMs in some aspects (, reasoning and decision). In other words, assessing task planning provides a reliable metric for evaluating the ability of LLMs. We simplified the evaluation by only considering the task type without its arguments. To better conduct evaluations on task planning, we group tasks into three distinct categories (see tab:evaluation) and formulate different metrics for them: [leftmargin=*] Single Task refers to a request that involves only one task. We consider the planning to be correct when and only when the task name (i.e., \"task\") and the predicted label are identically equal. F1 and accuracy are chosen as evaluation metrics for single-task scenarios. Sequential Task represents that the intent of the user request can be decomposed as a sequence of multiple sub-tasks. Here, we use F1 and normalized Edit Distance~ as the metrics. Graph Task indicates that user requests can be decomposed into directed acyclic graphs. Considering the possibility of multiple planning topologies within graph tasks, only using F1-score is not enough to reflect the LLM capability in planning. To address this, following Vicuna~, we employed GPT-4 as a critic to evaluate whether the planning is correct. The accuracy is obtained by evaluating the judgment of GPT-4, referred to as the GPT-4 Score. Detailed information about the GPT-4 Score can be found in gpt_4_score. r6.7cm Evaluation for the sequential task. ``ED'' means Edit Distance. Dataset To conduct our evaluation, we invite some annotators to submit some requests. We collect these data as the evaluation dataset. We use GPT-4 to generate task planning as the pseudo labels, which cover single, sequential, and graph tasks. Furthermore, we also invite some expert annotators to label task planning for some complex requests (46 examples) as a high-quality human-annotated dataset. We also plan to further improve the quality and quantity of this dataset to better help us to evaluate the LLM capability in planning, which leaves as future work. More details about this dataset are in dataset. Based on this dataset, we deploy some experimental evaluations on different LLMs (Alpaca-7b~, Vicuna-7b~, and GPT models) for task planning. l8.2cm Evaluation for the graph task. tab:evaluation_sing,tab:evaluation_seq,,tab:evaluation_graph show the planning capabilities of HuggingGPT on the three categories of GPT-4 annotated datasets, respectively. We observed that GPT-3.5 exhibits more prominent planning capabilities, outperforming the open-source LLMs Alpaca-7b and Vicuna-7b in terms of all types of user requests. Specifically, in more complex tasks (e.g., sequential and graph tasks), GPT-3.5 has shown absolute predominance over other LLMs. These results also demonstrate the evaluation of task planning can reflect the LLM capability to some degree. Therefore, we think that how to develop technologies to improve the ability of LLMs in task planning is very important, and we leave it as a future research direction. l7.3cm Evaluation on the human-annotated dataset. Furthermore, we also conduct experiments on the high-quality human-annotated dataset to obtain a more precise evaluation. tab:evaluation_human reports the comparisons on the human-annotated dataset. We find that these results are consistent with the above conclusion, that the more powerful LLM will exhibit more better performance in task planning. Moreover, we also compare the results between human annotations and GPT-4 annotations. We find that even though GPT-4 outperforms other LLMs, it still remains a substantial gap when compared with human annotations. These phenomena also validate the necessity for continued advancements in enhancing the planning capabilities of LLMs. Due to computational resource limitations, we have not yet conducted evaluations on larger and more diverse LLMs. Our current experiments are primarily focused on assessing the task planning capabilities of LLMs and emphasizing their importance in our HuggingGPT framework. In the future, we will deploy more experiments to evaluate the capability of various LLMs in conducting task planning and explore how to standardize the metrics for evaluating task planning. We expect further research and development efforts can facilitate the community to understand and improve LLMs from different aspects, such as planning. Limitations HuggingGPT has presented a new paradigm for designing AI solutions, but we want to highlight that it still remains some limitations or improvement spaces: 1) Planning in HuggingGPT heavily relies on the capability of LLM. Consequently, we cannot ensure that the generated plan will always be feasible and optimal. Therefore, it is crucial to explore ways to optimize the LLM in order to enhance its planning abilities; 2) Efficiency poses a common challenge in our framework. To build such a collaborative system (i.e., HuggingGPT) with task automation, it heavily relies on a powerful controller (e.g., ChatGPT). However, HuggingGPT requires multiple interactions with LLMs throughout the whole workflow and thus brings increasing time costs for generating the response; 3) Token Lengths is another common problem when using LLM, since the maximum token length is always limited. Although some works have extended the maximum length to 32K, it is still insatiable for us if we want to connect numerous models. Therefore, how to briefly and effectively summarize model descriptions is also worthy of exploration; 4) Instability is mainly caused because LLMs are usually uncontrollable. Although LLM is skilled in generation, it still possibly fails to conform to instructions or give incorrect answers during the prediction, leading to exceptions in the program workflow. How to reduce these uncertainties during inference should be considered in designing systems. Conclusion In this paper, we propose a system named HuggingGPT to solve AI tasks, with language as the interface to connect LLMs with AI models. The principle of our system is that an LLM can be viewed as a controller to manage AI models, and can utilize models from ML communities like Hugging Face to automatically solve different requests of users. By exploiting the advantages of LLMs in understanding and reasoning, HuggingGPT can dissect the intent of users and decompose it into multiple sub-tasks. And then, based on expert model descriptions, HuggingGPT is able to assign the most suitable models for each task and integrate results from different models to generate the final response. By utilizing the ability of numerous AI models from machine learning communities, HuggingGPT demonstrates immense potential in solving challenging AI tasks, thereby paving a new pathway towards achieving artificial general intelligence. Acknowledgement We appreciate the support of the Hugging Face team to help us in improving our GitHub project and web demo. Besides, we also appreciate the contributions of Bei Li, Kai Shen, Meiqi Chen, Qingyao Guo, Yichong Leng, Yuancheng Wang, Dingyao Yu for the data labeling and Wenqi Zhang, Wen Wang, Zeqi Tan for paper revision. Appendix More details In this section, we will present more details about some designs of each stage in HuggingGPT. Template for Task Planning To format the parsed task, we define the template [\"task\": task, \"id\", taskid, \"dep\": dependencytaskids, \"args\": \"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL] with four slots: \"task\", \"id\", \"dep\", and \"args\". tab:template_planning presents the definitions of each slot. Model Descriptions In general, the Hugging Face Hub hosts expert models that come with detailed model descriptions, typically provided by the developers. These descriptions encompass various aspects of the model, such as its function, architecture, supported languages and domains, licensing, and other relevant details. These comprehensive model descriptions play a crucial role in aiding the decision of HuggingGPT. By assessing the user's requests and comparing them with the model descriptions, HuggingGPT can effectively determine the most suitable model for the given task. Hybrid Endpoint in System Deployment An ideal scenario is that we only use inference endpoints on cloud service (e.g., Hugging Face). However, in some cases, we have to deploy local inference endpoints, such as when inference endpoints for certain models do not exist, the inference is time-consuming, or network access is limited. To keep the stability and efficiency of the system, HuggingGPT allows us to pull and run some common or time-consuming models locally. The local inference endpoints are fast but cover fewer models, while the inference endpoints in the cloud service (e.g., Hugging Face) are the opposite. Therefore, local endpoints have higher priority than cloud inference endpoints. Only if the matched model is not deployed locally, HuggingGPT will run the model on the cloud endpoint like Hugging Face. Overall, we think that how to design and deploy systems with better stability for HuggingGPT or other autonomous agents will be very important in the future. Task List Up to now, HuggingGPT has supported 24 AI tasks, which cover language, vision, speech and etc. tab:task_list presents the detailed information of the supported task list in HuggingGPT. 1.1 GPT-4 Score Following the evaluation method used by Vicuna , we employed GPT-4 as an evaluator to assess the planning capabilities of LLMs. In more detail, we include the user request and the task list planned by LLM in the prompt, and then let GPT-4 judge whether the list of tasks is accurate and also provide a rationale. To guide GPT-4 to make the correct judgments, we designed some task guidelines: 1) the tasks are in the supported task list (see tab:task_list); 2) the planned task list can reach the solution to the user request; 3) the logical relationship and order among the tasks are reasonable. In the prompt, we also supplement several positive and negative demonstrations of task planning to provide reference for GPT-4. The prompt for GPT-4 score is shown in tab:gptscore. We further want to emphasize that GPT-4 score is not always correct although it has shown a high correlation. Therefore, we also expect to explore more confident metrics to evaluate the ability of LLMs in planning. Datasets for Task Planning Evaluation As aforementioned, we create two datasets for evaluating task planning. Here we provide more details about these datasets. In total, we gathered a diverse set of 3,497 user requests. Since labeling this dataset to obtain the task planning for each request is heavy, we employed the capabilities of GPT-4 to annotate them. Finally, these auto-labeled requests can be categorized into three types: single task (1,450 requests), sequence task (1,917 requests), and graph task (130 requests). For a more reliable evaluation, we also construct a human-annotated dataset. We invite some expert annotators to label some complex requests, which include 46 examples. Currently, the human-annotated dataset includes 24 sequential tasks and 22 graph tasks. Detailed statistics about the GPT-4-annotated and human-annotated datasets are shown in tab:stats. Case Study Case Study on Various Tasks Through task planning and model selection, HuggingGPT, a multi-model collaborative system, empowers LLMs with an extended range of capabilities. Here, we extensively evaluate HuggingGPT across diverse multimodal tasks, and some selected cases are shown in d4,,d4-1. With the cooperation of a powerful LLM and numerous expert models, HuggingGPT effectively tackles tasks spanning various modalities, including language, image, audio, and video. Its proficiency encompasses diverse task forms, such as detection, generation, classification, and question answering. Case Study on Complex Tasks Sometimes, user requests may contain multiple implicit tasks or require multi-faceted information, in which case we cannot rely on a single expert model to solve them. To overcome this challenge, HuggingGPT organizes the collaboration of multiple models through task planning. As shown in d3,,d2,,d1, we conducted experiments to evaluate the effectiveness of HuggingGPT in the case of complex tasks: [leftmargin=*] d3 demonstrates the ability of HuggingGPT to cope with complex tasks in a multi-round conversation scenario. The user splits a complex request into several steps and reaches the final goal through multiple rounds of interaction. We find that HuggingGPT can track the contextual state of user requests through the dialogue context management in the task planning stage. Moreover, HuggingGPT demonstrates the ability to access user-referenced resources and proficiently resolve dependencies between tasks in the dialogue scenario. d2 shows that for a simple request like \"describe the image in as much detail as possible\", HuggingGPT can decompose it into five related tasks, namely image captioning, image classification, object detection, segmentation, and visual question answering tasks. HuggingGPT assigns expert models to handle each task to gather information about the image from various perspectives. Finally, the LLM integrates this diverse information to deliver a comprehensive and detailed description to the user. d1 shows two cases where a user request can contain several tasks. In these cases, HuggingGPT first performs all the tasks requested by the user by orchestrating the work of multiple expert models, and then let the LLM aggregate the model inference results to respond to the user. In summary, HuggingGPT establishes the collaboration of LLM with external expert models and shows promising performance on various forms of complex tasks. Case Study on More Scenarios We show more cases here to illustrate HuggingGPT's ability to handle realistic scenarios with task resource dependencies, multimodality, multiple resources, etc. To make clear the workflow of HuggingGPT, we also provide the results of the task planning and task execution stages. [leftmargin=*] d5 illustrates the operational process of HuggingGPT in the presence of resource dependencies among tasks. In this case, HuggingGPT can parse out concrete tasks based on abstract requests from the user, including pose detection, image captioning, and pose conditional image generation tasks. Furthermore, HuggingGPT effectively recognizes the dependencies between task 3 and tasks 1, 2, and injected the inferred results of tasks 1 and 2 into the input arguments of task 3 after the dependency tasks were completed. d7 demonstrates the conversational ability of HuggingGPT on audio and video modalities. In the two cases, it shows HuggingGPT completes the user-requested text-to-audio and text-to-video tasks via the expert models, respectively. In the top one, the two models are executed in parallel (generating audio and generating video concurrently), and in the bottom one, the two models are executed serially (generating text from the image first, and then generating audio based on the text). This further validates that HuggingGPT can organize the cooperation between models and the resource dependencies between tasks. d6 shows HuggingGPT integrating multiple user-input resources to perform simple reasoning. We can find that HuggingGPT can break up the main task into multiple basic tasks even with multiple resources, and finally integrate the results of multiple inferences from multiple models to get the correct answer. More Discussion about Related Works The emergence of ChatGPT and its subsequent variant GPT-4, has created a revolutionary technology wave in LLM and AI area. Especially in the past several weeks, we also have witnessed some experimental but also very interesting LLM applications, such as AutoGPT~, AgentGPT~, BabyAGI~, and etc. Therefore, we also give some discussions about these works and provide some comparisons from multiple dimensions, including scenarios, planning, tools, as shown in tab:comparison. Scenarios Currently, these experimental agents (e.g., AutoGPT, AgentGPT and BabyAGI) are mainly used to solve daily requests. While for HuggingGPT, it focuses on solving tasks in the AI area (e.g., vision, language, speech, etc), by utilizing the powers of Hugging Face. Therefore, HuggingGPT can be considered as a more professional agent. Generally speaking, users can choose the most suitable agent based on their requirements (e.g., daily requests or professional areas) or customize their own agent by defining knowledge, planning strategy and toolkits. Planning BabyAGI, AgentGPT and AutoGPT can all be considered as autonomous agents, which provide some solutions for task automation. For these agents, all of them adopt step-by-step thinking, which iteratively generates the next task by using LLMs. Besides, AutoGPT employs an addition reflexion module for each task generation, which is used to check whether the current predicted task is appropriate or not. Compared with these applications, HuggingGPT adopts a global planning strategy to obtain the entire task queue within one query. It is difficult to judge which one is better, since each one has its deficiencies and both of them heavily rely on the ability of LLMs, even though existing LLMs are not specifically designed for task planning. For example, iterative planning combined with reflexion requires a huge amount of LLM queries, and if one step generates an error prediction, the entire workflow would possibly enter an endless loop. While for global planning, although it can always produce a solution for each user request within one query, it still cannot guarantee the correctness of each step or the optimality of the entire plan. Therefore, both iterative and global planning have their own merits and can borrow from each other to alleviate their shortcoming. Additionally, one notable point is that the difficulty of task planning is also linearly correlated to the task range. As the scope of tasks increases, it becomes more challenging for the controller to predict precise plans. Consequently, optimizing the controller (i.e., LLM) for task planning will be crucial in building autonomous agents. Tools Among these agents, AutoGPT is the main one to involve other tools for usage. More specifically, AutoGPT primarily uses some common tools (e.g., web search, code executor), while HuggingGPT utilizes the expert models of ML communities (e.g., Hugging Face). Therefore, AutoGPT has a broader task range but is not suitable for more professional problems, whereas HuggingGPT is more specialized and focuses on solving more complex AI tasks. Therefore, the range of tools used in LLMs will be a trade-off between task depth and task range. In addition, we also note some industry products for LLM applications (e.g., ChatGPT plugins~) and developer tools (e.g., LangChain~, HuggingFace Transformer Agent~, Semantic Kernels~) for LLM applications. We believe these rapid developments will also facilitate the community to explore how to better integrate LLMs with external tools. Overall, HuggingGPT can also be considered as an autonomous agent. Compared with these experimental agents, which mainly use GPT models to generate solutions for user requests, HuggingGPT systematically presents a clear pipeline with four stages: task planning, model selection, task execution and response generation. Such a pipeline can effectively improve the success rate of solving user requests. Besides, HuggingGPT also introduces a global planning strategy to decompose user requests and thus accomplish task automation. Furthermore, HuggingGPT is a collaborative system, which fully utilizes the power of expert models from ML communities to solve AI tasks and present the huge potential of using external tools. Compared with these agents, HuggingGPT allows us to better solve tasks more effectively in professional areas and can be easily extended to any vertical domain. In the future, we will continue to enhance HuggingGPT with even more powerful abilities to develop a versatile autonomous agent with unlimited possibilities.",
    "idea_abstract": "We propose a framework, HuggingGPT, that leverages large language models as a controller to manage existing AI models across various domains and modalities. By utilizing the strong language capabilities of these models, we aim to empower a generic interface for solving complicated AI tasks, enabling the integration of diverse AI systems to tackle complex problems in a unified manner.",
    "proposal": "1. Title: Leveraging Large Language Models for General AI Capabilities\n\n2. Problem Statement: Current large language models (LLMs) have impressive capabilities in natural language processing tasks, but they are limited in their ability to process complex information from various modalities and domains. They also struggle with scheduling and cooperation of multiple models to solve complex tasks.\n\n3. Motivation: Existing methods are not sufficient to solve complex AI tasks due to their limitations in multimodal processing, task planning, and model selection. The proposed method, HuggingGPT, aims to address these challenges by leveraging the strong language capabilities of LLMs to connect with various AI models in machine learning communities.\n\n4. Proposed Method: HuggingGPT is a framework that uses LLMs as a controller to manage AI models from machine learning communities. It consists of four stages: task planning, model selection, task execution, and response generation. The LLM analyzes user requests, decomposes them into tasks, selects the most suitable models for each task, executes the tasks, and generates a response based on the results."
  },
  {
    "rank": 19,
    "title": "A Watermark for Large Language Models",
    "arxiv": "2301.10226v3",
    "paper_tar": "tar_files/2301.10226v3.tar.gz",
    "abstract": "-.2cm  -1 Potential harms of large language models can be mitigated by  watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of ``green'' tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",
    "content": "article graphicx subfigure booktabs enumitem multirow [dvipsnames]xcolor [pages=some,scale=1,angle=0,opacity=0.7]background [tracking=true]microtype encoding=*-50 [1]encoding=*#1 encoding=*0 hyphensurl hyperref colorlinks=true, pdftitle=A Watermark for Large Language Models, pdfpagemode=FullScreen, algorithm [accepted]shmicml2023 amsmath amssymb mathtools amsthm [capitalize,noabbrev,nameinlink]cleveref plain theoremTheorem[section] proposition[theorem]Proposition lemma[theorem]Lemma corollary[theorem]Corollary definition definition[theorem]Definition assumption[theorem]Assumption remark remark[theorem]Remark [textsize=tiny]todonotes [raggedrightboxes]ragged2e float xcolor soulpos [most]tcolorbox mdframed customgrayrgb0.9, 0.9, 0.9 lightgreenrgb0.83, 0.96, 0.72 lightyellowrgb0.94, 0.86, 0.51 lightredrgb0.99, 0.66, 0.66 [xoffset=1pt]customgray[-.7ex]2.5ex [xoffset=1pt]lightgreen[-.7ex]2.5ex [xoffset=1pt]lightyellow[-.7ex]2.5ex [xoffset=1pt]lightred[-.7ex]2.5ex *E *P *K V [1]red[Tom: #1] [1]teal[Jonas: #1] [1]blue[John: #1] [1]orange[Yuxin: #1] teaserblueRGB242, 242, 255 A Watermark for Large Language Models. Page of lastpagemaintext. frame code= center title, left=0pt, right=0pt, top=0pt, bottom=0pt, colframe=blue!5, colback=blue!5, enlarge left by=0mm, boxsep=5pt, arc=0pt,outer arc=0pt, [ A Watermark for Large Language Models equal* John Kirchenbauerequal Jonas Geipingequal Yuxin Wen Jonathan Katz Ian Miers Tom Goldstein University of Maryland John Kirchenbauerjkirchen@umd.edu Machine Learning, LLMs, Watermark, Language Model, Natural Language Processing, Generative AI 0.3in ] . Code and demo are available at github.com/jwkirchenbauer/lm-watermarking abstract -.5em0pt 0mm -.2cm -1 Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of ``green'' tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security. -0.5cm contents=9.2cm Introduction Large language models (LLMs), such as the recently developed ChatGPT, can write documents, create executable code, and answer questions, often with human-like capabilities . As these systems become more pervasive, there is increasing risk that they may be used for malicious purposes . These include social engineering and election manipulation campaigns that exploit automated bots on social media platforms, creation of fake news and web content, and use of AI systems for cheating on academic writing and coding assignments. Furthermore, the proliferation of synthetic data on the web complicates future dataset creation efforts, as synthetic data is often inferior to human content and must be detected and excluded before model training . For many reasons, the ability to detect and audit the usage of machine-generated text becomes a key principle of harm reduction for large language models~. [backgroundcolor=teaserblue,hidealllines=true] In this work, we study watermarking of language model output. A watermark is a hidden pattern in text that is imperceptible to humans, while making the text algorithmically identifiable as synthetic. We propose an efficient watermark that makes synthetic text detectable from short spans of tokens (as few as 25 tokens), while false-positives (where human text is marked as machine-generated) are statistically improbable. The watermark detection algorithm can be made public, enabling third parties (e.g., social media platforms) to run it themselves, or it can be kept private and run behind an API. We seek a watermark with the following properties: The watermark can be algorithmically detected without any knowledge of the model parameters or access to the language model API. This property allows the detection algorithm to be open sourced even when the model is not. This also makes detection cheap and fast because the LLM does not need to be loaded or run. Watermarked text can be generated using a standard language model without re-training. The watermark is detectable from only a contiguous portion of the generated text. This way, the watermark remains detectable when only a slice of the generation is used to create a larger document. The watermark cannot be removed without modifying a significant fraction of the generated tokens. We can compute a rigorous statistical measure of confidence that the watermark has been detected. Notation Language model basics Language models have a ``vocabulary'' containing words or word fragments known as ``tokens.'' Typical vocabularies contain tokens or more . Consider a sequence of tokens . Entries with negative indices, , represent a ``prompt'' of length and are tokens generated by an AI system in response to the prompt. A language model (LM) for next word prediction is a function , often parameterized by a neural network, that accepts as input a sequence of known tokens , which contains a prompt and the first tokens already produced by the language model, and then outputs a vector of logits, one for each word in the vocabulary. These logits are then passed through a softmax operator to convert them into a discrete probability distribution over the vocabulary. The next token at position is then sampled from this distribution using either standard multinomial sampling, or greedy sampling (greedy decoding) of the single most likely next token. Additionally, a procedure such as beam search can be employed to consider multiple possible sequences before selecting the one with the overall highest score. A caveat: The difficulty of watermarking low-entropy sequences Consider the following two sequences of tokens, with prompts in red: -2mm redThe quick brown fox jumps over the lazy dog redfor(i=0;i<n;i++) sum+=array[i] -2mm Were they produced by a human or by a language model? Determining this is fundamentally hard because these sequences have low entropy; the first few tokens strongly determine the following tokens. Low entropy text creates two problems for watermarking. First, both humans and machines provide similar if not identical completions for low entropy prompts, making it impossible to discern between them. Second, it is difficult to watermark low entropy text, as any changes to the choice of tokens may result in high perplexity, unexpected tokens that degrade the quality of the text. Later, we rigorously define sentence entropy, and analyze its impact on watermark detection. A simple proof of concept We start out by describing a simple ``hard'' red list watermark in hard that is easy to analyze, easy to detect and hard to remove. The simplicity of this approach comes at the cost of poor generation quality on low entropy sequences. We will discuss more sophisticated strategies later. The method works by generating a pseudo-random red list of tokens that are barred from appearing as The red list generator is seeded with the prior token , enabling the red list to be reproduced later without access to the entire generated sequence. Detecting the watermark. While producing watermarked text requires access to the language model, detecting the watermark does not. A third party with knowledge of the hash function and random number generator can re-produce the red list for each token and count how many times the red list rule is violated. We can detect the watermark by testing the following null hypothesis, Because the red list is chosen at random, a natural writer is expected to violate the red list rule with half of their tokens, while the watermarked model produces no violations. The probability that a natural source produces tokens without violating the red list rule is only which is vanishingly small even for short text fragments with a dozen words. This enables detection of the watermark (rejection of ) for, e.g., a synthetic tweet. A more robust detection approach uses a one proportion z-test to evaluate the null hypothesis. If the null hypothesis is true, then the number of green list tokens, denoted has expected value and variance The -statistic for this test is We reject the null hypothesis and detect the watermark if is above a chosen threshold. Suppose we choose to reject the null hypothesis if In this case, the probability of a false positive is which is the one-sided p-value corresponding to At the same time, we will detect any watermarked sequence with 16 or more tokens (the minimum value of that produces when =T). How hard is it to remove the watermark? The use of the one proportion z-test makes removal of the watermark difficult. Consider the case of a watermarked sequence of length . Suppose an adversary modifies 200 tokens in the sequence to add red list words and scrub the watermark. A modified token at position can violate the red list rule at position . Furthermore, the value of determines the red list for token and a maximally adversarial choice of will put in violation of the red list rule as well. For this reason, 200 token flips can create at most 400 violations of the red list rule. Unfortunately for the attacker, this maximally adversarial sequence with 600 remaining green list tokens still produces a z-statistic of and a p-value of leaving the watermark readily detectable with extremely high confidence. In general, removing the watermark of a long sequence requires modifying roughly one quarter of the tokens or more. -1 Note the analysis above assumes the attacker has complete knowledge of the watermark, and each selected token is maximally adversarial (which likely has a negative impact on quality). Without knowledge of the watermark algorithm, each flipped token has only a 50Drawbacks of the hard red list rule. The hard red list rule handles low entropy sequences in a simple way; it prevents the language model from producing them. For example, the token ``Barack'' is almost deterministically followed by ``Obama'' in many text datasets, yet ``Obama'' may be disallowed by the red list. A better behavior is to use a ``soft'' watermarking rule that is only active for high-entropy text that can be imperceptibly watermarked. As long as low-entropy sequences are wrapped inside a passage with enough total entropy, the passage will still easily trigger a watermark detector, solving the problem described in entropyproblem. Further, one can combine the watermark with a beam search decoder that ``irons-in'' the watermark. By searching the hypothesis space of likely token sequences, candidates sequences with a high density of tokens in the green list are found, resulting in a high strength watermark with minimal perplexity cost. A more sophisticated watermark We now discuss the ``soft'' watermark that promotes the use of the green list for high entropy tokens when many good choices are available, while having little impact on the choice of low-entropy tokens that are nearly deterministic. To derive this watermark, we examine what happens in the language model just before it produces a probability vector. The last layer of the language model outputs a vector of logits . These logits get converted into a probability vector using the softmax operator Rather than strictly prohibiting the red list tokens, Algorithm adds a constant to the logits of the green list tokens. -1 The soft red list rule adaptively enforces the watermark in situations where doing so will have little impact on quality, while almost ignoring the watermark rule in the low entropy case where there is a clear and unique choice of the ``best'' word. A highly likely word with has a much larger logit than other candidates, and this will remain the largest regardless of whether it is in the red list. But when the entropy is high, there are many comparably large logits to choose from, and the rule has a large impact on the sampling distribution, strongly biasing the output towards the green list. Detecting the soft watermark The process for detecting the soft watermark is identical to that for the hard watermark. We assume the null hypothesis null and compute a z-statistic using Equation zformula. We reject the null hypothesis and detect the watermark if is greater than a threshold. For arbitrary we have -1 Consider again the case in which we detect the watermark for Just like in the case of the hard watermark, we get false positives with rate In the case of the hard watermark, we could detect any watermarked sequence of length 16 tokens or more, regardless of the properties of the text. However, in the case of the soft watermark our ability to detect synthetic text depends on the entropy of the sequence. High entropy sequences are detected with relatively few tokens, while low entropy sequences require more tokens for detection. Below, we rigorously analyze the detection sensitivity of the soft watermark, and its dependence on entropy. Analysis of the soft watermark In this section, we examine the expected number of green list tokens used by a watermarked language model and analyze the dependence of this quantity on the entropy of a generated text fragment. Our analysis assumes the red list is sampled uniformly at random. This is a deviation from the method used in practice, which generates red lists using a pseudo-random number generator seeded with previous tokens. The consequences of pseudo-random sampling are explored in Section . We analyze the case in which text is generated by multinomial random sampling. In our experiments, we consider two more sampling schemes, greedy decoding and beam search. We need a definition of entropy that is appropriate for our analysis. The strength of our watermark is weak when the distribution over tokens has a large ``spike'' concentrated on one or several tokens. We define the following type of entropy to quantify this phenomenon. Given a discrete probability vector and a scalar we define the spike entropy of with modulus as Like the classical Shannon entropy, the spike entropy is a measure of how spread out a distribution is; The spike entropy assumes its minimal value of when the entire mass of is concentrated at a single location, and its maximal value of when the mass of is uniformly distributed. For large , the value of when and for For this reason, one can interpret the spike entropy as a softened measure of the number of entries in greater than The following theorem predicts the number of green list tokens that appear in a sequence with the watermark. Consider watermarked text sequences of tokens. Each sequence is produced by sequentially sampling a raw probability vector from the language model, sampling a random green list of size , and boosting the green list logits by using Equation before sampling each token. Define and let denote the number of green list tokens in sequence If a randomly generated watermarked sequence has average spike entropy at least i.e., then the number of green list tokens in the sequence has expected value at least Furthermore, the number of green list tokens has variance at most If we have chosen then we can use the strictly looser but simpler bound Remark. It may seem like there are a lot of messy constants floating around in this bound. However, when we choose and this bound simplifies to where is a bound on spike entropy with modulus 1/3. If we study the ``hard'' red list rules by choosing and letting we have where is a bound on spike entropy with modulus 1. Sensitivity of the watermark test The sensitivity of the soft watermark can be computed using standard type-II error analysis. For illustrative purposes, we estimate the type-II (false negative) error rate of a soft watermark with and We assume 200 tokens are generated using OPT-1.3B using prompts from the C4 dataset's RealNewsLike subset . We also assume a detection threshold of (which occurs at tokens) which gives us a type-I error (false positive) rate of . Theoretical bound. Our generations have an average spike entropy per sample of over generations. maintheorem says that the expected number of green list tokens per generation is at least . Indeed, the empirical average is . For sequences with entropy equal to the mean () we get tokens, and 98.6Empirical sensitivity. Empirically, n22h(h+1)nTnns^(i), -N_p<i<T.p^(T)T.T p^(T).TP^*=_k p^(T)_k (p^(T)_k)FF^(t)F_(s^(t-h),, s^(t-1))h||^1 + hh=1h 1hh=52.5=.5h=23hs^(t)s^(t-i^)s^(t),1/hh=2||^2i^^(t)hFh=1k=5 1-(1 -)^kknnT=2005T=200.z50010T=2005=.1TzT=.25=2zzz0.0 Tk=2050k500T=2005z0.01=0.10.33's's'ssw2^-wwbV|V|/ 2^bnz=2.0,=0.5 500z5zFz=.5,.z=10 (0,1)^Np N(1-) N (0,1).=exp().k_kGRN_GN_R,p_kk G.GRk Gf_kp_i, i k.f(p) = _ f( p),_kf_kp_-k p _ p p_i = (1-p_0)/(N-1)i k._k (0,1) 1f_k(p)>p_k = N_G/N.N_G,S^(t)tpp(1-p).S^t., .5,,(1-).kGRk G,k R.4,=0.5,2.0$) for both Google's FLAN-UL2 model and Huggingface BigScience's BLOOMZ model . However, we note that this particular experimental setup is not a situation where we would actually deploy the watermark or expect it to work very well. Generating 5 to 10 tokens per question under greedy decoding and then testing those tokens for exact correctness, is something of a worst-case estimate on the cost of watermarking. In this scenario the prompt is highly constraining and the only things the watermark can do are either nothing, or directly cause the model to deviate from the argmax. Such deviations would be detrimental on any question where the model ``knows'' the correct answer but isn't overwhelmingly confident in the token sequence required to represent it (especially the surface form). We leave a more comprehensive study of the impacts of watermarking strategies on the factuality of LLMs in question answering and other knowledge intensive settings to future research.",
    "idea_abstract": "We propose a watermarking framework for proprietary language models that embeds invisible signals into generated text, detectable by algorithmic means. The watermark can be embedded with minimal impact on text quality and can be detected using an open-source algorithm without access to the model's API or parameters. Our approach involves softly promoting specific tokens during sampling, and we provide a statistical test for detection and an information-theoretic framework for analyzing the watermark's sensitivity.",
    "proposal": "1. Title: A Watermark for Large Language Models\n\n2. Problem Statement: Large language models (LLMs) pose a significant risk of being used for malicious purposes, such as social engineering, fake news creation, and cheating on academic assignments. The ability to detect and audit machine-generated text is crucial for harm reduction.\n\n3. Motivation: Existing methods for detecting machine-generated text are often ineffective, and watermarking LLM output can be challenging due to low entropy sequences and the need to preserve text quality. Current watermarking methods either have poor generation quality or are difficult to detect.\n\n4. Proposed Method: We propose a watermarking framework for proprietary language models that embeds a hidden pattern in text, making it algorithmically identifiable as synthetic. The watermark can be detected without access to the language model API or parameters, and its presence can be confirmed with a statistical test. The watermark is designed to be imperceptible to humans and can be detected from a short span of tokens. We introduce a \"soft\" watermarking rule that adaptively enforces the watermark in high-entropy situations, minimizing its impact on text quality."
  },
  {
    "rank": 20,
    "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection Using Probability Curvature",
    "arxiv": "2301.11305v2",
    "paper_tar": "tar_files/2301.11305v2.tar.gz",
    "abstract": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call , does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find  is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for . See https://ericmitchell.ai/detectgptericmitchell.ai/detectgpt for code, data, and other project information.",
    "content": "[nohyperref]article microtype graphicx subfigure booktabs hyperref [noend]algorithmic algorithm [1]black!35//#1 [accepted]icml2023 amsmath amssymb mathtools amsthm multirow float wrapfig algorithmtlop xcolor human-bluergb0.619,0.722,0.827 machine-orangergb0.929,0.765,0.584 [capitalize,noabbrev]cleveref plain theoremTheorem[section] hypothesis[theorem]Hypothesis lemma[theorem]Lemma corollary[theorem]Corollary definition definition[theorem]Definition assumption[theorem]Assumption remark remark[theorem]Remark [textsize=tiny]todonotes [ equal* Eric Mitchellstan Yoonho Leestan Alexander Khazatskystan Christopher D. Manningstan Chelsea Finnstan stanStanford University Eric Mitchelleric.mitchell@cs.stanford.edu chatgpt, detection, zero-shot, text 0.3in ] The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call , does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for . See https://ericmitchell.ai/detectgptericmitchell.ai/detectgpt for code, data, and other project information. Introduction Large language models (LLMs) have proven able to generate remarkably fluent responses to a wide variety of user queries. Models such as GPT-3 , PaLM , and ChatGPT can convincingly answer complex questions about science, mathematics, historical and current events, and social trends. While recent work has found that cogent-sounding LLM-generated responses are often simply wrong , the articulate nature of such generated text may still make LLMs attractive for replacing human labor in some contexts, notably student essay writing and journalism. At least one major news source has released AI-written content with limited human review, leading to substantial factual errors in some articles . Such applications of LLMs are problematic for a variety of reasons, making fair student assessment difficult, impairing student learning, and proliferating convincing-but-inaccurate news articles. Unfortunately, humans perform only slightly better than chance when classifying machine-generated vs human-written text , leading researchers to consider automated detection methods that may identify signals difficult for humans to recognize. Such methods might give teachers and news-readers more confidence in the human origin of the text that they consume. As in prior work , we study the problem as a binary classification problem. Specifically, we aim to classify whether a candidate passage was generated by a particular source model. While several works have investigated methods for training a second deep network to detect machine-generated text, such an approach has several shortcomings, including a tendency to overfit to the topics it was trained on as well as the need to train a new model for each new source model that is released. We therefore consider the zero-shot version of , where we use the source model itself, without fine-tuning or adaptation of any kind, to detect its own samples. The most common method for zero-shot is evaluating the average per-token log probability of the generated text and thresholding . However, this zeroth-order approach to detection ignores the local structure of the learned probability function around a candidate passage, which we find contains useful information about the source of a passage. This paper poses a simple hypothesis: minor rewrites of model-generated text tend to have lower log probability under the model than the original sample, while minor rewrites of human-written text may have higher or lower log probability than the original sample. In other words, unlike human-written text, model-generated text tends to lie in areas where the log probability function has negative curvature (for example, near local maxima of the log probability). We empirically verify this hypothesis, and find that it holds true across a diverse body of LLMs, even when the minor rewrites, or perturbations, come from alternative language models. We leverage this observation to build , a zero-shot method for automated . To test if a passage came from a source model , compares the log probability of the candidate passage under with the average log probability of several perturbations of the passage under (generated with, e.g., T5; ). If the perturbed passages tend to have lower average log probability than the original by some margin, the candidate passage is likely to have come from . See Figure~ for an overview of the problem and . See Figure~ for an illustration of the underlying hypothesis and Figure~ for empirical evaluation of the hypothesis. Our experiments find that is more accurate than existing zero-shot methods for detecting machine-generated text, improving over the strongest zero-shot baseline by over 0.1 AUROC for multiple source models when detecting machine-generated news articles. Contributions. Our main contributions are: (a) the identification and empirical validation of the hypothesis that the curvature of a model's log probability function tends to be significantly more negative at model samples than for human text, and (b) , a practical algorithm inspired by this hypothesis that approximates the trace of the log probability function's Hessian to detect a model's samples. Related Work Increasingly large LLMs have led to dramatically improved performance on many language-related benchmarks and the ability to generate convincing and on-topic text. GROVER was the first LLM trained specifically for generating plausible news articles. Human evaluators found GROVER-generated propaganda at least as trustworthy as human-written propaganda, motivating the authors to study GROVER's ability to detect its own generations by fine-tuning a detector on top of its features; they found GROVER better able to detect GROVER-generated text than other pre-trained models. However, note that models trained explicitly to detect machine-generated text tend to overfit to their training distribution of data or source models. Other works have trained supervised models for on top of neural representations , bag-of-words features , and handcrafted statistical features . Alternatively, notes the surprising efficacy of a simple zero-shot method for , which thresholds a candidate passage based on its average log probability under the generative model, serving as a strong baseline for zero-shot in our work. In our work, we similarly use the generating model to detect its own generations in a zero shot manner, but through a different approach based on estimating local curvature of the log probability around the sample rather than the raw log probability of the sample itself. See for a complete survey on . Other work explores watermarks for generated text , which modify a model's generations to make them easier to detect. Our work does not assume text is generated with the goal of easy detection; detects text generated from publicly available LLMs using standard LLM sampling strategies. The widespread use of LLMs has led to much other contemporaneous work on detecting LLM output. show that the detection AUROC of the an detector is upper bounded by a function of the TV distance between the model and human text. However, we find that AUROC of is high even for the largest publicly-available models (Table~), suggesting that TV distance may not correlate strongly with model scale and capability. This disconnect may be exacerbated by new training objectives other than maximum likelihood, e.g., reinforcement learning with human feedback . Both and show the effectiveness of paraphrasing as a tool for evading detection, suggesting an important area of study for future work. show that multi-lingual detection is difficult, with non- detectors showing bias against non-native speakers; this result highlights the advantage of zero-shot detectors like , which generalize well to any data generated by the original generating model. study which proxy scoring models produce the most useful log probabilities for detection when the generating model is not known (a large-scale version of our Figure~). Surprisingly (but consistent with our findings), they find that smaller models are in fact better proxy models for performing detection with perturbation-based methods like . The problem of echoes earlier work on detecting deepfakes, artificial images or videos generated by deep nets, which has spawned substantial efforts in detection of fake visual content . While early works in deepfake detection used relatively general-purpose model architectures , many deepfake detection methods rely on the continuous nature of image data to achieve state-of-the-art performance , making direct application to text difficult. The Zero-Shot Problem We study zero-shot , the problem of detecting whether a piece of text, or candidate passage , is a sample from a source model . The problem is zero-shot in the sense that we do not assume access to human-written or generated samples to perform detection. As in prior work, we study a `white box' setting in which the detector may evaluate the log probability of a sample . The white box setting does not assume access to the model architecture or parameters. Most public APIs for LLMs (such as GPT-3) enable scoring text, though some exceptions exist, notably ChatGPT. While most of our experiments consider the white box setting, see Section~ for experiments in which we score text using models other than the source model. See for a comprehensive evaluation in this setting. The detection criterion we propose, , also makes use of generic pre-trained mask-filling models in order to generate passages that are `nearby' the candidate passage. However, these mask-filling models are used off-the-shelf, without any fine-tuning or adaptation to the target domain. : Zero-shot with Random Perturbations 0pt is based on the hypothesis that samples from a source model typically lie in areas of negative curvature of the log probability function of , unlike human text. In other words, if we apply small perturbations to a passage , producing , the quantity should be relatively large on average for machine-generated samples compared to human-written text. To leverage this hypothesis, first consider a perturbation function that gives a distribution over , slightly modified versions of with similar meaning (we will generally consider roughly paragraph-length texts ). As an example, might be the result of simply asking a human to rewrite one of the sentences of , while preserving the meaning of . Using the notion of a perturbation function, we can define the perturbation discrepancy : We state our hypothesis more formally as the Local Perturbation Discrepancy Gap Hypothesis, which describes a gap in the perturbation discrepancy for model-generated text and human-generated text. Discrepancy Gap Hypothesis. If produces samples on the data manifold, is positive and large with high probability for samples . For human-written text, tends toward zero for all . If we define to be samples from a mask-filling model such as T5 , rather than human rewrites, we can empirically test the Perturbation Discrepancy Gap Hypothesis in an automated, scalable manner. For real data, we use 500 news articles from the XSum dataset ; for model samples, we use the output of four different LLMs when prompted with the first 30 tokens of each article in XSum. We use T5-3B to apply perturbations, masking out randomly-sampled 2-word spans until 15Given these results, we can detect if a piece of text was generated by a model by simply thresholding the perturbation discrepancy. In practice, we find that normalizing the perturbation discrepancy by the standard deviation of the observed values used to estimate provides a slightly better signal for detection, typically increasing AUROC by around 0.020, so we use this normalized version of the perturbation discrepancy in our experiments. The resulting method, , is summarized in Alg.~. Having described an application of the perturbation discrepancy to , we next provide an interpretation of this quantity. Interpretation of perturbation discrepancy as curvature While Figure~ suggests that the perturbation discrepancy may be useful, it is not immediately obvious what it measures. In this section, we show that the perturbation discrepancy approximates a measure of the local curvature of the log probability function near the candidate passage, more specifically, that it is proportional to the negative trace of the Hessian of the log probability function. To handle the non-differentiability of discrete data, we consider candidate passages in a latent semantic space, where small displacements correspond to valid edits that retain similar meaning to the original. Because our perturbation function (T5) models natural text, we expect our perturbations to roughly capture such meaningful variations of the original passage, rather than arbitrary edits. We first invoke Hutchinson's trace estimator , giving an unbiased estimate of the trace of matrix : provided that the elements of are IID with and . To use Equation~ to estimate the trace of the Hessian of at , we must therefore compute the expectation of the directional second derivative . We approximate this expression with finite differences: Combining Equations~ and~ and simplifying with , we have an estimate of the negative Hessian trace If our noise distribution is symmetric, that is, for all , then we can simplify Equation~ to -0.3em 0.3em We note that the RHS of Equation~ corresponds to the perturbation discrepancy () where the perturbation function is replaced by the distribution used in Hutchinson's trace estimator (). Here, is a high-dimensional sequence of tokens while is a vector in a compact semantic space. Since the mask-filling model samples sentences similar to with minimal changes to semantic meaning, we can think of the mask-filling model as first sampling a similar semantic embedding () and then mapping this to a token sequence (). Sampling in semantic space ensures that all samples stay near the data manifold, which is useful because we would expect the log probability to always drop if we randomly perturb tokens. We can therefore interpret our objective as approximating the curvature restricted to the data manifold. Experiments We conduct experiments to better understand multiple facets of ; we study the effectiveness of for zero-shot compared to prior zero-shot approaches, the impact of distribution shift on zero-shot and supervised detectors, and detection accuracy for the largest publicly-available models. To further characterize factors that impact detection accuracy, we also study the robustness of zero-shot methods to machine-generated text that has been partially revised, the impact of alternative decoding strategies on detection accuracy, and a black-box variant of the detection task. Finally, we analyze more closely 's behavior as the choice of perturbation function, the number of samples used to estimate , the length of the passage, and the data distribution is varied. Comparisons. We compare with various existing zero-shot methods for that also leverage the predicted token-wise conditional distributions of the source model for detection. These methods correspond to statistical tests based on token log probabilities, token ranks, or predictive entropy . The first method uses the source model's average token-wise log probability to determine if a candidate passage is machine-generated or not; passages with high average log probability are likely to be generated by the model. The second and third methods use the average observed rank or log-rank of the tokens in the candidate passage according to the model's conditional distributions. Passages with smaller average (log-)rank are likely machine-generated. We also evaluate an entropy-based approach inspired by the hypothesis in that model-generated texts will be more `in-distribution' for the model, leading to more over-confident (thus lower entropy) predictive distributions. Empirically, we find predictive entropy to be positively correlated with passage fake-ness more often that not; therefore, this baseline uses high average entropy in the model's predictive distribution as a signal that a passage is machine-generated. While our main focus is on zero-shot detectors as they do not require re-training for new domains or source models, for completeness we perform comparisons to supervised detection models in Section~, using OpenAI's RoBERTa-based GPT-2 detector models,https://github.com/openai/gpt-2-output-tree/master/detector which are fine-tuned on millions of samples from various GPT-2 model sizes and decoding strategies. Datasets metrics Our experiments use six datasets that cover a variety of everyday domains and LLM use-cases. We use news articles from the XSum dataset to represent fake news detection, Wikipedia paragraphs from SQuAD contexts to represent machine-written academic essays, and prompted stories from the Reddit WritingPrompts dataset to represent detecting machine-generated creative writing submissions. To evaluate robustness to distribution shift, we also use the English and German splits of WMT16 as well as long-form answers written by human experts in the PubMedQA dataset . Each experiment uses between 150 and 500 examples for evaluation, as noted in the text. For each experiment, we generate the machine-generated text by prompting with the first 30 tokens of the real text (or just the question tokens for the PubMedQA experiments). We measure performance using the area under the receiver operating characteristic curve (AUROC), which can be interpreted as the probability that a classifier correctly ranks a randomly-selected positive (machine-generated) example higher than a randomly-selected negative (human-written) example. All experiments use an equal number of positive and negative examples. Hyperparameters. The key hyperparameters of are the fraction of words masked for perturbation, the length of the masked spans, the model used for mask filling, and the sampling hyperparameters for the mask-filling model. Using BERT masked language modeling as inspiration, we use 15Main Results We first present two groups of experiments to evaluate along with existing methods for zero-shot and supervised detection on models from 1.5B to 175B parameters. Zero-shot machine-generated text detection. We present the comparison of different zero-shot detection methods in Table . In these experiments, model samples are generated by sampling from the raw conditional distribution with temperature 1. most improves average detection accuracy for XSum stories (0.1 AUROC improvement) and SQuAD Wikipedia contexts (0.05 AUROC improvement). While it also performs accurate detection for WritingPrompts, the performance of all methods tends to increase, and the average margin of improvement is narrow. For 14 of the 15 combinations of dataset and model, provides the most accurate detection performance, with a 0.06 AUROC improvement on average. Log-rank thresholding proves to be a consistently stronger baseline than log probability thresholding, although it requires slightly more information (full predicted logits), which are not always available in public APIs. Comparison with supervised detectors. While our experiments generally focus on zero-shot detection, some works have evaluated the detection performance of supervised methods (typically fine-tuned transformers) for detecting machine-generated text. In this section, we explore several domains to better understand the relative strengths of supervised and zero-shot detectors. The results are presented in Figure~, using 200 samples from each dataset for evaluation. We find that supervised detectors can provide similar detection performance to on in-distribution data like English news, but perform significantly worse than zero-shot methods in the case of English scientific writing and fail altogether for German writing. This finding echoes past work showing that language models trained for machine-generated text detection overfit to their training data (source model, decoding strategy, topic, language, etc.; ). In contrast, zero-shot methods generalize relatively easily to new languages and domains; 's performance in particular is mostly unaffected by the change in language from English to German. While our experiments have shown that is effective on a variety of domains and models, it is natural to wonder if it is effective for the largest publicly-available LMs. Therefore, we also evaluate multiple zero-shot and supervised methods on two 175B parameter models, OpenAI's GPT-3 and AI21 Labs' Jurassic-2 Jumbo. Because neither API provides access to the complete conditional distribution for each token, we cannot compare to the rank, log rank, and entropy-based prior methods. We sample 150 examples from the PubMedQA, XSum, and WritingPrompts datasets and compare the two pre-trained RoBERTa-based detector models with and the probability thresholding baseline. We show in Table~ that can provide detection competitive with or better than the stronger of the two supervised models, and it again greatly outperforms probability thresholding on average. Variants of Detecting paraphrased machine-generated text. In practice, humans may manually edit or refine machine-generated text rather than blindly use a model's generations for their task of interest. We therefore conduct an experiment to simulate the detection problem for model samples that have been increasingly heavily revised. We simulate human revision by replacing 5 word spans of the text with samples from T5-3B until shows the strongest detection performance for all revision levels. Impact of alternative decoding strategies on detection. While Table~ suggests that is effective for detecting machine-generated text, prior work notes that the decoding strategy (i.e., temperature sampling, top-, nucleus/top-) can impact the difficulty of detection. We repeat the analysis from Section~ using top- sampling and nucleus sampling. Top- sampling truncates the sampling distribution to only the highest-probability next tokens; nucleus sampling samples from only the smallest set of tokens whose combined probability exceeds . The results are summarized in Table~; Appendix Tables~ and~ show complete results. We use , and , in line with prior work . We find that both top- and nucleus sampling make detection easier, on average. Averaging across domains, provides the clearest signal for zero-shot detection. -0.2em 0.2em Detection when the source model is unknown. While our experiments have focused on the white-box setting for , in this section, we r3.8cm -1mm -5mm performs best when scoring samples with the same model that generated them (diagonal), but the column means suggest that some models (GPT-Neo, GPT-2) may be better `scorers' than others (GPT-J). White values show mean (standard error) AUROC over XSum, SQuAD, and WritingPrompts; black shows row/column mean. -2mm explore the effect of using a different model to score a candidate passage (and perturbed texts) than the model that generated the passage. In other words, we aim to classify between human-generated text and text from model , but without access to model to compute log probabilities. Instead, we use log probabilities computed by a surrogate model . We consider three models, GPT-J, GPT-Neo-2.7, and GPT-2, evaluating all possible combinations of source model and surrogate model (9 total). We average the performance across 200 samples from XSum, SQuAD, and WritingPrompts. The results are presented in Figure~, showing that when the surrogate model is different from the source model, detection performance is reduced, indicating that is most suited to the white-box setting. Yet we also observe that if we fix the model used for scoring and average across source models whose generations are detected (average within column), there is significant variation in AUROC; GPT-2 and GPT-Neo-2.7 seem to be better `scorers' than GPT-J. These variations in cross-model scoring performance suggest ensembling scoring models may be a useful direction for future research; see for reference. Other factors impacting performance of In this section, we explore how factors such as the size of the mask-filling model, the number of perturbations used to estimate the expectation in Equation~, or the data distribution of the text to be detected impact detection quality. Source and mask-filling model scale. Here we study the impact of the size of the source model and mask-filling model on 's performance; the results are shown in Figure~. In particular, the increased discrimination power of for larger mask-filling models supports the interpretation that is estimating the curvature of the log probability in a latent semantic space, rather than in raw token embedding space. Larger T5 models better represent this latent space, where random directions correspond to meaningful changes in the text. Number of perturbations for . We evaluate the performance of as a function of the number of perturbations used to estimate the expectation in Equation~ on three datasets. The results are presented in Figure~. Detection accuracy continues to improve until 100 perturbations, where it converges. Evaluations use 100 examples from each dataset. Data distributional properties. We study more closely the impact of the data distribution on , particularly how the domain impacts the threshold separating the perturbation discrepancy distributions of model-generated and human texts as well as the impact of passage length on detection. Figure~ shows the perturbation discrepancy distributions for model-generated and human texts across four data distributions, using GPT-Neo-2.7B to generate samples. A threshold of slightly below 0.1 separates human and model texts across data distributions, which is important for practical scenarios in which a passage may be analyzed without knowing its domain a priori. Finally, Figure~ shows an analysis of 's performance as a function of passage length. We bin the paired human- and model-generated sequences by their average length into three bins of equal size (bottom/middle/top third), and plot the AUROC within each bin. The relationship between detection performance and passage length generally depends on the dataset and model (or tokenizer). For very long sequences, may see reduced performance because our implementation of applies all T5 mask-filling perturbations at once, and T5 may fail to track many mask tokens at once. By applying perturbations in multiple sequential rounds of smaller numbers of masks, this effect may be mitigated. Discussion As large language models continue to improve, they will become increasingly attractive tools for replacing human writers in a variety of contexts, such as education, journalism, and art. While legitimate uses of language model technologies exist in all of these settings, teachers, readers, and consumers are likely to demand tools for verifying the human origin of certain content with high educational, societal, or artistic significance, particularly when factuality (and not just fluency) is crucial. In light of these elevated stakes and the regular emergence of new large language models, we study the zero-shot machine-generated text detection problem, in which we use only the raw log probabilities computed by a generative model to determine if a candidate passage was sampled from it. We identify a property of the log probability function computed by a wide variety of large language models, showing that a tractable approximation to the trace of the Hessian of the model's log probability function provides a useful signal for detecting model samples. Our experiments find that this signal is more discriminative than existing zero-shot detection methods and is competitive with bespoke detection models trained with millions of model samples. and Watermarking. One interpretation of the perturbation function is producing semantically similar rephrasings of the original passage. If these rephrasings are systematically lower-probability than the original passage, the model is exposing its bias toward the specific (and roughly arbitrary, by human standards) phrasing used. In other words, LLMs that do not perfectly imitate human writing essentially watermark themselves implicitly. Under this interpretation, efforts to manually add watermarking biases to model outputs may further improve the effectiveness of methods such as , even as LLMs continue to improve. Limitations. One limitation of probability-based methods for zero-shot (like ) is the white-box assumption that we can evaluate log probabilities of the model(s) in question. For models behind APIs that do provide probabilities (such as GPT-3), evaluating probabilities nonetheless costs money. Another assumption of is access to a reasonable perturbation function. While in this work, we use off-the-shelf mask-filling models such as T5 and mT5 (for non-English languages), some domains may see reduced performance if existing mask-filling models do not well represent the space of meaningful rephrases, reducing the quality of the curvature estimate. While provides the best available detection performance for PubMedQA, its drop in performance compared to other datasets may be a result of lower quality perturbations. Finally, is more compute-intensive than other methods for detection, as it requires sampling and scoring the set of perturbations for each candidate passage, rather than just the candidate passage; a better tuned perturbation function or more efficient curvature approximation may help mitigate these costs. Future Work. While the methods in this work make no assumptions about the models generating the samples, future work may explore how watermarking algorithms can be used in conjunction with detection algorithms like to further improve detection robustness as language models continually improve their reproductions of human text. Separately, the results in Section~ suggest that extending to use ensembles of models for scoring, rather than a single model, may improve detection in the black box setting. Another topic that remains unexplored is the relationship between prompting and detection; that is, can a clever prompt successfully prevent a model's generations from being detected by existing methods? Finally, future work may explore whether the local log probability curvature property we identify is present for generative models in other domains, such as audio, video, or images. We hope that the present work serves as inspiration to future work developing effective, general-purpose methods for mitigating potential harms of machine-generated media. Acknowledgements EM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. CF and CM are CIFAR Fellows. The Stanford Center for Research on Foundation Models (CRFM) provided part of the compute resources used for the experiments in this work. Complete Results for Top- and Top- Decoding Tables~ and~ contain the complete results for XSum, SQuAD, and WritingPrompts for the five models considered in Table~. On average, both top- and top- sampling seem to make the detection task easier. This result is perhaps intuitive, as both sampling methods strictly increase the average log likelihood of model generations under the model (as they truncate low-probability tokens, albeit with different heuristics). Therefore methods based on probability or rank of tokens should become more discriminative. -0.2em 0.2em -0.2em 0.2em",
    "idea_abstract": "We propose a novel approach to detecting text generated by large language models, leveraging a property of the model's probability function that distinguishes generated text from human-written text. By analyzing the curvature of the model's log probability function, we define a criterion for judging whether a passage is generated from a given model. This approach, which we call , relies solely on log probabilities computed by the model of interest and random perturbations of the passage from a generic pre-trained language model.",
    "proposal": "1. Title: Detecting Model-Generated Text with Local Curvature\n\n2. Problem Statement: Detecting whether a piece of text was generated by a large language model (LLM) is a crucial task, especially in applications where factuality and authenticity are important, such as education, journalism, and art. However, existing methods for zero-shot detection have limitations, and there is a need for a more effective and generalizable approach.\n\n3. Motivation: Current zero-shot detection methods rely on evaluating the average log probability of the generated text or using statistical tests based on token log probabilities, token ranks, or predictive entropy. However, these methods have limitations, such as overfitting to the training distribution or requiring access to human-written or generated samples. In contrast, our proposed method, , leverages the local curvature of the log probability function to detect model-generated text, which is a more nuanced and informative signal.\n\n4. Proposed Method: Our method, , uses a perturbation function to generate semantically similar rephrasings of the original passage and estimates the local curvature of the log probability function by approximating the trace of the Hessian of the model's log probability function. We use a mask-filling model to generate perturbations and compute the perturbation"
  }
]